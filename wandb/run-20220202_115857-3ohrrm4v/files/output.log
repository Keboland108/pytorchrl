Sequential(
  (0): Linear(in_features=4, out_features=500, bias=True)
  (1): ReLU()
  (2): Linear(in_features=500, out_features=500, bias=True)
  (3): ReLU()
  (4): DeterministicMB(
    (output): Linear(in_features=500, out_features=3, bias=True)
  )
)
Training model from scratch
Training model from scratch
Collecting initial samples...
Created CWorker with worker_index 0
Created GWorker with worker_index 0
New EPOCH! 0
Update 1, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.9263
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 2, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.4869
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 3, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.2825
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 4, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 5, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.8739
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 6, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.5265
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 7, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 8, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.4258
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 9, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.6083
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 10, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 11, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2911
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 12, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.3587
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 13, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 14, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.4431
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 15, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 16, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.3148
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 17, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.1919
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 18, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.6343
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 19, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.5830
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 20, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0422
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 21, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 21
Update 22, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.5659
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 23, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.1988
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 24, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.2906
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 25, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.9834
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 26, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 27, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 28, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 29, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 30, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 31, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 32, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.5106
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 33, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 34, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 35, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.4128
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 36, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.5207
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 37, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 38, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.6997
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 39, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1029
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 40, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 41, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 42, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 42
Update 43, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 44, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0647
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 45, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 46, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 47, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.3432
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 48, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.8171
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 49, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2739
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 50, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 51, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.1394
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 52, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.3451
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 53, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 54, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 55, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.6397
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 56, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0922
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 57, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2402
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 58, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 59, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.5189
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 60, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 61, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.5482
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 62, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 63, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.4377
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 63
Update 64, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2682
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 65, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.6723
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 66, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 67, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 68, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.3516
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 69, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 70, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.6511
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 71, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 72, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.4472
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 73, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 74, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 75, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 76, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.2729
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 77, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.3498
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 78, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 79, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.5364
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 80, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.4053
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 81, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 82, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.5052
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 83, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 84, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 84
Update 85, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.1274
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 86, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.6864
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 87, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3410
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 88, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2800
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 89, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 90, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3224
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 91, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 92, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 93, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.4088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 94, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 95, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2699
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 96, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.7484
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 97, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2811
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 98, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2785
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 99, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2762
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 100, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 101, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 102, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.2525
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 103, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0828
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 104, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.1350
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 105, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 105
Update 106, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 107, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.6932
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 108, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.3406
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 109, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.1339
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 110, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.3406
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 111, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.7449
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 112, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 113, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 114, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2677
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 115, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2714
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 116, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1904
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 117, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 118, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.3178
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 119, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1918
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 120, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.5154
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 121, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2558
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 122, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 123, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0942
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 124, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0858
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 125, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3268
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 126, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 126
Update 127, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.7150
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 128, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 129, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 130, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.2195
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 131, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.6745
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 132, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 133, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.1918
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 134, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.2639
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 135, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.2810
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 136, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3436
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 137, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.4500
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 138, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 139, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.3264
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 140, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.3422
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 141, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 142, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 143, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.2834
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 144, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 145, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.3444
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 146, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.3456
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 147, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 147
Update 148, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.5181
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 149, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 150, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.5867
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 151, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 152, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 153, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.3988
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 154, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.3541
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 155, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 156, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 157, num samples collected 5250, FPS 304
  Algorithm: train_loss 1.0132
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 158, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.4087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 159, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 160, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 161, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 162, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3055
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 163, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 164, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 165, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 166, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3470
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 167, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 168, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 168
Update 169, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.1652
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 170, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 171, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 172, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.6578
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 173, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 174, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.4980
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 175, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0892
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 176, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.3434
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 177, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 178, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 179, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.5052
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 180, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 181, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.1835
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 182, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.5033
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 183, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 184, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 185, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.4245
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 186, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 187, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 188, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.4174
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 189, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 189
Update 190, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.7619
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 191, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 192, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 193, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 194, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 195, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 196, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1285
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 197, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.7528
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 198, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 199, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.4565
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 200, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 201, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.4297
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 202, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 203, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 204, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 205, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.2694
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 206, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 207, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0818
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 208, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.5603
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 209, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 210, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 210
Update 211, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 212, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 213, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 214, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.4182
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 215, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.7884
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 216, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.2879
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 217, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 218, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.2260
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 219, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0991
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 220, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 221, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 222, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 223, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 224, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.4073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 225, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.5764
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 226, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.5793
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 227, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.5576
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 228, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.4823
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 229, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 230, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 231, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 231
Update 232, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.3248
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 233, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.2167
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 234, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 235, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.3390
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 236, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 237, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.7555
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 238, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 239, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.5325
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 240, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3150
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 241, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 242, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 243, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 244, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.1863
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 245, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.5335
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 246, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 247, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 248, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3450
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 249, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2263
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 250, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 251, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 252, num samples collected 5250, FPS 296
  Algorithm: train_loss 1.3156
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 252
Update 253, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2768
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 254, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3426
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 255, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 256, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 257, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3388
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 258, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 259, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.6096
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 260, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2668
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 261, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 262, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.3888
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 263, num samples collected 5250, FPS 295
  Algorithm: train_loss 1.0000
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 264, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 265, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 266, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2720
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 267, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 268, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 269, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 270, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2507
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 271, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.3155
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 272, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.4831
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 273, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 273
Update 274, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.5293
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 275, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 276, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.6076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 277, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.4907
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 278, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2946
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 279, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.3928
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 280, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2729
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 281, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 282, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 283, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0949
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 284, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.3465
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 285, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 286, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 287, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.5110
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 288, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2291
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 289, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.1358
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 290, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2395
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 291, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 292, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.2988
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 293, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0768
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 294, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 294
Update 295, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 296, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0871
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 297, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1673
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 298, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 299, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1865
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 300, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 301, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 302, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.5037
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 303, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 304, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.2823
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 305, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 306, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.5038
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 307, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 308, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.3538
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 309, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.2181
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 310, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1350
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 311, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 312, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.8249
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 313, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.6466
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 314, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 315, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 315
Update 316, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.4838
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 317, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 318, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3826
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 319, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 320, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 321, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.5005
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 322, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 323, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2665
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 324, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3471
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 325, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 326, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.6967
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 327, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.7394
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 328, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 329, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 330, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 331, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2988
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 332, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0664
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 333, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 334, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 335, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 336, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 336
Update 337, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 338, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 339, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.4992
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 340, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.4879
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 341, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 342, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 343, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 344, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3142
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 345, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 346, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.1330
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 347, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.5545
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 348, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 349, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.6548
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 350, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.2558
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 351, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 352, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.4002
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 353, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 354, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 355, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 356, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.4239
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 357, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.7064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 357
Update 358, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 359, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.5184
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 360, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 361, num samples collected 5250, FPS 288
  Algorithm: train_loss 1.1124
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 362, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1356
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 363, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 364, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 365, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.2481
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 366, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 367, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 368, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1689
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 369, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.6288
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 370, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 371, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.5405
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 372, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 373, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.5551
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 374, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1799
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 375, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 376, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 377, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1602
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 378, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.6750
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 378
Update 379, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 380, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.5654
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 381, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 382, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.6872
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 383, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 384, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2842
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 385, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.4617
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 386, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1352
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 387, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1376
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 388, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2703
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 389, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 390, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 391, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3521
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 392, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 393, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.2725
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 394, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 395, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.2305
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 396, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 397, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 398, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 399, num samples collected 5250, FPS 286
  Algorithm: train_loss 1.3814
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 399
Update 400, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.2589
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 401, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 402, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 403, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 404, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 405, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.7852
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 406, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 407, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.3125
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 408, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.3704
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 409, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.3626
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 410, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 411, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 412, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 413, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 414, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2257
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 415, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 416, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 417, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1317
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 418, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 419, num samples collected 5250, FPS 284
  Algorithm: train_loss 1.1340
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 420, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.1623
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 420
Update 421, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 422, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 423, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.6744
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 424, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 425, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 426, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.4049
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 427, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.1579
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 428, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.6445
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 429, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 430, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 431, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 432, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 433, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.6530
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 434, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2747
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 435, num samples collected 5250, FPS 283
  Algorithm: train_loss 1.2782
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 436, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0875
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 437, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 438, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 439, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 440, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.4407
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 441, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 441
Update 442, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 443, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 444, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.5004
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 445, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 446, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 447, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 448, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0813
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 449, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 450, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.1319
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 451, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.7688
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 452, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2645
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 453, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2862
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 454, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 455, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3231
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 456, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 457, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 458, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.5619
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 459, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2791
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 460, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 461, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3724
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 462, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 462
Update 463, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.4352
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 464, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 465, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.5982
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 466, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 467, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.7028
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 468, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.2649
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 469, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 470, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 471, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 472, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.3328
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 473, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 474, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.7382
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 475, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 476, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.4997
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 477, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2192
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 478, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 479, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 480, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 481, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 482, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.6044
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 483, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 483
Update 484, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 485, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 486, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 487, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2695
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 488, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 489, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 490, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 491, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 492, num samples collected 5250, FPS 279
  Algorithm: train_loss 1.0489
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 493, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 494, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 495, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 496, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0741
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 497, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.6575
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 498, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.8687
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 499, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 500, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2895
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 501, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2725
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 502, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 503, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2637
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 504, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 504
Update 505, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.8502
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 506, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 507, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 508, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 509, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 510, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.4708
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 511, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 512, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 513, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.4801
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 514, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 515, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 516, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.7363
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 517, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 518, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 519, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 520, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.2951
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 521, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0912
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 522, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.8455
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 523, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 524, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 525, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 525
Update 526, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.5378
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 527, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 528, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1557
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 529, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.4023
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 530, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 531, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 532, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 533, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 534, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0866
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 535, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 536, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.3925
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 537, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.9587
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 538, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 539, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2694
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 540, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2868
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 541, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 542, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 543, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2263
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 544, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0890
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 545, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2637
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 546, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.9938
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 546
Update 547, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.4383
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 548, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 549, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2624
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 550, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 551, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.3796
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 552, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0979
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 553, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2750
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 554, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.6468
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 555, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 556, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2777
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 557, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1673
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 558, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.4686
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 559, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 560, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.6524
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 561, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 562, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 563, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1008
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 564, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0911
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 565, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2257
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 566, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2458
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 567, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 567
Update 568, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 569, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 570, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 571, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 572, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0959
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 573, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.6884
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 574, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 575, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.4817
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 576, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 577, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.5727
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 578, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 579, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 580, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.5031
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 581, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 582, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.5504
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 583, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.6493
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 584, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.4555
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 585, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1677
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 586, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 587, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 588, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1470
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 588
Update 589, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 590, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 591, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1316
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 592, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 593, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.5470
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 594, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.4671
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 595, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 596, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 597, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.6861
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 598, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.9034
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 599, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0778
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 600, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 601, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2521
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 602, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2732
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 603, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3422
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 604, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 605, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 606, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 607, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3916
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 608, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.1848
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 609, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 609
Update 610, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 611, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 612, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 613, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.4442
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 614, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.1587
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 615, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.6057
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 616, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.4545
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 617, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1317
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 618, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.3916
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 619, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2659
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 620, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2971
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 621, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.6522
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 622, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1822
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 623, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.4922
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 624, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 625, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 626, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2689
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 627, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 628, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.3459
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 629, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 630, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1457
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 630
Update 631, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.1803
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 632, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.7335
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 633, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 634, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 635, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 636, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 637, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.4499
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 638, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 639, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0885
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 640, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.7939
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 641, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 642, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 643, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2653
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 644, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.1931
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 645, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.3380
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 646, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.9590
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 647, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.3697
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 648, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 649, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 650, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 651, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 651
Update 652, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 653, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.4911
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 654, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 655, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 656, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1612
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 657, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 658, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 659, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.3147
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 660, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.8204
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 661, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0996
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 662, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1639
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 663, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.3924
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 664, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 665, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1355
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 666, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 667, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2588
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 668, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 669, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.8522
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 670, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.3124
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 671, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 672, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 672
Update 673, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.3808
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 674, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1816
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 675, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0891
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 676, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 677, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 678, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 679, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.5319
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 680, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0780
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 681, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.2628
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 682, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.9247
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 683, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 684, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.4895
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 685, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 686, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 687, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 688, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 689, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 690, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.6282
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 691, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.4769
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 692, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.2918
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 693, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 693
Update 694, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.1617
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 695, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 696, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 697, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 698, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 699, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 700, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.5873
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 701, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.2619
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 702, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.8091
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 703, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.2673
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 704, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 705, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 706, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.3877
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 707, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.2270
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 708, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0775
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 709, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.5443
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 710, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1321
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 711, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2721
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 712, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2638
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 713, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 714, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.6700
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 714
Update 715, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3821
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 716, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1281
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 717, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 718, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 719, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2514
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 720, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1415
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 721, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0752
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 722, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3060
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 723, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2453
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 724, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 725, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.4428
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 726, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 727, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 728, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.3150
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 729, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 730, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.6799
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 731, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 732, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 733, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.7463
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 734, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 735, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 735
Update 736, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 737, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 738, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 739, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 740, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0869
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 741, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1820
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 742, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2943
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 743, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 744, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 745, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.8125
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 746, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.4807
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 747, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 748, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 749, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 750, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 751, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2695
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 752, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 753, num samples collected 5250, FPS 263
  Algorithm: train_loss 1.2163
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 754, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.3161
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 755, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2959
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 756, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2537
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 756
Update 757, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.4510
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 758, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 759, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.4320
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 760, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.8246
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 761, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 762, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 763, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3716
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 764, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 765, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 766, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.2191
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 767, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 768, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1641
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 769, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0764
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 770, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3984
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 771, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3922
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 772, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 773, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.4683
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 774, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3604
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 775, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 776, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.2275
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 777, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 777
Update 778, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1867
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 779, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 780, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.5542
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 781, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 782, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 783, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 784, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.4944
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 785, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.7714
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 786, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 787, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2198
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 788, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 789, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.4394
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 790, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.4574
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 791, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 792, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 793, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 794, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2950
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 795, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 796, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.3382
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 797, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.2645
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 798, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.4198
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 798
Update 799, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 800, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0987
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 801, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.5628
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 802, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 803, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.1602
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 804, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.3471
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 805, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 806, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 807, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 808, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 809, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 810, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.6440
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 811, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 812, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.1460
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 813, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5187
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 814, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.6088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 815, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2609
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 816, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5400
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 817, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 818, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 819, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5237
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 819
Update 820, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.7702
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 821, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 822, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 823, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.3409
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 824, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 825, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 826, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 827, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0769
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 828, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.6964
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 829, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 830, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2630
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 831, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2492
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 832, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.5910
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 833, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.3908
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 834, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.4853
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 835, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 836, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 837, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 838, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1852
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 839, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 840, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 840
Update 841, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 842, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 843, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.3038
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 844, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 845, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.4719
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 846, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 847, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.3960
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 848, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.3569
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 849, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 850, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0747
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 851, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.5642
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 852, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.2311
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 853, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 854, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0865
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 855, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0640
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 856, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.4920
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 857, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.8670
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 858, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.1875
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 859, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 860, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 861, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 861
Update 862, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 863, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3928
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 864, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.5168
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 865, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.4968
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 866, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 867, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3355
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 868, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3526
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 869, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.2262
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 870, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.1665
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 871, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 872, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 873, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.5093
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 874, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 875, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.2219
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 876, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.4837
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 877, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.1863
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 878, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3128
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 879, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 880, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3367
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 881, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 882, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 882
Update 883, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 884, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1615
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 885, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.4085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 886, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2230
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 887, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 888, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.4217
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 889, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2728
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 890, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 891, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 892, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 893, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0873
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 894, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.8121
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 895, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.5977
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 896, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1316
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 897, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.3354
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 898, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 899, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2164
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 900, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 901, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3410
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 902, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3114
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 903, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 903
Update 904, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0958
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 905, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2689
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 906, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.4850
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 907, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.4020
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 908, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2807
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 909, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1843
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 910, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3208
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 911, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0900
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 912, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2292
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 913, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.4835
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 914, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 915, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.4426
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 916, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2593
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 917, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1674
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 918, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.3416
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 919, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 920, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 921, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 922, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2147
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 923, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 924, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.8429
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 924
Update 925, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 926, num samples collected 5250, FPS 253
  Algorithm: train_loss 1.0831
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 927, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 928, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.9210
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 929, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.5049
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 930, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 931, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 932, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 933, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 934, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 935, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 936, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3756
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 937, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 938, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2455
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 939, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.4105
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 940, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 941, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0860
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 942, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 943, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1580
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 944, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 945, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 945
Update 946, num samples collected 5250, FPS 252
  Algorithm: train_loss 1.2351
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 947, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.4186
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 948, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 949, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 950, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0940
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 951, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 952, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1893
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 953, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 954, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 955, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 956, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 957, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 958, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 959, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 960, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 961, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 962, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.6193
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 963, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.4300
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 964, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 965, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3962
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 966, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1891
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 966
Update 967, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 968, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.8788
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 969, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0858
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 970, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 971, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.4380
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 972, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.5602
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 973, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 974, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 975, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.3200
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 976, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 977, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 978, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.4033
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 979, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.6221
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 980, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.5956
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 981, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 982, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 983, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 984, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 985, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 986, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2240
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 987, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 987
Update 988, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.4668
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 989, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1336
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 990, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3964
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 991, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 992, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 993, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.5408
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 994, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2150
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 995, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 996, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 997, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 998, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2788
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 999, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.5154
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1000, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1001, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.4862
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1002, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1003, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1004, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3892
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1005, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3433
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1006, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1007, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1008, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1008
Update 1009, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.6397
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1010, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1011, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1012, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1013, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.4891
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1014, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1015, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.4308
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1016, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3906
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1017, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2231
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1018, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0936
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1019, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1020, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3885
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1021, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.1966
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1022, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1023, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1024, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2725
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1025, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1026, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3023
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1027, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.6110
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1028, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1650
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1029, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1029
Update 1030, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1031, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1032, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.4419
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1033, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1034, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1035, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1036, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.7639
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1037, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.5118
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1038, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1870
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1039, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.3404
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1040, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1041, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1042, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.3383
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1043, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1044, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.3850
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1045, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.3044
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1046, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.3181
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1047, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2627
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1048, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1049, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.1534
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1050, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1050
Update 1051, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.4390
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1052, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1053, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.4255
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1054, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1055, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0976
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1056, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1057, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2858
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1058, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.6306
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1059, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1442
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1060, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1061, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1062, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1332
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1063, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.5207
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1064, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.3108
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1065, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1066, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.5346
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1067, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0892
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1068, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2839
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1069, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.3402
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1070, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.3799
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1071, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1071
Update 1072, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1860
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1073, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1074, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1075, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.6227
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1076, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1077, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.8538
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1078, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1079, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3322
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1080, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1081, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3851
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1082, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1083, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1084, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0821
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1085, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2626
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1086, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1087, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1088, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1535
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1089, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1090, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2570
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1091, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.5825
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1092, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1092
Update 1093, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1801
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1094, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.4040
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1095, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1096, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2421
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1097, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0847
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1098, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0627
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1099, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1100, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1101, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1102, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1103, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1104, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.4883
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1105, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1106, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1107, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1108, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1109, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.7425
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1110, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1111, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1112, num samples collected 5250, FPS 242
  Algorithm: train_loss 1.0011
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1113, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.9153
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1113
Update 1114, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1115, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1116, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2479
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1117, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.6830
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1118, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.4194
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1119, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3279
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1120, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1121, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1122, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1123, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1124, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1125, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.5117
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1126, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2604
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1127, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3109
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1128, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2277
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1129, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3824
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1130, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2121
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1131, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1132, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3368
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1133, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3355
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1134, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1134
Update 1135, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3058
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1136, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1137, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1138, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.6057
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1139, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.5338
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1140, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1141, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.5312
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1142, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.1067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1143, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1144, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1145, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1146, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1147, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0855
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1148, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.6532
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1149, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1324
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1150, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.7400
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1151, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1152, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1153, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.6630
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1154, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1155, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.4766
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1155
Update 1156, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1157, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1158, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1159, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1160, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1161, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1162, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.6621
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1163, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1164, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.4639
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1165, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1316
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1166, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.4023
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1167, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1168, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2613
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1169, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1170, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.6521
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1171, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1172, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3136
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1173, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3136
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1174, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3412
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1175, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2727
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1176, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.6613
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1176
Update 1177, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1178, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2863
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1179, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3268
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1180, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1181, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.6365
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1182, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1183, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2179
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1184, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.4977
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1185, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1186, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.5579
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1187, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3611
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1188, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0898
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1189, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1190, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2773
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1191, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1192, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1193, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.5818
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1194, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.4471
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1195, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0766
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1196, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1197, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1197
Update 1198, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.5467
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1199, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3129
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1200, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3176
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1201, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1202, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1203, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1204, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1205, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1206, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.3830
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1207, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1208, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.2858
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1209, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1210, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.1868
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1211, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0722
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1212, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0868
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1213, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1214, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1215, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1216, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.9914
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1217, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.9170
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1218, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1218
Update 1219, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1220, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.5347
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1221, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0778
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1222, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.3320
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1223, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0890
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1224, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1225, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0807
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1226, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.1304
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1227, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1228, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.1654
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1229, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.1828
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1230, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2417
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1231, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.6109
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1232, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.3588
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1233, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.5521
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1234, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.4865
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1235, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1236, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2528
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1237, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1238, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1239, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.5210
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
New EPOCH! 1239
Update 1240, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2100
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1241, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1242, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1243, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.1359
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1244, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1245, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.6512
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1246, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.6062
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1247, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.4894
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1248, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1249, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.2685
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1250, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1251, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1252, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1253, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.3864
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1254, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.1762
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1255, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1256, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.6400
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1257, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1258, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1259, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
Update 1260, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1381.1136, l 200.0000, t 102.5380, TestReward -1337.6450
