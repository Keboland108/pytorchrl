Sequential(
  (0): Linear(in_features=4, out_features=500, bias=True)
  (1): ReLU()
  (2): Linear(in_features=500, out_features=500, bias=True)
  (3): ReLU()
  (4): DeterministicMB(
    (output): Linear(in_features=500, out_features=3, bias=True)
  )
)
Training model from scratch
Training model from scratch
Collecting initial samples...
Created CWorker with worker_index 0
Created GWorker with worker_index 0
New EPOCH! 0
Update 1, num samples collected 450, FPS 27
  Algorithm: train_loss 0.8536
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 2, num samples collected 450, FPS 27
  Algorithm: train_loss 0.8731
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 2
Update 3, num samples collected 450, FPS 27
  Algorithm: train_loss 0.3992
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 4, num samples collected 450, FPS 27
  Algorithm: train_loss 0.4766
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 4
Update 5, num samples collected 450, FPS 27
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 6, num samples collected 450, FPS 27
  Algorithm: train_loss 0.2659
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 6
Update 7, num samples collected 450, FPS 27
  Algorithm: train_loss 0.3976
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 8, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0751
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 8
Update 9, num samples collected 450, FPS 26
  Algorithm: train_loss 0.3236
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 10, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 10
Update 11, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 12, num samples collected 450, FPS 26
  Algorithm: train_loss 0.4101
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 12
Update 13, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 14, num samples collected 450, FPS 26
  Algorithm: train_loss 0.4083
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 14
Update 15, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2320
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 16, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 16
Update 17, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 18, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2845
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 18
Update 19, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2941
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 20, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 20
Update 21, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2843
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 22, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 22
Update 23, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 24, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2652
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 24
Update 25, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 26, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 26
Update 27, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2763
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 28, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 28
Update 29, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 30, num samples collected 450, FPS 26
  Algorithm: train_loss 0.3528
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 30
Update 31, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 32, num samples collected 450, FPS 26
  Algorithm: train_loss 0.3516
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 32
Update 33, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2658
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 34, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 34
Update 35, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 36, num samples collected 450, FPS 26
  Algorithm: train_loss 0.3404
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 36
Update 37, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 38, num samples collected 450, FPS 26
  Algorithm: train_loss 0.3368
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 38
Update 39, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2540
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 40, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 40
Update 41, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 42, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 42
Update 43, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0915
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 44, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 44
Update 45, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0921
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 46, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2177
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 46
Update 47, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 48, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 48
Update 49, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 50, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 50
Update 51, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1591
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 52, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1219
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 52
Update 53, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 54, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 54
Update 55, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1597
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 56, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 56
Update 57, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2300
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 58, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 58
Update 59, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 60, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2946
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 60
Update 61, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 62, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 62
Update 63, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 64, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 64
Update 65, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0849
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 66, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1828
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 66
Update 67, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0886
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 68, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 68
Update 69, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1309
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 70, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 70
Update 71, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1268
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 72, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1164
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 72
Update 73, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0910
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 74, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 74
Update 75, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 76, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1537
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 76
Update 77, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1920
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 78, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 78
Update 79, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 80, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2457
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 80
Update 81, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 82, num samples collected 450, FPS 26
  Algorithm: train_loss 0.2358
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 82
Update 83, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0985
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 84, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1226
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 84
Update 85, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0899
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 86, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 86
Update 87, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 88, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 88
Update 89, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0898
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 90, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 90
Update 91, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0869
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 92, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 92
Update 93, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0915
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 94, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1021
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 94
Update 95, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 96, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 96
Update 97, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 98, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0941
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 98
Update 99, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0670
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 100, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 100
Update 101, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 102, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1831
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 102
Update 103, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0692
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 104, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 104
Update 105, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 106, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 106
Update 107, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 108, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 108
Update 109, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 110, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 110
Update 111, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0836
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 112, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 112
Update 113, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 114, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 114
Update 115, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1240
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 116, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 116
Update 117, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0813
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 118, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 118
Update 119, num samples collected 450, FPS 26
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
Update 120, num samples collected 450, FPS 26
  Algorithm: train_loss 0.1486
  Episodes: TrainReward -1184.9456, l 200.0000, t 9.8824, TestReward -1473.0168
New EPOCH! 120
Update 121, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 122, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 123, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 123
Update 124, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 125, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 126, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1027
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 126
Update 127, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0797
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 128, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1147
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 129, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 129
Update 130, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 131, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 132, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 132
Update 133, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 134, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 135, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1978
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 135
Update 136, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 137, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 138, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 138
Update 139, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 140, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 141, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 141
Update 142, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 143, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 144, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 144
Update 145, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 146, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 147, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1643
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 147
Update 148, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 149, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0974
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 150, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 150
Update 151, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 152, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 153, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 153
Update 154, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 155, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 156, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1879
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 156
Update 157, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 158, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 159, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 159
Update 160, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 161, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 162, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0316
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 162
Update 163, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 164, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 165, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 165
Update 166, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 167, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 168, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 168
Update 169, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1171
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 170, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 171, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 171
Update 172, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 173, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 174, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 174
Update 175, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 176, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1212
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 177, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 177
Update 178, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 179, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 180, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1283
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 180
Update 181, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 182, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 183, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 183
Update 184, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0981
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 185, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 186, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 186
Update 187, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 188, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0982
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 189, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 189
Update 190, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 191, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 192, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 192
Update 193, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 194, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 195, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0322
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 195
Update 196, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 197, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 198, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1303
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 198
Update 199, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0982
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 200, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 201, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 201
Update 202, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 203, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 204, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 204
Update 205, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 206, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0997
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 207, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 207
Update 208, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 209, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0956
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 210, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 210
Update 211, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 212, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0953
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 213, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 213
Update 214, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 215, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 216, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 216
Update 217, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 218, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 219, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 219
Update 220, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1110
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 221, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 222, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 222
Update 223, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 224, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0968
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 225, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0029
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 225
Update 226, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 227, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 228, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 228
Update 229, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 230, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 231, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1238
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 231
Update 232, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 233, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 234, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 234
Update 235, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0887
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 236, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 237, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0346
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 237
Update 238, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 239, num samples collected 700, FPS 20
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 240, num samples collected 700, FPS 20
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 240
Update 241, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1097
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 242, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 243, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 243
Update 244, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 245, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0920
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 246, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 246
Update 247, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 248, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 249, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1278
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 249
Update 250, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 251, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 252, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 252
Update 253, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 254, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 255, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 255
Update 256, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 257, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0912
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 258, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 258
Update 259, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 260, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 261, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1465
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 261
Update 262, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 263, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 264, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 264
Update 265, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 266, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 267, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 267
Update 268, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 269, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0928
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 270, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 270
Update 271, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0910
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 272, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0033
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 273, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 273
Update 274, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 275, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 276, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 276
Update 277, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0868
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 278, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 279, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 279
Update 280, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 281, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 282, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 282
Update 283, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 284, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1077
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 285, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 285
Update 286, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 287, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1052
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 288, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 288
Update 289, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 290, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 291, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 291
Update 292, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 293, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 294, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1246
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 294
Update 295, num samples collected 700, FPS 19
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 296, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 297, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 297
Update 298, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0848
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 299, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
Update 300, num samples collected 700, FPS 19
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1327.9720, l 200.0000, t 26.2148, TestReward -1218.0014
New EPOCH! 300
Update 301, num samples collected 950, FPS 18
  Algorithm: train_loss 0.2196
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 302, num samples collected 950, FPS 18
  Algorithm: train_loss 0.4116
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 303, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 304, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3685
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 304
Update 305, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3400
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 306, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 307, num samples collected 950, FPS 18
  Algorithm: train_loss 0.2435
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 308, num samples collected 950, FPS 18
  Algorithm: train_loss 0.2098
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 308
Update 309, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 310, num samples collected 950, FPS 18
  Algorithm: train_loss 0.5736
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 311, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 312, num samples collected 950, FPS 18
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 312
Update 313, num samples collected 950, FPS 18
  Algorithm: train_loss 0.2765
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 314, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3622
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 315, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0760
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 316, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 316
Update 317, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 318, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0912
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 319, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 320, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3631
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 320
Update 321, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 322, num samples collected 950, FPS 18
  Algorithm: train_loss 0.4025
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 323, num samples collected 950, FPS 18
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 324, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 324
Update 325, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0826
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 326, num samples collected 950, FPS 18
  Algorithm: train_loss 0.2560
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 327, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3261
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 328, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 328
Update 329, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 330, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 331, num samples collected 950, FPS 18
  Algorithm: train_loss 0.5545
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 332, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 332
Update 333, num samples collected 950, FPS 18
  Algorithm: train_loss 0.4135
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 334, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 335, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 336, num samples collected 950, FPS 18
  Algorithm: train_loss 0.3139
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 336
Update 337, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 338, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 339, num samples collected 950, FPS 18
  Algorithm: train_loss 0.0871
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 340, num samples collected 950, FPS 17
  Algorithm: train_loss 0.7470
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 340
Update 341, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 342, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 343, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3906
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 344, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3219
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 344
Update 345, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3173
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 346, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 347, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2824
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 348, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 348
Update 349, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 350, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3408
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 351, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 352, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2896
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 352
Update 353, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3851
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 354, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 355, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 356, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2959
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 356
Update 357, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2017
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 358, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4178
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 359, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 360, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 360
Update 361, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 362, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 363, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3284
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 364, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1332
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 364
Update 365, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 366, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 367, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4077
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 368, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 368
Update 369, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 370, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1811
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 371, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 372, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4518
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 372
Update 373, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 374, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 375, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5596
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 376, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 376
Update 377, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 378, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0376
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 379, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5771
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 380, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 380
Update 381, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 382, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1960
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 383, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 384, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5696
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 384
Update 385, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0983
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 386, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 387, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4957
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 388, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0403
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 388
Update 389, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1900
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 390, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3098
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 391, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 392, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1509
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 392
Update 393, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0867
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 394, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 395, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3535
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 396, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2693
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 396
Update 397, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0780
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 398, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 399, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 400, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4681
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 400
Update 401, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 402, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3311
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 403, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 404, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 404
Update 405, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3375
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 406, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1853
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 407, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0915
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 408, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 408
Update 409, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 410, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3995
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 411, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 412, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 412
Update 413, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 414, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 415, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2632
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 416, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 416
Update 417, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 418, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 419, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 420, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5292
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 420
Update 421, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 422, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3253
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 423, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 424, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3610
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 424
Update 425, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 426, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3730
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 427, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 428, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 428
Update 429, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 430, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3214
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 431, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 432, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 432
Update 433, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5490
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 434, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 435, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 436, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 436
Update 437, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 438, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4761
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 439, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 440, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1258
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 440
Update 441, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0696
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 442, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 443, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 444, num samples collected 950, FPS 17
  Algorithm: train_loss 0.7036
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 444
Update 445, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 446, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 447, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3299
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 448, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 448
Update 449, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0741
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 450, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3312
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 451, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 452, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2542
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 452
Update 453, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3087
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 454, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0747
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 455, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1898
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 456, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 456
Update 457, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3126
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 458, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 459, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 460, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3378
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 460
Update 461, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3051
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 462, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 463, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 464, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2584
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 464
Update 465, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 466, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3249
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 467, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 468, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 468
Update 469, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3230
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 470, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 471, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1695
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 472, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1204
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 472
Update 473, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 474, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1807
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 475, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 476, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5449
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 476
Update 477, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 478, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 479, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4599
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 480, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 480
Update 481, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 482, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2980
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 483, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 484, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 484
Update 485, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4579
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 486, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 487, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0955
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 488, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 488
Update 489, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2196
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 490, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 491, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3376
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 492, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 492
Update 493, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 494, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3054
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 495, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1742
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 496, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 496
Update 497, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1692
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 498, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0838
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 499, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3098
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 500, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 500
Update 501, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 502, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 503, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1876
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 504, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5328
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 504
Update 505, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1602
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 506, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3572
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 507, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 508, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 508
Update 509, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 510, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 511, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 512, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 512
Update 513, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 514, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2255
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 515, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3149
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 516, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 516
Update 517, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 518, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 519, num samples collected 950, FPS 17
  Algorithm: train_loss 0.5221
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 520, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 520
Update 521, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 522, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2959
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 523, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 524, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3328
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 524
Update 525, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 526, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 527, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 528, num samples collected 950, FPS 17
  Algorithm: train_loss 0.4400
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 528
Update 529, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 530, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3665
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 531, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1648
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 532, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 532
Update 533, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2905
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 534, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0822
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 535, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0351
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 536, num samples collected 950, FPS 17
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 536
Update 537, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0668
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 538, num samples collected 950, FPS 17
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 539, num samples collected 950, FPS 17
  Algorithm: train_loss 0.3038
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
Update 540, num samples collected 950, FPS 17
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -1315.8996, l 200.0000, t 43.2457, TestReward -821.0907
New EPOCH! 540
Update 541, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3178
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 542, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.6010
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 543, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 544, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 545, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1092
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 545
Update 546, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4292
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 547, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1707
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 548, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2984
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 549, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0703
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 550, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 550
Update 551, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 552, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 553, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.6248
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 554, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1725
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 555, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 555
Update 556, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 557, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4184
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 558, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1974
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 559, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 560, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4177
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 560
Update 561, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 562, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0913
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 563, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 564, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.6888
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 565, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2437
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 565
Update 566, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 567, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3439
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 568, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 569, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1683
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 570, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4279
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 570
Update 571, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2888
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 572, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4338
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 573, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0860
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 574, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1382
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 575, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 575
Update 576, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 577, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3993
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 578, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1522
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 579, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3900
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 580, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 580
Update 581, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3899
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 582, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 583, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2953
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 584, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0740
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 585, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 585
Update 586, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5664
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 587, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 588, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 589, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 590, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 590
Update 591, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0692
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 592, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 593, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 594, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3066
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 595, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5743
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 595
Update 596, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.7224
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 597, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 598, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 599, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 600, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2346
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 600
Update 601, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 602, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 603, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 604, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2887
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 605, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.7047
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 605
Update 606, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 607, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1541
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 608, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2983
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 609, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 610, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4729
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 610
Update 611, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.6120
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 612, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 613, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 614, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2711
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 615, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 615
Update 616, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4005
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 617, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 618, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1848
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 619, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 620, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4127
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 620
Update 621, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 622, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2907
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 623, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 624, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0415
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 625, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2028
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 625
Update 626, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 627, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3734
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 628, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2886
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 629, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0959
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 630, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 630
Update 631, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 632, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 633, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 634, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 635, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3851
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 635
Update 636, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2700
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 637, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3054
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 638, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0778
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 639, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 640, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 640
Update 641, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1435
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 642, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2922
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 643, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 644, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2830
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 645, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 645
Update 646, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 647, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5288
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 648, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 649, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2548
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 650, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1279
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 650
Update 651, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 652, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 653, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 654, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 655, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5644
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 655
Update 656, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 657, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 658, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 659, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 660, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.7479
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 660
Update 661, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 662, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 663, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.7033
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 664, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 665, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1854
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 665
Update 666, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 667, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1528
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 668, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 669, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3345
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 670, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 670
Update 671, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4883
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 672, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2786
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 673, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 674, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0721
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 675, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 675
Update 676, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0684
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 677, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2555
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 678, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1310
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 679, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1279
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 680, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4546
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 680
Update 681, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1199
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 682, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3224
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 683, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 684, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 685, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5512
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 685
Update 686, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 687, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2550
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 688, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1700
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 689, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2756
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 690, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2493
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 690
Update 691, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 692, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3205
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 693, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3846
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 694, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1300
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 695, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 695
Update 696, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 697, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3369
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 698, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 699, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 700, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 700
Update 701, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2717
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 702, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2668
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 703, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1252
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 704, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1790
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 705, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 705
Update 706, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 707, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2664
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 708, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1692
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 709, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 710, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4081
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 710
Update 711, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0766
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 712, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 713, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 714, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2623
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 715, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5541
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 715
Update 716, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2380
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 717, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 718, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2881
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 719, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 720, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 720
Update 721, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3920
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 722, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 723, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 724, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 725, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5975
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 725
Update 726, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 727, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 728, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4157
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 729, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1216
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 730, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4101
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 730
Update 731, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3857
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 732, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 733, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 734, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3110
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 735, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 735
Update 736, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1318
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 737, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0731
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 738, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5881
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 739, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 740, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 740
Update 741, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 742, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 743, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2671
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 744, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2428
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 745, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4215
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 745
Update 746, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0728
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 747, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4857
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 748, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 749, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 750, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1710
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 750
Update 751, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 752, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3673
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 753, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0769
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 754, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 755, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1639
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 755
Update 756, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 757, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4917
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 758, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 759, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 760, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 760
Update 761, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 762, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 763, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4941
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 764, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0690
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 765, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0351
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 765
Update 766, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2547
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 767, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 768, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3553
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 769, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 770, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 770
Update 771, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5662
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 772, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 773, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 774, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 775, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 775
Update 776, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4468
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 777, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 778, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 779, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 780, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 780
Update 781, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 782, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 783, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1198
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 784, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3785
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 785, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 785
Update 786, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1602
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 787, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 788, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 789, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 790, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5234
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 790
Update 791, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 792, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2720
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 793, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3892
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 794, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 795, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 795
Update 796, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2501
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 797, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 798, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2383
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 799, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1041
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 800, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1541
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 800
Update 801, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 802, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 803, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 804, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4738
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 805, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0464
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 805
Update 806, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5452
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 807, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 808, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 809, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 810, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 810
Update 811, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1201
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 812, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2672
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 813, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1190
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 814, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2911
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 815, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 815
Update 816, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 817, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 818, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 819, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.4993
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 820, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 820
Update 821, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 822, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2861
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 823, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2690
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 824, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 825, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 825
Update 826, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 827, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.5837
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 828, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 829, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1361
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 830, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 830
Update 831, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 832, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1017
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 833, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.1246
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 834, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0772
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 835, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.6590
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 835
Update 836, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 837, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 838, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3289
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 839, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.0709
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
Update 840, num samples collected 1200, FPS 16
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -1157.8334, l 200.0000, t 62.7629, TestReward -1533.0800
New EPOCH! 840
Update 841, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 842, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0931
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 843, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 844, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 845, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2594
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 846, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.1678
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 846
Update 847, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 848, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 849, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 850, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 851, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.3067
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 852, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 852
Update 853, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 854, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2670
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 855, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0847
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 856, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.3319
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 857, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.1209
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 858, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 858
Update 859, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 860, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 861, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 862, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.4134
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 863, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 864, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 864
Update 865, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.3024
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 866, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 867, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 868, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 869, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.3078
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 870, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 870
Update 871, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.3417
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 872, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 873, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 874, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 875, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 876, num samples collected 1450, FPS 16
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 876
Update 877, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2932
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 878, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1272
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 879, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3419
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 880, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 881, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 882, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 882
Update 883, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 884, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 885, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4569
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 886, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 887, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 888, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 888
Update 889, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 890, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 891, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2254
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 892, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3167
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 893, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0944
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 894, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 894
Update 895, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 896, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4376
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 897, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 898, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 899, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 900, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 900
Update 901, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 902, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2458
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 903, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 904, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1102
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 905, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0738
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 906, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 906
Update 907, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 908, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 909, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 910, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3072
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 911, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 912, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 912
Update 913, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 914, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 915, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.5816
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 916, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 917, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 918, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 918
Update 919, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 920, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1067
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 921, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 922, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 923, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3157
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 924, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4498
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 924
Update 925, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 926, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 927, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 928, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 929, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 930, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4429
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 930
Update 931, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 932, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2658
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 933, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2319
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 934, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 935, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 936, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 936
Update 937, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1003
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 938, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 939, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 940, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 941, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3135
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 942, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 942
Update 943, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4590
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 944, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 945, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1209
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 946, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1323
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 947, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 948, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 948
Update 949, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0307
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 950, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3400
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 951, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3296
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 952, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 953, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 954, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 954
Update 955, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3187
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 956, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 957, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 958, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 959, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2393
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 960, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 960
Update 961, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0997
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 962, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 963, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2358
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 964, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 965, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 966, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 966
Update 967, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 968, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 969, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 970, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 971, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4305
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 972, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 972
Update 973, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 974, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 975, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 976, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 977, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4076
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 978, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 978
Update 979, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 980, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 981, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1659
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 982, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1936
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 983, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0911
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 984, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3830
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 984
Update 985, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1041
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 986, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 987, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3445
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 988, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1816
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 989, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 990, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 990
Update 991, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 992, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0416
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 993, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0772
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 994, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2950
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 995, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 996, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2757
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 996
Update 997, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 998, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1087
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 999, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1972
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1000, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2968
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1001, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1002, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1002
Update 1003, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1004, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1063
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1005, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1006, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1864
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1007, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0961
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1008, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4229
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1008
Update 1009, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1010, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1011, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1843
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1012, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1157
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1013, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1014, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1014
Update 1015, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1055
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1016, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1815
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1017, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1018, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3065
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1019, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0809
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1020, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1020
Update 1021, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1022, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4351
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1023, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1024, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1025, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1039
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1026, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1026
Update 1027, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1874
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1028, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1029, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1030, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0695
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1031, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2939
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1032, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1032
Update 1033, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1034, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3868
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1035, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1036, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1037, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1171
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1038, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1038
Update 1039, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1040, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1041, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1042, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2143
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1043, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2565
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1044, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1044
Update 1045, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1046, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1047, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1048, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2425
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1049, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3222
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1050, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1063
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1050
Update 1051, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1052, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1053, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1054, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1055, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1056, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.4157
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1056
Update 1057, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0690
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1058, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2332
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1059, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1060, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2551
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1061, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1062, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1164
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1062
Update 1063, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1064, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1065, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0899
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1066, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1668
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1067, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1068, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1068
Update 1069, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1070, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2697
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1071, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1072, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1073, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1074, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1074
Update 1075, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1076, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1077, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1477
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1078, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1868
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1079, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0316
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1080, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1080
Update 1081, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1008
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1082, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1083, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1084, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1085, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3469
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1086, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1086
Update 1087, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1088, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1089, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1090, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1091, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1092, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1092
Update 1093, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1094, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0967
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1095, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2245
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1096, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1097, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1098, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3351
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1098
Update 1099, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1100, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2878
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1101, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1102, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1103, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1104, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1104
Update 1105, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1106, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0695
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1107, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1108, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0319
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1109, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1110, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1365
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1110
Update 1111, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1112, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1113, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1114, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2939
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1115, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1116, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1116
Update 1117, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1966
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1118, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1119, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1120, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1121, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2648
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1122, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1122
Update 1123, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1124, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1125, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1126, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1127, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0899
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1128, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1128
Update 1129, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1130, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1131, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1132, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1133, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3216
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1134, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0841
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1134
Update 1135, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1971
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1136, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1137, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1138, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1139, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1552
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1140, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1140
Update 1141, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2365
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1142, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0378
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1143, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2328
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1144, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1145, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1146, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1146
Update 1147, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0685
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1148, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1149, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1150, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1151, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1152, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1152
Update 1153, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1154, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1155, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1156, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1157, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1158, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3179
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1158
Update 1159, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0672
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1160, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1161, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1162, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1041
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1163, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1989
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1164, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0879
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1164
Update 1165, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2303
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1166, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1602
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1167, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1168, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1304
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1169, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1170, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1170
Update 1171, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1254
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1172, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1173, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1174, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.3284
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1175, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1176, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1176
Update 1177, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1178, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1884
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1179, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1180, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1181, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1182, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1182
Update 1183, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1184, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1237
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1185, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1186, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2993
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1187, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1188, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1188
Update 1189, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2905
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1190, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1191, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1192, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1193, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1261
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1194, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1194
Update 1195, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0427
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1196, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.1056
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1197, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0863
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1198, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1199, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.2998
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
Update 1200, num samples collected 1450, FPS 15
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1281.7027, l 200.0000, t 82.4965, TestReward -1087.1867
New EPOCH! 1200
Update 1201, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1202, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1203, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2363
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1204, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1205, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1243
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1206, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1207, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1207
Update 1208, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3759
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1209, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1210, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1211, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1212, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1213, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1214, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1876
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1214
Update 1215, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1216, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1217, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1218, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1219, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2691
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1220, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1221, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1221
Update 1222, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1223, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1224, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1146
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1225, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3207
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1226, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1227, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1506
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1228, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1228
Update 1229, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1734
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1230, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1231, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1232, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1233, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1234, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1235, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1235
Update 1236, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1237, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1675
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1238, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1239, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1240, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1241, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1242, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1242
Update 1243, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1244, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1245, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1246, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1247, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2052
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1248, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0999
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1249, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3340
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1249
Update 1250, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1251, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2158
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1252, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1253, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1254, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1255, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1256, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1256
Update 1257, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1258, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1814
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1259, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1260, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2876
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1261, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1262, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1263, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1263
Update 1264, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1265, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1266, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1748
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1267, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1268, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1269, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1270, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2086
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1270
Update 1271, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1272, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1273, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1274, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1798
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1275, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2151
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1276, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1277, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2337
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1277
Update 1278, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1760
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1279, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1280, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1281, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1282, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1773
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1283, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1284, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2682
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1284
Update 1285, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2883
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1286, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1287, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0788
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1288, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1289, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0953
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1290, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0903
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1291, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1291
Update 1292, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1293, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1294, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1295, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1296, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0855
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1297, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0898
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1298, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1298
Update 1299, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1457
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1300, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2647
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1301, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1302, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1303, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1304, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1269
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1305, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1305
Update 1306, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1307, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0898
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1308, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1309, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0934
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1310, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1731
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1311, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1643
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1312, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1312
Update 1313, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2535
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1314, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1315, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1316, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1317, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1873
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1318, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1319, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1319
Update 1320, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1321, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1322, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1323, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1324, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3002
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1325, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1326, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1326
Update 1327, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0752
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1328, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1860
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1329, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1330, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0677
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1331, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1332, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1333, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0366
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1333
Update 1334, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1335, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1336, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1337, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1338, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2130
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1339, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2175
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1340, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1340
Update 1341, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1342, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1343, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1344, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0496
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1345, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0860
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1346, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2827
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1347, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1347
Update 1348, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1349, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1350, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3059
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1351, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1352, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1353, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0946
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1354, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1354
Update 1355, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1356, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1357, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1358, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1359, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1360, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1361, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1361
Update 1362, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1227
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1363, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1364, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1365, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1366, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1367, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1368, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1368
Update 1369, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1370, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1371, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2022
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1372, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2289
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1373, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1374, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1375, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1375
Update 1376, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1377, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1378, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1379, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1380, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2002
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1381, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0800
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1382, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.4084
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1382
Update 1383, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1384, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1385, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1321
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1386, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1439
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1387, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1388, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0798
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1389, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1389
Update 1390, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0729
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1391, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2763
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1392, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1393, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0822
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1394, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1395, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0862
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1396, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1396
Update 1397, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1398, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1399, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2056
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1400, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1401, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1402, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1403, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1403
Update 1404, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1764
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1405, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2119
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1406, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0383
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1407, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0798
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1408, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1409, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1410, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1410
Update 1411, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1412, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0669
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1413, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1414, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1415, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2262
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1416, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1417, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1417
Update 1418, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1419, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1420, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0307
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1421, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1422, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1423, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1424, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1424
Update 1425, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2587
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1426, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1427, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1428, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1429, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1430, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0816
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1431, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1431
Update 1432, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1433, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1434, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1435, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1436, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1437, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0819
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1438, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.4537
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1438
Update 1439, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1440, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0886
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1441, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1442, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2008
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1443, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1444, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1445, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1445
Update 1446, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1447, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1694
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1448, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1483
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1449, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1450, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1451, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1452, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1452
Update 1453, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1454, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1455, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1456, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1457, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0862
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1458, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3307
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1459, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1459
Update 1460, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1532
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1461, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1508
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1462, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1463, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1464, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1465, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1466, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1466
Update 1467, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1468, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1469, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1470, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1471, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1472, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1473, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1473
Update 1474, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1475, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1585
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1476, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1477, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1478, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0520
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1479, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1480, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1480
Update 1481, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1527
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1482, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1483, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1484, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1485, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0460
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1486, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1487, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1487
Update 1488, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1489, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2438
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1490, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0484
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1491, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1492, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1493, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1559
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1494, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1494
Update 1495, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1496, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1497, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1498, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1379
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1499, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1527
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1500, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1501, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1501
Update 1502, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0873
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1503, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1504, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1505, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1506, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1820
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1507, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1508, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1508
Update 1509, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1510, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1511, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1512, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0640
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1513, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1514, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1515, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1515
Update 1516, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1517, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1518, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1519, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1972
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1520, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1521, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1522, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1522
Update 1523, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1642
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1524, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1525, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1526, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1527, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1528, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1529, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1529
Update 1530, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1531, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1532, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0791
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1533, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1534, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1396
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1535, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0805
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1536, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1536
Update 1537, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1538, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1539, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1540, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1541, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0739
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1542, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1543, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.5197
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1543
Update 1544, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1545, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1546, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1547, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0798
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1548, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1549, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1550, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1550
Update 1551, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1552, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1553, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1884
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1554, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1555, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2181
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1556, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1557, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0372
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1557
Update 1558, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1672
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1559, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1560, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1561, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1562, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1563, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1564, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1564
Update 1565, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1566, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1567, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0704
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1568, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1569, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1570, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1571, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1571
Update 1572, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1573, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2679
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1574, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1575, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1576, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1577, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1578, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1578
Update 1579, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1580, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1581, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1582, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1477
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1583, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1040
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1584, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1585, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1585
Update 1586, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1587, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1588, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2518
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1589, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1590, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1591, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1592, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1592
Update 1593, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1594, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0800
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1595, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1596, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1597, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1018
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1598, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1599, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1599
Update 1600, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1601, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1602, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1603, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0809
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1604, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1605, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1606, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1606
Update 1607, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1608, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1609, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1610, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1678
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1611, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1612, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1349
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1613, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1613
Update 1614, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1615, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0481
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1616, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1617, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1618, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1619, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
Update 1620, num samples collected 1700, FPS 15
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -1278.4431, l 200.0000, t 101.3162, TestReward -1293.0377
New EPOCH! 1620
Update 1621, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1622, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1623, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0519
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1624, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1625, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1626, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.3728
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1627, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1628, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1628
Update 1629, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1630, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2843
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1631, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1632, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0346
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1633, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2359
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1634, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1635, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1636, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1636
Update 1637, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0668
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1638, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1639, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1640, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1279
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1641, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1642, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1643, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2415
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1644, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1644
Update 1645, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1646, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1647, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1648, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1649, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1650, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1651, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1652, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1193
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1652
Update 1653, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1654, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1655, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1656, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1657, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1658, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1520
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1659, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1660, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1660
Update 1661, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1662, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1347
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1663, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1664, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1665, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1666, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2919
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1667, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1525
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1668, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1668
Update 1669, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1670, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2016
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1671, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1672, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1673, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1674, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1675, num samples collected 1950, FPS 15
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1676, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1676
Update 1677, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1678, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2356
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1679, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2019
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1680, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1681, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1682, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1683, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1684, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0357
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1684
Update 1685, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1686, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1687, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1688, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1689, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1690, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2503
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1691, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1692, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1692
Update 1693, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1694, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1695, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1696, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1697, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1698, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1699, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1700, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1700
Update 1701, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1645
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1702, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1703, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1704, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1705, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1706, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1707, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1708, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3484
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1708
Update 1709, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2174
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1710, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2008
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1711, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1712, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0731
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1713, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1714, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1715, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1716, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1716
Update 1717, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1718, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1719, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1720, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1721, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0754
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1722, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1723, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1724, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3769
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1724
Update 1725, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0881
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1726, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1727, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2971
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1728, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1653
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1729, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1730, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1731, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1732, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1732
Update 1733, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1734, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1735, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1690
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1736, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1737, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1738, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0689
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1739, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1600
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1740, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1740
Update 1741, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1742, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1743, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1744, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2332
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1745, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1746, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1649
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1747, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1386
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1748, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1081
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1748
Update 1749, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1750, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1751, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1752, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1753, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1754, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1755, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3848
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1756, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1756
Update 1757, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1758, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1759, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1760, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1761, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1762, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1763, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1764, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1764
Update 1765, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2021
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1766, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1767, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1768, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1769, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1770, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1771, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1772, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1772
Update 1773, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1774, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2121
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1775, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1776, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1777, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1394
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1778, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1779, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1141
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1780, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1780
Update 1781, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1782, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1783, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1173
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1784, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3564
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1785, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1786, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1787, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0785
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1788, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1788
Update 1789, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1790, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1791, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1792, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1793, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2754
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1794, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1795, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1796, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1077
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1796
Update 1797, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1798, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1799, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1800, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1691
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1801, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1199
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1802, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1803, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1804, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1804
Update 1805, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2675
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1806, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1807, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1815
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1808, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1170
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1809, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1810, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1811, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1812, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1812
Update 1813, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1814, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1815, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1816, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1817, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0835
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1818, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1819, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1820, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1820
Update 1821, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1822, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1823, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0778
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1824, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3315
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1825, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1826, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1193
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1827, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1828, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1828
Update 1829, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1830, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1831, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1832, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3357
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1833, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1834, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1835, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1836, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0357
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1836
Update 1837, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0923
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1838, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1839, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0702
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1840, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3413
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1841, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1842, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1843, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1844, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1128
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1844
Update 1845, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1846, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1847, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1848, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2860
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1849, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1850, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1851, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1429
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1852, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1852
Update 1853, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1854, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1855, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0392
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1856, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1203
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1857, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1858, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1859, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1860, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1860
Update 1861, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1862, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1863, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1466
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1864, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1607
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1865, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1866, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1867, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1868, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1868
Update 1869, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1870, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1871, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2319
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1872, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2307
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1873, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1874, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1875, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1876, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1876
Update 1877, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1878, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1879, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3054
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1880, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0350
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1881, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1882, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1883, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1884, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1884
Update 1885, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1506
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1886, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1887, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1888, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1889, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1890, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1891, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0921
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1892, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0446
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1892
Update 1893, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0422
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1894, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1895, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1896, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1897, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1898, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1899, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1900, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1900
Update 1901, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1902, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1903, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1904, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2272
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1905, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2090
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1906, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1907, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1293
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1908, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1908
Update 1909, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1910, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1911, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1912, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1913, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2065
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1914, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1915, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2132
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1916, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1916
Update 1917, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1918, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1919, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0826
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1920, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1845
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1921, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1922, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1923, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1924, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1924
Update 1925, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1926, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1927, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0952
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1928, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1929, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1344
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1930, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0885
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1931, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1932, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1932
Update 1933, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1934, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1935, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1936, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1937, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1938, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1939, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1940, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1940
Update 1941, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1942, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1943, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1944, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1945, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1946, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1947, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1948, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1948
Update 1949, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1950, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0874
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1951, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1952, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1953, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1954, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0396
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1955, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1956, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1956
Update 1957, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1958, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1959, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1960, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1849
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1961, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2033
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1962, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1963, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1964, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1964
Update 1965, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1969
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1966, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1967, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1968, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1969, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1970, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1971, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1015
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1972, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1972
Update 1973, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1012
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1974, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1975, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2527
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1976, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1977, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1978, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1979, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1980, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1980
Update 1981, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1982, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1983, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1984, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1985, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1986, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1987, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1988, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.4029
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1988
Update 1989, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1990, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2038
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1991, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1992, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1993, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0766
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1994, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1995, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1996, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1670
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 1996
Update 1997, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1998, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 1999, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2000, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2001, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2002, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2003, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2004, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2132
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2004
Update 2005, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2006, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2007, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2008, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2009, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2010, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0805
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2011, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2012, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1241
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2012
Update 2013, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2014, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2015, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0749
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2016, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2017, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2018, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2019, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2020, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2020
Update 2021, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0403
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2022, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2023, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2024, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2025, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2026, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2765
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2027, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2028, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2028
Update 2029, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2030, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2031, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1291
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2032, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2041
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2033, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2034, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2035, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0708
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2036, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2036
Update 2037, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0788
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2038, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0681
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2039, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2040, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2041, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2042, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2043, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1585
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2044, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2044
Update 2045, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2046, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2047, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2048, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0376
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2049, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.3974
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2050, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2051, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2052, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2052
Update 2053, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0686
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2054, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2161
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2055, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1339
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2056, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2057, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2058, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2059, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2060, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2060
Update 2061, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2062, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2063, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2064, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2065, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2066, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0875
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2067, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2068, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2068
Update 2069, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2070, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1351
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2071, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2088
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2072, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0717
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2073, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2074, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0419
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2075, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2076, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2076
Update 2077, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2078, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0805
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2079, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2080, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2081, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2082, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2083, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2084, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2084
Update 2085, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2086, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2087, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0704
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2088, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2089, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2090, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2091, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2092, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1054
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2092
Update 2093, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.1451
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2094, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2095, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2096, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2097, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2098, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2099, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
Update 2100, num samples collected 1950, FPS 14
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1083.1247, l 200.0000, t 120.1247, TestReward -1257.9952
New EPOCH! 2100
Update 2101, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1742
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2102, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0946
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2103, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2866
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2104, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2105, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2106, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2107, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2108, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2109, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2109
Update 2110, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3847
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2111, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2112, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2113, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2114, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2115, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0723
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2116, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4293
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2117, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2118, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2118
Update 2119, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2120, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2121, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1981
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2122, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2123, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2124, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2125, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2126, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2127, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4366
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2127
Update 2128, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1151
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2129, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2130, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2852
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2131, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3116
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2132, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2133, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1248
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2134, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2135, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2136, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2935
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2136
Update 2137, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2138, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2756
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2139, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2140, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2141, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2130
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2142, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2143, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2144, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2145, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2145
Update 2146, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2147, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1265
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2148, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2783
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2149, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2150, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2151, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1865
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2152, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2153, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2154, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3366
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2154
Update 2155, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2156, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1304
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2157, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0974
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2158, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2065
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2159, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2839
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2160, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1840
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2161, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2162, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0917
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2163, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0307
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2163
Update 2164, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2165, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0827
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2166, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2167, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0803
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2168, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1985
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2169, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.5326
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2170, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2171, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2172, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2172
Update 2173, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2174, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2175, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2176, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0696
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2177, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2178, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2179, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3625
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2180, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2574
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2181, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2181
Update 2182, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2781
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2183, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2184, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2185, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2186, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2187, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2188, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1896
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2189, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2190, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2190
Update 2191, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2192, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2193, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2194, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0703
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2195, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1965
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2196, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2197, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0796
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2198, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1981
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2199, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2199
Update 2200, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2201, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2202, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3075
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2203, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2204, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2205, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2206, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2207, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1659
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2208, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2208
Update 2209, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2287
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2210, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2211, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2212, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2213, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2543
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2214, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2215, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2216, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2217, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2217
Update 2218, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1003
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2219, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2220, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2221, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2008
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2222, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2223, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2224, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2225, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.5333
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2226, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2226
Update 2227, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2228, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2229, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0961
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2230, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2231, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2232, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2233, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2234, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2235, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2235
Update 2236, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2237, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2238, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3191
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2239, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2240, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2559
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2241, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2242, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1313
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2243, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2244, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2244
Update 2245, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1975
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2246, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2469
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2247, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2248, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2249, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2250, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2251, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1732
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2252, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2253, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2253
Update 2254, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2255, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2256, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2257, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2258, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2259, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2620
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2260, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2759
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2261, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1009
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2262, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2262
Update 2263, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2264, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2265, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2266, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0408
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2267, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2268, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0891
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2269, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1157
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2270, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4755
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2271, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2271
Update 2272, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2273, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2274, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2275, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2276, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2277, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2278, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2279, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2280, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2280
Update 2281, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2282, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2047
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2283, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2720
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2284, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2285, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2286, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2287, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2288, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2289, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.5703
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2289
Update 2290, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4012
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2291, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2292, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2293, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2294, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3436
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2295, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2296, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2297, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0834
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2298, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2298
Update 2299, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2300, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2301, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2302, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2303, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2304, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2305, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4425
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2306, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3823
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2307, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2307
Update 2308, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2309, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2310, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2311, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3498
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2312, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2621
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2313, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2314, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2315, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2316, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2316
Update 2317, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1967
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2318, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2319, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0672
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2320, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3452
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2321, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2322, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2323, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2324, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2325, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2325
Update 2326, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2654
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2327, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2328, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2329, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2330, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2331, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3730
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2332, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2333, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2334, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2334
Update 2335, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1546
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2336, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2528
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2337, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2338, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2339, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2477
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2340, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2341, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2010
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2342, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2343, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2343
Update 2344, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2378
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2345, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2346, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2347, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2348, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2368
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2349, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2350, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2351, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2352, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2352
Update 2353, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2354, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2558
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2355, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2356, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2357, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2358, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2359, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1936
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2360, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1231
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2361, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2879
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2361
Update 2362, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2363, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2789
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2364, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2330
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2365, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2366, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2367, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2368, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2369, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1890
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2370, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2370
Update 2371, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2372, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2373, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2374, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2375, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2376, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3449
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2377, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3081
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2378, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0728
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2379, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2379
Update 2380, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1752
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2381, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2382, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2383, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3581
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2384, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2385, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0297
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2386, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2387, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2388, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2388
Update 2389, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2390, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2391, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2392, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2393, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2394, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2395, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2396, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2397, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2397
Update 2398, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2399, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2400, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2401, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2402, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2403, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2404, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1825
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2405, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4810
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2406, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2406
Update 2407, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2408, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2569
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2409, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2410, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2411, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2412, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2413, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2589
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2414, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2415, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3131
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2415
Update 2416, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2417, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0691
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2418, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2419, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2420, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1121
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2421, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2422, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1682
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2423, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2424, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3066
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2424
Update 2425, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2426, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2427, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2428, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2429, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.4162
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2430, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2431, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2432, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2433, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2433
Update 2434, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2435, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2436, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2437, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2438, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2403
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2439, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1595
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2440, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2441, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2629
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2442, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2442
Update 2443, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2444, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2445, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2446, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2447, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2448, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2397
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2449, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2450, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2451, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2451
Update 2452, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2681
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2453, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3702
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2454, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2455, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2456, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2457, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2458, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2459, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2460, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2460
Update 2461, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2462, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3841
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2463, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2464, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2465, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2466, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2467, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2468, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1979
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2469, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2469
Update 2470, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2417
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2471, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2472, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2473, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2185
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2474, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2475, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2476, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2477, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0827
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2478, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2478
Update 2479, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2727
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2480, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2481, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2482, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2483, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2484, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2485, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2486, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3443
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2487, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2487
Update 2488, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2961
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2489, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2490, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2491, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2492, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2493, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2494, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2495, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2654
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2496, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2496
Update 2497, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2498, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2499, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2500, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2501, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1940
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2502, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0403
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2503, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2504, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2505, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2505
Update 2506, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2167
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2507, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2508, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2509, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2510, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2511, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2768
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2512, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2513, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2514, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2514
Update 2515, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2516, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2517, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2518, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2519, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2520, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2521, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2222
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2522, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2523, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2523
Update 2524, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2525, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2664
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2526, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2527, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2528, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2529, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1147
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2530, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2531, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2532, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2532
Update 2533, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2338
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2534, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2535, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2536, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2537, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2992
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2538, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2539, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2540, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2541, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2541
Update 2542, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1854
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2543, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1906
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2544, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2307
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2545, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2546, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1882
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2547, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2548, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2549, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2550, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2550
Update 2551, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2552, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2553, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2554, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2555, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2556, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1804
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2557, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1852
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2558, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2559, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2559
Update 2560, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2561, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2562, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2563, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2986
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2564, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2565, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2566, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2567, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2568, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2568
Update 2569, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2570, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2571, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1160
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2572, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2573, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2574, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1837
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2575, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2576, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2577, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3323
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2577
Update 2578, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2330
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2579, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3072
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2580, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2581, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2582, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2583, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2584, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0627
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2585, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2586, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2586
Update 2587, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2588, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2589, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2590, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3051
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2591, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1400
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2592, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2056
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2593, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0332
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2594, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2595, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2595
Update 2596, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2597, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2598, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2599, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2600, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2601, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1475
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2602, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2603, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2604, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3755
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2604
Update 2605, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2606, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2607, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2608, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1102
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2609, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2610, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2611, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2612, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1870
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2613, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3667
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2613
Update 2614, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2615, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2616, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3600
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2617, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2618, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2619, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2620, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2621, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2227
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2622, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2622
Update 2623, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2624, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2625, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2626, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2627, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2628, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3846
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2629, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.1909
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2630, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2631, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2631
Update 2632, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2633, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2139
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2634, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2635, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2636, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.3011
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2637, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2638, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2639, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
Update 2640, num samples collected 2200, FPS 14
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1167.2812, l 200.0000, t 141.3350, TestReward -1433.9915
New EPOCH! 2640
Update 2641, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2642, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2643, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0754
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2644, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2645, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1238
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2646, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1707
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2647, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1862
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2648, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2649, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3137
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2650, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2650
Update 2651, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2652, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2653, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3051
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2654, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2655, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2656, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2473
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2657, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2658, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2659, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2660, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2660
Update 2661, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2662, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2663, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3802
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2664, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2665, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2666, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2667, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2668, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2397
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2669, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2670, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2670
Update 2671, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2672, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2673, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2674, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1689
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2675, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3850
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2676, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2677, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2678, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2679, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2680, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2680
Update 2681, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2682, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2683, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2684, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1420
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2685, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2686, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2687, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2688, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2513
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2689, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1975
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2690, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0947
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2690
Update 2691, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2692, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2493
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2693, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2694, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2695, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2696, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2697, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1472
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2698, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2699, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2700, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.5183
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2700
Update 2701, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0327
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2702, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2703, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2704, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2705, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0952
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2706, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2707, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2708, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2709, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2330
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2710, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2710
Update 2711, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2712, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2713, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2714, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2715, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2716, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2717, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2718, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2719, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2720, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2720
Update 2721, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2722, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2723, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2724, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2725, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2726, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2727, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2321
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2728, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1333
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2729, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2730, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2811
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2730
Update 2731, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2732, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2369
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2733, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1057
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2734, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2735, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2736, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2737, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2738, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1683
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2739, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2740, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2740
Update 2741, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2742, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2743, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2744, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2745, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3001
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2746, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1674
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2747, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2748, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3608
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2749, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0520
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2750, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2750
Update 2751, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1312
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2752, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2753, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2754, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2473
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2755, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2756, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2757, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1950
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2758, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2683
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2759, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2760, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2760
Update 2761, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2762, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1605
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2763, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2764, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0985
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2765, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2766, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3786
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2767, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2768, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2769, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2770, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2770
Update 2771, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2772, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0427
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2773, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2774, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2775, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2776, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2777, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2778, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2779, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2780, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3287
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2780
Update 2781, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2794
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2782, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2783, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2784, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2785, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2786, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2787, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2788, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2789, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.5300
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2790, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2790
Update 2791, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2792, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2793, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2794, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2795, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2796, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1951
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2797, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2798, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2799, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3920
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2800, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2800
Update 2801, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2802, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2700
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2803, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2804, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2805, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2806, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2807, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2808, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2809, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2810, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.4736
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2810
Update 2811, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2812, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2813, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2814, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2639
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2815, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2486
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2816, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1166
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2817, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2818, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2819, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2820, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0382
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2820
Update 2821, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2822, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2823, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2824, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2825, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2826, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3336
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2827, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2828, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2829, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2830, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2830
Update 2831, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2832, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2833, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0980
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2834, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2835, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3264
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2836, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2837, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2838, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2839, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1919
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2840, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2840
Update 2841, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0992
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2842, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2843, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2844, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2845, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2846, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0955
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2847, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3757
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2848, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2970
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2849, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2850, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2850
Update 2851, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2852, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1079
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2853, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2854, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2855, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2856, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2857, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3751
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2858, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2859, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2860, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2860
Update 2861, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2862, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2863, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2864, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2865, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3592
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2866, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2867, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1338
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2868, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2869, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2870, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2870
Update 2871, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2872, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0377
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2873, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1828
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2874, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2706
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2875, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2354
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2876, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2877, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1242
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2878, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2879, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2880, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0674
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2880
Update 2881, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2882, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2883, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3385
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2884, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0908
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2885, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2886, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2887, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2021
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2888, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2889, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2890, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2890
Update 2891, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2892, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1158
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2893, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2894, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2895, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0350
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2896, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2897, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2898, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1944
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2899, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0443
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2900, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2900
Update 2901, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2902, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2903, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2904, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2905, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2906, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2907, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2908, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2909, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3231
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2910, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2910
Update 2911, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2912, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2913, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2914, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1795
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2915, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1963
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2916, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2917, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2918, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3171
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2919, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2920, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2920
Update 2921, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2922, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2923, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2924, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2925, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2926, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2927, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2928, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2100
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2929, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.5529
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2930, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2930
Update 2931, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2448
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2932, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2933, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2934, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2935, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2936, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2937, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1857
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2938, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2939, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2940, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2940
Update 2941, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2942, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2943, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2944, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2945, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2946, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2947, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2948, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2949, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2950, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2504
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2950
Update 2951, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2952, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2953, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2954, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.4056
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2955, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2956, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2957, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0382
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2958, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2959, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2960, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2960
Update 2961, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2962, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2963, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0682
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2964, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2965, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2966, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2188
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2967, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2968, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1240
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2969, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2539
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2970, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2970
Update 2971, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0859
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2972, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2973, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2275
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2974, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2975, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2976, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2977, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2978, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1873
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2979, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2980, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2524
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2980
Update 2981, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1417
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2982, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2983, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2984, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2985, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2986, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2987, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2988, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2989, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3123
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2990, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 2990
Update 2991, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3250
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2992, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2993, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2994, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2786
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2995, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2996, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2997, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2998, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 2999, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3000, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3000
Update 3001, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3002, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3003, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2319
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3004, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3005, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3006, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3007, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3008, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3009, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3010, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3010
Update 3011, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3012, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2080
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3013, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0908
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3014, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3015, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3016, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3017, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1207
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3018, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3019, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2286
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3020, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3020
Update 3021, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3022, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3023, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3237
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3024, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3295
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3025, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3026, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3027, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3028, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3029, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3030, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3030
Update 3031, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3032, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3033, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1862
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3034, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3035, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3036, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3037, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3038, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3039, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3040, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3040
Update 3041, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3042, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3043, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3044, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3045, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1911
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3046, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3047, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3048, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0460
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3049, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1016
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3050, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1929
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3050
Update 3051, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0297
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3052, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1405
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3053, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3054, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3055, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3056, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3057, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3171
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3058, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3059, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1338
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3060, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3060
Update 3061, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1938
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3062, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3063, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3064, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3065, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2513
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3066, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3067, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3068, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3069, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3070, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3070
Update 3071, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3072, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3073, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3074, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1079
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3075, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3076, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0868
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3077, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3078, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2985
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3079, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3080, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3080
Update 3081, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3082, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3083, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3084, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3085, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3086, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3087, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3088, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3089, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1884
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3090, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2401
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3090
Update 3091, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0496
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3092, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3093, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3094, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3095, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3096, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3097, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3098, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3099, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.4030
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3100, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3100
Update 3101, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3102, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3103, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3104, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1978
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3105, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3106, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3107, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3108, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3948
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3109, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0356
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3110, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1276
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3110
Update 3111, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3112, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3113, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3114, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1369
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3115, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3116, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3117, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.4323
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3118, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1210
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3119, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3120, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3120
Update 3121, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3122, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3123, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3124, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3125, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3126, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3932
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3127, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3128, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3129, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1819
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3130, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3130
Update 3131, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3132, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3133, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3134, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3135, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3136, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2957
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3137, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3138, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3139, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3140, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3140
Update 3141, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1374
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3142, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3143, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3144, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3145, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1000
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3146, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0822
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3147, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3148, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3149, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3150, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3150
Update 3151, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3152, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1880
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3153, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3154, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3155, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3156, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1906
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3157, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3158, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3159, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3160, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3160
Update 3161, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2737
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3162, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3163, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3164, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3165, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3166, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1054
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3167, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1607
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3168, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3169, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3170, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3170
Update 3171, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2343
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3172, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1790
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3173, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3174, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3175, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3176, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0850
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3177, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3178, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3179, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3180, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3180
Update 3181, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3277
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3182, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.4120
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3183, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3184, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3185, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3186, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3187, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3188, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3189, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3190, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3190
Update 3191, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3192, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3193, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3194, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3195, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3196, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3197, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3198, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2587
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3199, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.3060
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3200, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1412
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3200
Update 3201, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3202, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2743
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3203, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3204, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0837
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3205, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3206, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3207, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3208, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1367
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3209, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3210, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3210
Update 3211, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1258
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3212, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3213, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3214, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3215, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3216, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.4133
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3217, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3218, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3219, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3220, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3220
Update 3221, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3222, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3223, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0979
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3224, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3225, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3226, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1452
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3227, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3228, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3229, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1983
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3230, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1899
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3230
Update 3231, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3232, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0831
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3233, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1764
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3234, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3235, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3236, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3237, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3238, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3239, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
Update 3240, num samples collected 2450, FPS 14
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1433.8586, l 200.0000, t 163.0348, TestReward -1152.7526
New EPOCH! 3240
Update 3241, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3242, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3243, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3244, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1356
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3245, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3246, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3247, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1216
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3248, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0362
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3249, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3250, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3251, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1665
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3251
Update 3252, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3253, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3254, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3255, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3256, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3257, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3258, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3259, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2949
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3260, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3261, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1517
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3262, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3262
Update 3263, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3264, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3265, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3266, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3267, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3268, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3269, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2042
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3270, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3271, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3272, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3273, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4149
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3273
Update 3274, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3275, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3276, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3277, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3278, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3279, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3280, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3502
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3281, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3282, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3283, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3284, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3284
Update 3285, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3286, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3287, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3288, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2334
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3289, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3290, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3251
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3291, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3292, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1050
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3293, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3294, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3295, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3295
Update 3296, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3297, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1765
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3298, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3299, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3300, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1731
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3301, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2385
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3302, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3303, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3304, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2269
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3305, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3306, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3306
Update 3307, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3308, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3309, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0763
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3310, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3880
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3311, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3312, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1674
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3313, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3314, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3315, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3316, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3317, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4011
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3317
Update 3318, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3319, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3320, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3321, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3322, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3323, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0884
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3324, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3325, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2272
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3326, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1059
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3327, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3328, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3328
Update 3329, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3330, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3331, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3332, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3333, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3334, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1790
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3335, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0846
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3336, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1254
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3337, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3318
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3338, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3339, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3339
Update 3340, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3341, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2191
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3342, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3343, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3344, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3345, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3346, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3347, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3348, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1312
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3349, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3350, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4843
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3350
Update 3351, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3352, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3353, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3354, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3355, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3356, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3015
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3357, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3358, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3359, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3360, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3361, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3069
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3361
Update 3362, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3363, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3364, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3365, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3366, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3367, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3368, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3369, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1218
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3370, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3371, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3048
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3372, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3372
Update 3373, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2567
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3374, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3375, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3376, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3377, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0846
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3378, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3379, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3380, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3381, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1345
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3382, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3383, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3939
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3383
Update 3384, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1258
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3385, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3386, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3387, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3388, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2368
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3389, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3390, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3391, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3392, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3393, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3394, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1325
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3394
Update 3395, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3396, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3397, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3398, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3399, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3400, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3401, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1148
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3402, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3403, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2148
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3404, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2493
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3405, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2301
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3405
Update 3406, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3407, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3408, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2046
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3409, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3410, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3411, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3412, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3413, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3482
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3414, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0782
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3415, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3416, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3416
Update 3417, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2235
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3418, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0737
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3419, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3420, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3421, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3422, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3423, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3424, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1218
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3425, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3426, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3427, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3983
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3427
Update 3428, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3429, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3430, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3431, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1835
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3432, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3433, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3434, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3435, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3436, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3437, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3438, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3438
Update 3439, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3440, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3441, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3442, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3443, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3444, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.3489
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3445, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0374
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3446, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3447, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3448, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2899
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3449, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3449
Update 3450, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3451, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3452, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3453, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2902
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3454, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3455, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1711
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3456, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3457, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3458, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3459, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3460, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3460
Update 3461, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3462, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3463, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3464, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3465, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1653
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3466, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3467, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3468, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4176
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3469, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3470, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3471, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1850
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3471
Update 3472, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3473, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3474, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3475, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3476, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1225
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3477, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3478, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2721
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3479, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2913
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3480, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3481, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3482, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3482
Update 3483, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3484, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2003
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3485, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3486, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3487, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3488, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1231
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3489, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3490, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3491, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0829
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3492, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4310
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3493, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3493
Update 3494, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3495, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3496, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3497, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4566
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3498, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3499, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0793
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3500, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.2167
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3501, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3502, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3503, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1369
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3504, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3504
Update 3505, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3506, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3507, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1735
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3508, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3509, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3510, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.1224
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3511, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.4461
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3512, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3513, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3514, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3515, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3515
Update 3516, num samples collected 2700, FPS 14
  Algorithm: train_loss 0.0443
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3517, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3518, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3519, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3520, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3521, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2061
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3522, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3523, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3524, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3525, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3526, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3526
Update 3527, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3528, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1663
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3529, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3530, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3531, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3532, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3533, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3272
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3534, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3535, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3536, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3537, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3828
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3537
Update 3538, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3539, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3540, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3541, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2033
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3542, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.4670
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3543, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3544, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3545, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3546, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3547, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3548, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3548
Update 3549, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3550, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3551, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3552, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3553, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3554, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3555, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3556, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3557, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2769
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3558, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2717
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3559, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3559
Update 3560, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3561, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3562, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3563, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1110
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3564, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2329
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3565, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3566, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3567, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3568, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3569, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3570, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.6152
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3570
Update 3571, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3572, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0911
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3573, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1039
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3574, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3575, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3576, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3454
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3577, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1817
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3578, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3579, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3580, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3581, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3873
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3581
Update 3582, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3583, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3584, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3585, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3586, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0749
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3587, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3588, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3589, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3590, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0327
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3591, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.4305
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3592, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3592
Update 3593, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3594, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3595, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1034
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3596, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3597, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3598, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3599, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3600, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3601, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2076
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3602, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3603, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3603
Update 3604, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2963
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3605, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1696
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3606, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3607, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3608, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3609, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3610, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3611, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3612, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3613, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3614, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1258
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3614
Update 3615, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3616, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3617, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3618, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3619, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.5520
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3620, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3621, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3622, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3623, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3624, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3625, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3625
Update 3626, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3627, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3628, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3629, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1665
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3630, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3631, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3632, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3377
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3633, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3634, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3635, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2247
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3636, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3636
Update 3637, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3638, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2667
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3639, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3640, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2707
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3641, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3642, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3643, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3644, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3645, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3646, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3647, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3647
Update 3648, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3649, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3650, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3651, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3956
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3652, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0818
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3653, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3654, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2776
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3655, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0489
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3656, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3657, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1219
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3658, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3658
Update 3659, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3660, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3661, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3662, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2298
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3663, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3664, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3665, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0404
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3666, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1055
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3667, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3668, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3624
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3669, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3669
Update 3670, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3671, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3672, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3673, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3674, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3675, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3676, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3677, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3678, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.4684
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3679, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0812
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3680, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3680
Update 3681, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0729
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3682, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3683, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1653
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3684, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3685, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3686, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3687, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3688, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3689, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3690, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3691, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3691
Update 3692, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3693, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3694, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3425
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3695, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3696, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3098
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3697, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3698, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3699, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3700, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2255
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3701, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3702, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3702
Update 3703, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3889
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3704, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3705, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0764
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3706, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3707, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1048
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3708, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3709, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3710, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3711, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3712, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3713, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3713
Update 3714, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0783
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3715, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1101
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3716, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3717, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3718, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3719, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3762
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3720, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3721, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3722, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3723, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0670
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3724, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3705
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3724
Update 3725, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1127
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3726, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3727, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3728, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3729, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2019
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3730, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3731, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1620
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3732, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3733, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1350
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3734, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2786
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3735, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3735
Update 3736, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3737, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3738, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3739, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2018
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3740, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2173
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3741, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3742, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3743, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3744, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3745, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2173
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3746, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3746
Update 3747, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1931
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3748, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3749, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3750, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1790
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3751, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2638
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3752, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3753, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3754, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3755, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3756, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3757, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3757
Update 3758, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3759, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3760, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2836
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3761, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3762, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3763, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3764, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3765, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3766, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3767, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3768, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1908
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3768
Update 3769, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3770, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3771, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1645
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3772, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3773, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3774, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0793
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3775, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3776, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3777, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2213
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3778, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3779, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3779
Update 3780, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3781, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0744
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3782, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3783, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3784, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3785, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3786, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3787, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3788, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3789, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.4703
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3790, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3790
Update 3791, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1244
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3792, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3793, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3794, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3795, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3532
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3796, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3797, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2629
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3798, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3799, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1035
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3800, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3801, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3801
Update 3802, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3803, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3804, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3805, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1017
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3806, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3807, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3808, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3809, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1802
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3810, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2122
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3811, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3812, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3812
Update 3813, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1003
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3814, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3815, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3816, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3817, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3818, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3819, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2607
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3820, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3821, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1751
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3822, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3141
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3823, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3823
Update 3824, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3825, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3826, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1972
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3827, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3828, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.4520
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3829, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3830, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3831, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0822
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3832, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3833, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3834, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0414
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3834
Update 3835, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3836, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2043
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3837, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3838, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3839, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3840, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0464
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3841, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3842, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3843, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3844, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3845, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3057
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3845
Update 3846, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3847, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3848, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3849, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3850, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3851, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3852, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1069
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3853, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2904
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3854, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3855, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0833
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3856, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3745
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3856
Update 3857, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3858, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3859, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3860, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1037
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3861, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3862, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3943
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3863, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3864, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3865, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3866, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3867, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3867
Update 3868, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3869, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3870, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3871, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3872, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3873, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3874, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3875, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1693
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3876, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1152
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3877, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3878, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3878
Update 3879, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3880, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3881, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3882, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0681
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3883, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3884, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2919
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3885, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3886, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1659
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3887, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3888, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.3110
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3889, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3889
Update 3890, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3891, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3892, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2778
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3893, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3894, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3895, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3896, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3897, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3898, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3899, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0737
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
Update 3900, num samples collected 2700, FPS 13
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -1300.0863, l 200.0000, t 182.7702, TestReward -1182.3719
New EPOCH! 3900
Update 3901, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4507
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3902, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3903, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3904, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3905, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3906, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3907, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1207
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3908, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3909, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2679
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3910, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3911, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3912, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3912
Update 3913, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4270
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3914, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1525
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3915, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3916, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3917, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3918, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3919, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3920, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3921, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3922, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3923, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3924, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3924
Update 3925, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3926, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0985
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3927, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3928, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3929, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3019
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3930, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3931, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2425
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3932, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3933, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3934, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3935, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3936, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3936
Update 3937, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3938, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3939, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3940, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3941, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2041
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3942, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3943, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3944, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2156
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3945, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1162
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3946, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3947, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3948, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2026
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3948
Update 3949, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3950, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3951, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3952, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3953, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3954, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.5637
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3955, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3956, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1027
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3957, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3958, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3959, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3960, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3960
Update 3961, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3962, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3963, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2884
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3964, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3965, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3966, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3967, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3968, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3969, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3970, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3971, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3972, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3972
Update 3973, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3974, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3975, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3976, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3977, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3978, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4579
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3979, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0934
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3980, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3981, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3982, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3983, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3984, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3984
Update 3985, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2437
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3986, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1897
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3987, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3802
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3988, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3989, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3990, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3991, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3992, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3993, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3994, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3995, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3996, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 3996
Update 3997, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3998, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1097
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 3999, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4300
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4000, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4001, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4002, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4003, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4004, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4005, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2380
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4006, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4007, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1940
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4008, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2221
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4008
Update 4009, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4010, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2794
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4011, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4012, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4013, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4014, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4015, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4016, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4017, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4018, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4019, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4020, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4020
Update 4021, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4022, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1128
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4023, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0940
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4024, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4025, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4026, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4027, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4028, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2887
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4029, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2481
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4030, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4031, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4032, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4032
Update 4033, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4034, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4035, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4036, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4037, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4038, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4039, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4040, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4041, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3763
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4042, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4043, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4044, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4044
Update 4045, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1060
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4046, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2965
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4047, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4048, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4049, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4050, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4051, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4052, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4053, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3882
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4054, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1552
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4055, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1205
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4056, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4056
Update 4057, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4058, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4059, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2568
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4060, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4061, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4062, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4063, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4064, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4065, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2896
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4066, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2035
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4067, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4068, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4068
Update 4069, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4070, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4071, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4072, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4073, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0866
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4074, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4075, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4076, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4077, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1657
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4078, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4079, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4080, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4049
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4080
Update 4081, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4082, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4083, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3034
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4084, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4085, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4086, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1933
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4087, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4088, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4089, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4090, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4091, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4092, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4092
Update 4093, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0897
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4094, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4095, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3732
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4096, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4097, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4098, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4099, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4100, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4101, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0697
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4102, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4103, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4104, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4104
Update 4105, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4106, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4104
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4107, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4108, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4109, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4110, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4111, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4112, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1117
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4113, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4114, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4115, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0706
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4116, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4116
Update 4117, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4118, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4119, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0923
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4120, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1773
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4121, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4291
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4122, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4123, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4124, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4125, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3608
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4126, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4127, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4128, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4128
Update 4129, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4130, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4131, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4132, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4133, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4134, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4135, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4136, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2869
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4137, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3451
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4138, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4139, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4140, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4401
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4140
Update 4141, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4142, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4143, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4144, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0627
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4145, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4146, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4147, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4148, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4149, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2815
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4150, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4151, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4152, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2823
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4152
Update 4153, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4154, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4155, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4156, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1916
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4157, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4158, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4159, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4178
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4160, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4161, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4162, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4163, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4164, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4164
Update 4165, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4166, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1827
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4167, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2217
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4168, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4169, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4170, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4171, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1963
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4172, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4173, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4174, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4175, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3362
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4176, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4176
Update 4177, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4178, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2642
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4179, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4180, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0627
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4181, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4182, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4183, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4184, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4185, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4186, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4187, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4188, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4188
Update 4189, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3353
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4190, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4191, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1012
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4192, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0708
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4193, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2048
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4194, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4195, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4196, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4197, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4198, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4199, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4200, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4467
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4200
Update 4201, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4202, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2813
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4203, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3354
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4204, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4205, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4206, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4207, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4208, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0753
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4209, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4210, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2400
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4211, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1351
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4212, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4212
Update 4213, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4214, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4215, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4216, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4217, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4218, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2457
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4219, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4220, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4221, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4222, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2518
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4223, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4224, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4224
Update 4225, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4226, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4227, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4228, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4229, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4230, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2901
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4231, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2441
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4232, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4233, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4234, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4235, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0983
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4236, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3783
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4236
Update 4237, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4238, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0682
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4239, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4240, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4241, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4242, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4243, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4244, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0826
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4245, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4246, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4247, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4248, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4248
Update 4249, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4250, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4251, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0709
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4252, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4253, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1219
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4254, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1976
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4255, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4256, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4257, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1605
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4258, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2210
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4259, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4260, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4260
Update 4261, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4262, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2832
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4263, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4264, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4101
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4265, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4266, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1054
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4267, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4268, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4269, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4270, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4271, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4272, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4272
Update 4273, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4274, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2846
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4275, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0392
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4276, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4277, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4278, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4279, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4280, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4281, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0932
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4282, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1707
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4283, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4284, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4284
Update 4285, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4286, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4287, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1973
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4288, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4289, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4290, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1506
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4291, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4292, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0954
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4293, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4294, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4295, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3411
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4296, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4296
Update 4297, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4298, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4299, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4300, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4301, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4302, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0897
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4303, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4304, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4305, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0859
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4306, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1986
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4307, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4308, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.6484
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4308
Update 4309, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4310, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4311, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4312, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4313, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1144
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4314, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4315, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4316, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4317, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4318, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4350
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4319, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4320, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4320
Update 4321, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4322, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4223
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4323, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4324, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4325, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4326, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4327, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0765
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4328, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4329, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4330, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1624
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4331, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4332, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4332
Update 4333, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4334, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4335, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1058
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4336, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4337, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4338, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4339, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4340, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4341, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4342, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3408
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4343, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4344, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4344
Update 4345, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4346, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4347, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4348, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4349, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4350, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4351, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3441
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4352, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4353, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3358
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4354, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4355, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4356, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4356
Update 4357, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4358, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1444
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4359, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2813
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4360, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1991
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4361, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4362, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4363, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4364, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4365, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4366, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4367, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4368, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4368
Update 4369, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4370, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4371, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4372, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4373, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4374, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4375, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4376, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4377, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4378, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4379, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4380, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4380
Update 4381, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4382, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4383, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4384, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1987
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4385, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4386, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4387, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4388, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4389, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4390, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4391, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1785
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4392, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4392
Update 4393, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4394, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4395, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4396, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0691
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4397, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4398, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2895
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4399, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4400, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2608
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4401, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4402, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4403, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4404, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4404
Update 4405, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4406, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4407, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4408, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2389
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4409, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4410, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4411, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4412, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4413, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4414, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3479
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4415, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0910
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4416, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4416
Update 4417, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4418, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4419, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4420, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4421, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0727
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4422, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4423, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3278
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4424, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4425, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4426, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4427, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4428, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2889
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4428
Update 4429, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4430, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4431, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4432, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4433, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4434, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1410
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4435, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4436, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4437, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4438, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3665
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4439, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4440, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4440
Update 4441, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4442, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4443, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4444, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1909
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4445, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4171
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4446, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1474
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4447, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4448, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4449, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1592
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4450, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4451, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4452, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4452
Update 4453, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4454, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4455, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4456, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4457, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4458, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4459, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4123
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4460, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4461, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4462, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4463, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0901
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4464, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4464
Update 4465, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3163
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4466, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4467, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4468, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4469, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4470, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4471, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0672
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4472, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4473, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4474, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4475, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4476, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.7013
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4476
Update 4477, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4478, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4479, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4480, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4481, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4482, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2402
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4483, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4484, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0720
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4485, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4486, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4487, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3795
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4488, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4488
Update 4489, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4490, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4491, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4492, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2239
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4493, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4494, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4495, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4496, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4941
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4497, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4498, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4499, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4500, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2031
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4500
Update 4501, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4502, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2302
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4503, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4504, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4505, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4506, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4507, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4508, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1785
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4509, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4510, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1878
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4511, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4512, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4512
Update 4513, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4514, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4515, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4516, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4517, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1734
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4518, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1875
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4519, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4520, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4521, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4522, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4523, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4524, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4395
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4524
Update 4525, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4526, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4527, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4528, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4529, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4530, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1947
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4531, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1943
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4532, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2697
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4533, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4534, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4535, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4536, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4536
Update 4537, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4538, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1753
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4539, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4540, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4541, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4542, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4543, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1994
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4544, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4545, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4546, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4547, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1025
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4548, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2808
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4548
Update 4549, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1474
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4550, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1899
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4551, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4552, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4553, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4554, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4555, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4556, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0798
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4557, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4558, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3009
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4559, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1823
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4560, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0332
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4560
Update 4561, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4562, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1647
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4563, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4564, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4565, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4566, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4567, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4568, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4569, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4570, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1936
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4571, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4572, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4572
Update 4573, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2643
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4574, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4575, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4576, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4577, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4578, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4579, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4580, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1546
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4581, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0632
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4582, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2412
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4583, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4584, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3579
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4584
Update 4585, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2312
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4586, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4587, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4588, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4218
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4589, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4590, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4591, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4592, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4593, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4594, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4595, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4596, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4596
Update 4597, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1411
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4598, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1861
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4599, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4600, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4601, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4602, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4603, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4604, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4605, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4606, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.4116
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4607, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4608, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4608
Update 4609, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4610, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4611, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4612, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4613, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4614, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4615, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4616, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.3393
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4617, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4618, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4619, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
Update 4620, num samples collected 2950, FPS 13
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1068.5532, l 200.0000, t 202.9999, TestReward -1245.0283
New EPOCH! 4620
Update 4621, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4622, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1725
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4623, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4624, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4625, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4626, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2600
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4627, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2832
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4628, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4629, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4630, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4631, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4632, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3753
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4633, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4633
Update 4634, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4635, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4636, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4637, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4638, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4585
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4639, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4640, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4641, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4642, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4643, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4644, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4645, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0885
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4646, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4646
Update 4647, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4648, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4649, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4650, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4651, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4652, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1496
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4653, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3089
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4654, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4655, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4656, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2425
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4657, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3180
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4658, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4659, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4659
Update 4660, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1915
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4661, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0850
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4662, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1492
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4663, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4664, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1444
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4665, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4666, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4667, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3498
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4668, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2141
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4669, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4670, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4671, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4672, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4672
Update 4673, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4674, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4675, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2810
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4676, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4677, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4678, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4679, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4680, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4681, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4682, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2448
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4683, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4684, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2312
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4685, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4685
Update 4686, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4687, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2462
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4688, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4689, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4690, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1998
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4691, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4692, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4693, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0873
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4694, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4695, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2055
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4696, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3018
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4697, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4698, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2955
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4698
Update 4699, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4700, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1117
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4701, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1693
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4702, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4703, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4704, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4705, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2003
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4706, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4707, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4708, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1858
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4709, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4710, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4711, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4711
Update 4712, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4713, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4714, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4715, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4716, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4717, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4718, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4719, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4720, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4721, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4722, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4723, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.5430
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4724, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4724
Update 4725, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4726, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4727, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4728, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3627
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4729, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4730, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4731, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4732, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4733, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4734, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1175
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4735, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1659
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4736, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0901
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4737, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4737
Update 4738, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4739, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4740, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4741, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4742, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4222
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4743, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4744, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4745, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4746, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4747, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4748, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4749, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4750, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4750
Update 4751, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4752, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4753, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4754, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1640
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4755, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4756, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4757, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4758, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4759, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2240
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4760, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4761, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4762, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2537
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4763, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4763
Update 4764, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4765, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4766, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4767, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4768, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2252
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4769, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1918
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4770, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4771, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3441
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4772, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4773, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4774, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1029
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4775, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1642
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4776, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4776
Update 4777, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1490
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4778, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1998
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4779, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4780, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1943
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4781, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4782, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4783, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4784, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4785, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4786, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4787, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4788, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4789, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4789
Update 4790, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1918
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4791, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4792, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4793, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2068
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4794, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4795, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4796, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4797, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4798, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4209
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4799, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4800, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4801, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4802, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4802
Update 4803, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2288
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4804, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2010
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4805, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4806, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4807, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0773
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4808, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4809, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4810, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4811, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.5799
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4812, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4813, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4814, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4815, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4815
Update 4816, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4817, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1333
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4818, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0813
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4819, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4820, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1534
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4821, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4822, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1849
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4823, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1938
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4824, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2289
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4825, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4826, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4827, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4828, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4828
Update 4829, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1828
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4830, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4831, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4832, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1897
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4833, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4834, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4835, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4836, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1106
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4837, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3588
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4838, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4839, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4840, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4841, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4841
Update 4842, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4843, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4844, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4845, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4846, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4847, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1924
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4848, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4849, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4850, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4851, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4852, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4853, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3633
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4854, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4854
Update 4855, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4856, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4857, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4858, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4859, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4860, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4861, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4862, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4863, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1087
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4864, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4865, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4544
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4866, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4867, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4867
Update 4868, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4869, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4870, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2764
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4871, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4872, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4483
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4873, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4874, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4875, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4876, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4877, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4878, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4879, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2232
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4880, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4880
Update 4881, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4412
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4882, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1632
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4883, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4884, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4885, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1501
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4886, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4887, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4888, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4889, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4890, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4891, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4892, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4893, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4893
Update 4894, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4895, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4896, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4897, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4898, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4899, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4900, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4901, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1355
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4902, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.5467
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4903, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4904, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4905, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1915
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4906, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2945
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4906
Update 4907, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4908, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4909, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4910, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.5483
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4911, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4912, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4913, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1935
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4914, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2669
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4915, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4916, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4917, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4918, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4919, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4919
Update 4920, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4921, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4922, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2003
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4923, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4924, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1705
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4925, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4926, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3757
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4927, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4928, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4929, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4930, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4931, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4932, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4932
Update 4933, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4934, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2971
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4935, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4936, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4937, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4938, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4939, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4940, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4941, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4942, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4943, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4944, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4945, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4945
Update 4946, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4947, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4948, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4949, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1843
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4950, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4951, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4952, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4953, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4954, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2535
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4955, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4956, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0376
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4957, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4958, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4958
Update 4959, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4960, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4961, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4962, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4963, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4964, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1892
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4965, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2312
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4966, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4967, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2383
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4968, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1675
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4969, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4970, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1965
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4971, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4971
Update 4972, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4973, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4974, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4975, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4976, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4977, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2008
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4978, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4979, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4980, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4981, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4982, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4983, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4984, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4984
Update 4985, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2777
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4986, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4987, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4988, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4989, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4990, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4991, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4992, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4993, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4994, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1612
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4995, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4996, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4997, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 4997
Update 4998, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 4999, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0945
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5000, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2609
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5001, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5002, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5003, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2280
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5004, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5005, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5006, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5007, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5008, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5009, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5010, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5010
Update 5011, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5012, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2048
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5013, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5014, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5015, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5016, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5017, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5018, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2404
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5019, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5020, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5021, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5022, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5023, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5023
Update 5024, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5025, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5026, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5027, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5028, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5029, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5030, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5031, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5032, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3762
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5033, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2029
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5034, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5035, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1309
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5036, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5036
Update 5037, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5038, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2046
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5039, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1563
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5040, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5041, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5042, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2239
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5043, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5044, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1241
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5045, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5046, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5047, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5048, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5049, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5049
Update 5050, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5051, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5052, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1737
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5053, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5054, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1670
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5055, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5056, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4014
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5057, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5058, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5059, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5060, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1470
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5061, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5062, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5062
Update 5063, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1445
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5064, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5065, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5066, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5067, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5068, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5069, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5070, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5071, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5072, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5073, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1951
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5074, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5075, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5075
Update 5076, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5077, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5078, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5079, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2346
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5080, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2469
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5081, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1518
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5082, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3428
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5083, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0811
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5084, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5085, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0327
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5086, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5087, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5088, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5088
Update 5089, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5090, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5091, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5092, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1484
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5093, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5094, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5095, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5096, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5097, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5098, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5099, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5100, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5101, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3753
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5101
Update 5102, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5103, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5104, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3517
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5105, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5106, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5107, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5108, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1794
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5109, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5110, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5111, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5112, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1610
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5113, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2326
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5114, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5114
Update 5115, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5116, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2217
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5117, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1936
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5118, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5119, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3150
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5120, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5121, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0332
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5122, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5123, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5124, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5125, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5126, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5127, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5127
Update 5128, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2528
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5129, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5130, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5131, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5132, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5133, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5134, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5135, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1455
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5136, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5137, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5138, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5139, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5140, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5140
Update 5141, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5142, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5143, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1445
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5144, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5145, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5146, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3882
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5147, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1882
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5148, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5149, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5150, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5151, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5152, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5153, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2685
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5153
Update 5154, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3704
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5155, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5156, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5157, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1873
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5158, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5159, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5160, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0378
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5161, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1157
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5162, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5163, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5164, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5165, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5166, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0947
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5166
Update 5167, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5168, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5169, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2356
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5170, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5171, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1815
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5172, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5173, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3210
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5174, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5175, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0322
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5176, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1367
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5177, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5178, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5179, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5179
Update 5180, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3544
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5181, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1874
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5182, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5183, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5184, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1583
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5185, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5186, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5187, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1386
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5188, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0351
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5189, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5190, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5191, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5192, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5192
Update 5193, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5194, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5195, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3514
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5196, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1410
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5197, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5198, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5199, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5200, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5201, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5202, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5203, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5204, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1244
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5205, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5205
Update 5206, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5207, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3100
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5208, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5209, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5210, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.4079
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5211, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5212, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5213, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5214, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5215, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5216, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5217, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5218, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
New EPOCH! 5218
Update 5219, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5220, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5221, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3249
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5222, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5223, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5224, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5225, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5226, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.0464
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5227, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.1086
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5228, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.3655
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
Update 5229, num samples collected 3200, FPS 13
  Algorithm: train_loss 0.2220
  Episodes: TrainReward -1396.2125, l 200.0000, t 225.4618, TestReward -1458.2708
