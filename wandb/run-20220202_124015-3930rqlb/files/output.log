Sequential(
  (0): Linear(in_features=4, out_features=500, bias=True)
  (1): ReLU()
  (2): Linear(in_features=500, out_features=500, bias=True)
  (3): ReLU()
  (4): DeterministicMB(
    (output): Linear(in_features=500, out_features=3, bias=True)
  )
)
Training model from scratch
Training model from scratch
Collecting initial samples...
Created CWorker with worker_index 0
Created GWorker with worker_index 0
New EPOCH! 0
Update 1, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.8581
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 2, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.8517
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 3, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.6808
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 4, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 5, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.4131
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 6, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2881
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 7, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 8, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.5046
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 9, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 10, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2074
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 11, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.5227
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 12, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 13, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.4457
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 14, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.4481
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 15, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0853
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 16, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.1596
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 17, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 18, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 19, num samples collected 5250, FPS 315
  Algorithm: train_loss 1.0433
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 20, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.2600
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 21, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.6580
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 21
Update 22, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0845
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 23, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.3937
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 24, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.4432
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 25, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 26, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 27, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.4186
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 28, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.3662
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 29, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1590
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 30, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.2900
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 31, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 32, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.2812
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 33, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.6829
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 34, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1126
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 35, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 36, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 37, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 38, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.3530
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 39, num samples collected 5250, FPS 313
  Algorithm: train_loss 1.0188
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 40, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 41, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 42, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2341
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 42
Update 43, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.4086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 44, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 45, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 46, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 47, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 48, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2471
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 49, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 50, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.6418
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 51, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.6789
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 52, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.3680
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 53, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.4932
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 54, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 55, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 56, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.4313
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 57, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2037
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 58, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 59, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.3940
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 60, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 61, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2961
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 62, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.6109
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 63, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 63
Update 64, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 65, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 66, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 67, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 68, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.5576
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 69, num samples collected 5250, FPS 311
  Algorithm: train_loss 1.2246
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 70, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.4175
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 71, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2933
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 72, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 73, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 74, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 75, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 76, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 77, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.1489
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 78, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 79, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 80, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.4007
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 81, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 82, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 83, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 84, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.7299
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 84
Update 85, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 86, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.7761
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 87, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3232
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 88, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.6131
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 89, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3077
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 90, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 91, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 92, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 93, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.1710
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 94, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3456
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 95, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.5020
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 96, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2548
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 97, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 98, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.4682
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 99, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 100, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 101, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 102, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.5842
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 103, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 104, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 105, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 105
Update 106, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.3692
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 107, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 108, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.2979
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 109, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 110, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 111, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.3763
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 112, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.5369
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 113, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 114, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 115, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1852
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 116, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.7775
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 117, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 118, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 119, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.9241
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 120, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.3007
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 121, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.5136
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 122, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 123, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 124, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 125, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 126, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0922
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 126
Update 127, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 128, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 129, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 130, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3267
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 131, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3500
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 132, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.8816
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 133, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 134, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3952
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 135, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3005
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 136, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 137, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0929
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 138, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 139, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.1910
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 140, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.5833
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 141, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.2773
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 142, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 143, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.3247
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 144, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 145, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.7506
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 146, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 147, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 147
Update 148, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 149, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.2901
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 150, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.2647
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 151, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 152, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 153, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 154, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 155, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.2801
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 156, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.3493
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 157, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 158, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.1201
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 159, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.4689
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 160, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.4269
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 161, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 162, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 163, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3718
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 164, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 165, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3457
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 166, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 167, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.4206
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 168, num samples collected 5250, FPS 303
  Algorithm: train_loss 1.2855
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 168
Update 169, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 170, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 171, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.7348
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 172, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 173, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 174, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3166
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 175, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 176, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.7356
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 177, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.5704
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 178, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 179, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.4093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 180, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2756
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 181, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 182, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.6640
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 183, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.4789
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 184, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.6020
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 185, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 186, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 187, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 188, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 189, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 189
Update 190, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.3023
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 191, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.3429
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 192, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 193, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 194, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 195, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.3436
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 196, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.3643
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 197, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.6985
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 198, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 199, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 200, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 201, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 202, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.4898
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 203, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.5352
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 204, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.4070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 205, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 206, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 207, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.2391
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 208, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 209, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.5305
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 210, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 210
Update 211, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1323
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 212, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.4651
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 213, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1477
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 214, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.5184
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 215, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.3604
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 216, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.5503
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 217, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.1596
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 218, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 219, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.1790
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 220, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 221, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.3981
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 222, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0903
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 223, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 224, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 225, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.3634
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 226, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.2589
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 227, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.4194
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 228, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 229, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.6884
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 230, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 231, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 231
Update 232, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 233, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 234, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1893
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 235, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 236, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.4182
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 237, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 238, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 239, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.3641
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 240, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 241, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.3871
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 242, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.7124
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 243, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0722
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 244, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3655
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 245, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3490
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 246, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.8420
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 247, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.2738
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 248, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 249, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 250, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 251, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3515
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 252, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 252
Update 253, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.2894
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 254, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3810
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 255, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.5765
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 256, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.1509
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 257, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 258, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2949
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 259, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3873
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 260, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.5865
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 261, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 262, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 263, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 264, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3454
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 265, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 266, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0728
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 267, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3984
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 268, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 269, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.1632
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 270, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3540
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 271, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.3637
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 272, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 273, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.9845
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 273
Update 274, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 275, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.4961
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 276, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 277, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0733
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 278, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.3180
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 279, num samples collected 5250, FPS 295
  Algorithm: train_loss 1.1842
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 280, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 281, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 282, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 283, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1181
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 284, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 285, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 286, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.5872
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 287, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2361
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 288, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2954
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 289, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 290, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.5367
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 291, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.3666
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 292, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.3163
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 293, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2365
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 294, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 294
Update 295, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.3822
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 296, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 297, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.4508
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 298, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3745
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 299, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 300, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 301, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.2973
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 302, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3592
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 303, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3190
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 304, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3476
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 305, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.5086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 306, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 307, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 308, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3526
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 309, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3190
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 310, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 311, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.2595
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 312, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 313, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.4839
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 314, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 315, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 315
Update 316, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0697
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 317, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 318, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 319, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 320, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 321, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 322, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.8354
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 323, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 324, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.7074
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 325, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.2373
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 326, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 327, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.5433
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 328, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3163
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 329, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3483
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 330, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2740
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 331, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3323
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 332, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3908
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 333, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 334, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 335, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 336, num samples collected 5250, FPS 291
  Algorithm: train_loss 1.2108
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 336
Update 337, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3182
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 338, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0733
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 339, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2373
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 340, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.5925
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 341, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.5003
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 342, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3575
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 343, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3618
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 344, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 345, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3880
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 346, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 347, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 348, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 349, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.4567
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 350, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 351, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.7435
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 352, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3006
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 353, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.2042
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 354, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.1752
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 355, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 356, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 357, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 357
Update 358, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 359, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.6413
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 360, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.1855
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 361, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 362, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.6150
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 363, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.4902
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 364, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 365, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.3455
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 366, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 367, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 368, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.5877
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 369, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.5049
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 370, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.3699
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 371, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 372, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.3057
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 373, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.3593
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 374, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 375, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 376, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 377, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 378, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 378
Update 379, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 380, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 381, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 382, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 383, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 384, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 385, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.3103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 386, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.6057
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 387, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 388, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.3607
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 389, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 390, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2548
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 391, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.8617
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 392, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 393, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 394, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.5456
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 395, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 396, num samples collected 5250, FPS 287
  Algorithm: train_loss 1.2329
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 397, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0553
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 398, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 399, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.6201
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 399
Update 400, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.2516
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 401, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.1943
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 402, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 403, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 404, num samples collected 5250, FPS 286
  Algorithm: train_loss 1.0337
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 405, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3779
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 406, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 407, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 408, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 409, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 410, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3155
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 411, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 412, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.7377
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 413, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 414, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.6127
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 415, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.6093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 416, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2412
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 417, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.4120
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 418, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 419, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 420, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 420
Update 421, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 422, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 423, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.7406
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 424, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 425, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 426, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.5658
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 427, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 428, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 429, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.2401
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 430, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 431, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.6380
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 432, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.3402
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 433, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 434, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.3211
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 435, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 436, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 437, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.3151
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 438, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.3624
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 439, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 440, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 441, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.5855
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 441
Update 442, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.5566
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 443, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.1523
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 444, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 445, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 446, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 447, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.5105
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 448, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 449, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 450, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.4098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 451, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 452, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.3421
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 453, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.6808
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 454, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.4043
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 455, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 456, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 457, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.5695
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 458, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 459, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 460, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3217
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 461, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3727
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 462, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 462
Update 463, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 464, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.1551
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 465, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3403
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 466, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 467, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3813
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 468, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 469, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.6072
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 470, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.6166
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 471, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 472, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.4024
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 473, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.5696
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 474, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.3645
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 475, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.2615
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 476, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 477, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 478, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 479, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.4987
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 480, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 481, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 482, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.3447
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 483, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 483
Update 484, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 485, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.8645
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 486, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.6391
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 487, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.7704
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 488, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 489, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.5031
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 490, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 491, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0835
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 492, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 493, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.3436
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 494, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.3582
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 495, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 496, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2798
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 497, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 498, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.4418
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 499, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 500, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 501, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 502, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 503, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 504, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.5051
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 504
Update 505, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0709
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 506, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 507, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.1466
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 508, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 509, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0669
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 510, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 511, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2593
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 512, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.7361
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 513, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.4726
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 514, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.2739
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 515, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 516, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 517, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1516
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 518, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.8452
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 519, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 520, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.6038
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 521, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.4647
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 522, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 523, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 524, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.4567
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 525, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 525
Update 526, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0738
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 527, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 528, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.6806
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 529, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.3180
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 530, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 531, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 532, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.3417
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 533, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.7069
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 534, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1227
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 535, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 536, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.4088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 537, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.4866
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 538, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.4519
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 539, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 540, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 541, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 542, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1508
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 543, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.6042
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 544, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 545, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.3975
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 546, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 546
Update 547, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2501
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 548, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.6009
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 549, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 550, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.5674
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 551, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 552, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.6887
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 553, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 554, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 555, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.6838
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 556, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 557, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.5775
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 558, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 559, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 560, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 561, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.7614
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 562, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 563, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.4429
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 564, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 565, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 566, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1822
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 567, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 567
Update 568, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 569, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.5721
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 570, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.3679
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 571, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 572, num samples collected 5250, FPS 275
  Algorithm: train_loss 1.1160
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 573, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.4729
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 574, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 575, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 576, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.2310
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 577, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 578, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0753
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 579, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 580, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 581, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 582, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 583, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 584, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.5041
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 585, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0647
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 586, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.7013
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 587, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.3251
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 588, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 588
Update 589, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.3339
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 590, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 591, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1489
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 592, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 593, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0484
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 594, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 595, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.9043
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 596, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 597, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 598, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.3434
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 599, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.3427
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 600, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 601, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 602, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.7355
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 603, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 604, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1595
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 605, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2417
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 606, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3856
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 607, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 608, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 609, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 609
Update 610, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 611, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2559
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 612, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 613, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.8735
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 614, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3429
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 615, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3764
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 616, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3446
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 617, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 618, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 619, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 620, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.3094
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 621, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 622, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 623, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.5182
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 624, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 625, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 626, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 627, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 628, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.3192
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 629, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1157
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 630, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.7730
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 630
Update 631, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 632, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.3361
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 633, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1243
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 634, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.3752
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 635, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 636, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2975
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 637, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.7405
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 638, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 639, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 640, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 641, num samples collected 5250, FPS 270
  Algorithm: train_loss 1.0957
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 642, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 643, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 644, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0553
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 645, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.5747
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 646, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 647, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 648, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 649, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.5728
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 650, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 651, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 651
Update 652, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.7080
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 653, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1794
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 654, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 655, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 656, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.3135
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 657, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 658, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 659, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.5018
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 660, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2751
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 661, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.7261
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 662, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 663, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 664, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2669
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 665, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 666, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.5343
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 667, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 668, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.3430
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 669, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1223
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 670, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0664
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 671, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 672, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 672
Update 673, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0479
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 674, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 675, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 676, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.3645
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 677, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.7701
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 678, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0713
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 679, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 680, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 681, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.5165
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 682, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1896
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 683, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 684, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.3490
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 685, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 686, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.4261
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 687, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.5592
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 688, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 689, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.3606
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 690, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.3462
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 691, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 692, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.2694
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 693, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.6680
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 693
Update 694, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.1714
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 695, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 696, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 697, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.3270
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 698, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.8596
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 699, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 700, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 701, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.5457
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 702, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 703, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 704, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 705, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.6040
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 706, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1191
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 707, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.5953
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 708, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 709, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 710, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 711, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 712, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 713, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.6047
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 714, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.7092
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 714
Update 715, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 716, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.3773
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 717, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1218
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 718, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 719, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.4154
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 720, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2138
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 721, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3166
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 722, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 723, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.4813
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 724, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.7315
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 725, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3605
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 726, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2490
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 727, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 728, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 729, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3455
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 730, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 731, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1557
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 732, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 733, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 734, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.6250
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 735, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 735
Update 736, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.5802
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 737, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2819
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 738, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 739, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 740, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 741, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2359
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 742, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 743, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 744, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2301
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 745, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 746, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 747, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1810
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 748, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.8193
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 749, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.8074
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 750, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.3561
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 751, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 752, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 753, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 754, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.7353
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 755, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 756, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 756
Update 757, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 758, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.5187
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 759, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2393
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 760, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.3546
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 761, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 762, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 763, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2230
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 764, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.3004
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 765, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 766, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.3621
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 767, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 768, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0809
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 769, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3347
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 770, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.5499
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 771, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3341
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 772, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.4755
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 773, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 774, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 775, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.4681
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 776, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.2759
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 777, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 777
Update 778, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 779, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 780, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.5138
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 781, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 782, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3542
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 783, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.3602
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 784, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.3199
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 785, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.8950
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 786, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 787, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 788, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 789, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.3710
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 790, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.7690
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 791, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 792, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 793, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2326
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 794, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 795, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 796, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0723
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 797, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 798, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 798
Update 799, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 800, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0703
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 801, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 802, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.3362
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 803, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 804, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.2438
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 805, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.3136
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 806, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 807, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.5433
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 808, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.6328
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 809, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.3962
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 810, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 811, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 812, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 813, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.2362
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 814, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.5695
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 815, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 816, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 817, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.8598
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 818, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 819, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2869
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 819
Update 820, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 821, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.6117
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 822, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 823, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 824, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 825, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5288
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 826, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.4970
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 827, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.3322
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 828, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2100
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 829, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 830, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 831, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 832, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.7078
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 833, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 834, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 835, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 836, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.3001
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 837, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.5012
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 838, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1548
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 839, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.4454
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 840, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1223
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 840
Update 841, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 842, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 843, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.5637
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 844, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 845, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 846, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.6634
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 847, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 848, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 849, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.5546
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 850, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 851, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 852, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 853, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.5586
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 854, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 855, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 856, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0740
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 857, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.3648
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 858, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.4259
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 859, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.4880
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 860, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 861, num samples collected 5250, FPS 257
  Algorithm: train_loss 1.1222
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 861
Update 862, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 863, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 864, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.3129
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 865, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.8434
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 866, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 867, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 868, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.2453
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 869, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 870, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.3151
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 871, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 872, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 873, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3480
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 874, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.4367
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 875, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.1913
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 876, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3919
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 877, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.4064
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 878, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.2670
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 879, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3792
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 880, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 881, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3948
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 882, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 882
Update 883, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 884, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.4407
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 885, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 886, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.7203
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 887, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.4423
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 888, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 889, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0733
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 890, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 891, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 892, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3152
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 893, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 894, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0479
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 895, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 896, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3301
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 897, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 898, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3726
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 899, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3122
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 900, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.4301
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 901, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3528
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 902, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3339
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 903, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 903
Update 904, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 905, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3152
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 906, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1922
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 907, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3378
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 908, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 909, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.3106
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 910, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 911, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.3026
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 912, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 913, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 914, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 915, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2371
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 916, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 917, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 918, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 919, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.4674
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 920, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.9802
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 921, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.7409
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 922, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 923, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 924, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2191
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 924
Update 925, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.3888
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 926, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 927, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 928, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 929, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.4293
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 930, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 931, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 932, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.5895
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 933, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3504
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 934, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3151
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 935, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3157
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 936, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.6286
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 937, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3584
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 938, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 939, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 940, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2690
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 941, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3174
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 942, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 943, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2617
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 944, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 945, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 945
Update 946, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 947, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 948, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3113
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 949, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 950, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 951, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.2937
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 952, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0995
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 953, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3966
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 954, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3100
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 955, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3330
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 956, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.2349
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 957, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 958, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 959, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3294
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 960, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1702
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 961, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.5845
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 962, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 963, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 964, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.6072
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 965, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.7361
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 966, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.5029
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 966
Update 967, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.8096
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 968, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.3169
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 969, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1926
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 970, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 971, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2170
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 972, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.8127
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 973, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 974, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 975, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.7905
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 976, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 977, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.8266
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 978, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 979, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 980, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 981, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2659
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 982, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 983, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1725
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 984, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 985, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 986, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1823
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 987, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 987
Update 988, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 989, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 990, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.5021
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 991, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3704
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 992, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 993, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3833
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 994, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1700
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 995, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1898
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 996, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3173
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 997, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 998, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 999, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3269
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1000, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1001, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.4263
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1002, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.5703
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1003, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3706
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1004, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0684
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1005, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3757
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1006, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1007, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2665
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1008, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0864
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1008
Update 1009, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2967
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1010, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2313
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1011, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3768
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1012, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1013, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1014, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.4732
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1015, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1016, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3854
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1017, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3320
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1018, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1019, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1020, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.6568
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1021, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.1192
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1022, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1023, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.3896
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1024, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0721
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1025, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.8494
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1026, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.2567
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1027, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1028, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1029, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1265
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1029
Update 1030, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0488
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1031, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.3900
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1032, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.7979
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1033, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0734
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1034, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.6609
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1035, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1036, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1037, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1038, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1039, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1040, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1041, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2633
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1042, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.3138
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1043, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2376
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1044, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.6099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1045, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1046, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2438
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1047, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2262
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1048, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.1879
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1049, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1050, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.6076
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1050
Update 1051, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1052, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1053, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.3095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1054, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1055, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1056, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.5346
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1057, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.9015
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1058, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1059, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1060, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1061, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1062, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0496
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1063, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2352
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1064, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1065, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1066, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1067, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.9299
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1068, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3114
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1069, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1070, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.7138
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1071, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2242
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1071
Update 1072, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.8573
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1073, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1218
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1074, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3965
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1075, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0864
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1076, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1077, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1078, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1079, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.7879
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1080, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.4619
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1081, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2326
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1082, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3919
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1083, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1157
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1084, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2990
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1085, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1086, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1920
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1087, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1088, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1089, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1090, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1091, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1092, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.3694
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1092
Update 1093, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.3619
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1094, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1095, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1096, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2536
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1097, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1098, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.3578
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1099, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2258
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1100, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.3536
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1101, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2918
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1102, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1103, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1104, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1678
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1105, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.4840
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1106, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1107, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1108, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.6640
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1109, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1110, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1111, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3420
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1112, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1113, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1113
Update 1114, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0749
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1115, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0708
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1116, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.5741
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1117, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0419
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1118, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2211
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1119, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.5367
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1120, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1121, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3324
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1122, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1123, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1124, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3071
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1125, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.6292
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1126, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3711
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1127, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1128, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.4686
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1129, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1130, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0731
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1131, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0513
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1132, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.5017
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1133, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3300
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1134, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2915
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1134
Update 1135, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3383
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1136, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1137, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.4857
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1138, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3792
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1139, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3242
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1140, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3393
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1141, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1142, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3321
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1143, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.4438
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1144, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.6012
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1145, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2656
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1146, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1147, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1148, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1149, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.3698
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1150, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1151, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1152, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1153, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1481
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1154, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.2822
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1155, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1155
Update 1156, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.6864
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1157, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.3528
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1158, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1159, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.3611
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1160, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1161, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1162, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.2616
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1163, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.2354
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1164, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.6835
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1165, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.5548
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1166, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1167, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1516
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1168, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.2632
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1169, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1170, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.5117
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1171, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1172, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1173, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1174, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1175, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1176, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1176
Update 1177, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1178, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2987
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1179, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1180, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1181, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1182, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.4112
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1183, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1184, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.8795
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1185, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.1158
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1186, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.5345
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1187, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1188, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1189, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.4797
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1190, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1191, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1192, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2710
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1193, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1194, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3186
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1195, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3333
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1196, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3960
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1197, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.6154
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1197
Update 1198, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.7379
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1199, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1200, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0681
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1201, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1910
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1202, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1359
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1203, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3228
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1204, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1205, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.4618
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1206, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1612
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1207, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1208, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.6768
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1209, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1210, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3385
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1211, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3607
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1212, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1213, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1214, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.5390
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1215, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.1509
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1216, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.2594
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1217, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1218, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1218
Update 1219, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.1505
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1220, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1221, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1222, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.5561
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1223, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.5623
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1224, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.1546
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1225, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0684
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1226, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.5559
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1227, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1228, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2588
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1229, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1230, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.3798
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1231, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2189
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1232, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1233, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1234, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1235, num samples collected 5250, FPS 236
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1236, num samples collected 5250, FPS 236
  Algorithm: train_loss 1.0272
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1237, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1238, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1239, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.4479
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1239
Finished MB training, ran for 60 epochs
Update 1240, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.4119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1241, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.4167
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1242, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.4662
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1243, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0713
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1244, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1245, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1246, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1247, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1248, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.3420
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1249, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1250, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.1907
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1251, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1252, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.4920
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1253, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.3537
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1254, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0747
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1255, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1256, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.1662
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1257, num samples collected 5250, FPS 235
  Algorithm: train_loss 0.5760
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1258, num samples collected 5250, FPS 234
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1259, num samples collected 5250, FPS 234
  Algorithm: train_loss 0.4008
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
Update 1260, num samples collected 5250, FPS 234
  Algorithm: train_loss 0.6526
  Episodes: TrainReward -1385.6219, l 200.0000, t 102.9365, TestReward -1446.2852
New EPOCH! 1260
Update 1261, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.3622
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1262, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1263, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1264, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1265, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1266, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1267, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1922
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1268, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1911
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1269, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.3654
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1270, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1271, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3975
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1272, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1273, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1274, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.5793
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1275, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3629
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1276, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4100
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1277, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4642
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1278, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.5397
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1279, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1280, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0736
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1281, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2858
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1282, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1282
Update 1283, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.8498
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1284, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4815
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1285, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1286, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1287, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2147
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1288, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1289, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1290, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3043
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1291, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1164
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1292, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1293, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2198
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1294, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1295, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.9399
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1296, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1297, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1298, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1299, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3675
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1300, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1301, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4626
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1302, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3204
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1303, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2642
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1304, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1304
Update 1305, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1306, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1307, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1308, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4041
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1309, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.5257
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1310, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1311, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1312, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3584
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1313, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1314, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1315, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1495
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1316, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1317, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1318, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3248
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1319, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.6380
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1320, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1321, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3371
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1322, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1197
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1323, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3843
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1324, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3349
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1325, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2358
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1326, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4607
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1326
Update 1327, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3128
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1328, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1329, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4284
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1330, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1331, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1332, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2599
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1333, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1525
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1334, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.5826
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1335, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1336, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4724
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1337, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0867
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1338, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1339, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.5540
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1340, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1341, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0884
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1342, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.6511
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1343, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1204
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1344, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.5171
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1345, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1346, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1347, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2656
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1348, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1348
Update 1349, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.6327
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1350, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1351, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4165
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1352, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1353, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.7825
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1354, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1355, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3955
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1356, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1357, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1358, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1359, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1529
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1360, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1361, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1362, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3207
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1363, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1364, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1365, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.7112
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1366, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.5455
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1367, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.6321
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1368, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1369, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1370, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1370
Update 1371, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1372, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.6924
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1373, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1374, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1375, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2068
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1376, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1377, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1378, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1379, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1380, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1381, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.5491
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1382, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2091
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1383, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1384, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1385, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1386, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1387, num samples collected 5500, FPS 138
  Algorithm: train_loss 1.2210
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1388, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4176
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1389, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1390, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3607
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1391, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1199
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1392, num samples collected 5500, FPS 138
  Algorithm: train_loss 1.1936
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1392
Update 1393, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1394, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1395, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1525
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1396, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.5160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1397, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1398, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3148
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1399, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3260
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1400, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2661
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1401, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3269
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1402, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1403, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1404, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4342
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1405, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.6223
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1406, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1407, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.6273
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1408, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1409, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1152
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1410, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3609
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1411, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1412, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1413, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4662
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1414, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0769
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1414
Update 1415, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2109
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1416, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1501
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1417, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.6795
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1418, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1419, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1420, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4271
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1421, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1422, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.7585
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1423, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1424, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1425, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0484
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1426, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1427, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3320
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1428, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1429, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2179
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1430, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1431, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3576
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1432, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1433, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.9833
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1434, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1435, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3128
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1436, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1436
Update 1437, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1438, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4352
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1439, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1440, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.6665
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1441, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2499
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1442, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1443, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1444, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4755
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1445, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1446, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1447, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1448, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1449, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2596
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1450, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1451, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2479
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1452, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.6797
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1453, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1454, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1455, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1456, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1457, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.7075
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1458, num samples collected 5500, FPS 137
  Algorithm: train_loss 1.0196
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1458
Update 1459, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.5922
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1460, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1461, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1462, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3536
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1463, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3157
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1464, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1465, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1466, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1467, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1468, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1469, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1470, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.6925
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1471, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3578
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1472, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1473, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1474, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1475, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4881
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1476, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0736
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1477, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1958
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1478, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1479, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4982
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1480, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1480
Update 1481, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1482, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2595
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1483, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1484, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3192
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1485, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1486, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1487, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.5226
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1488, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.7569
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1489, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1490, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1491, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1546
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1492, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1493, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3304
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1494, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1495, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2120
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1496, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1497, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4490
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1498, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1761
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1499, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.7114
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1500, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3135
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1501, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3040
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1502, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1502
Update 1503, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1504, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1505, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4624
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1506, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1507, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1508, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.8212
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1509, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2367
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1510, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3428
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1511, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3753
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1512, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3328
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1513, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1514, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1515, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1516, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0863
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1517, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0694
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1518, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3386
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1519, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1520, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1521, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.6616
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1522, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1523, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.7573
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1524, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1524
Update 1525, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1526, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1527, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4708
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1528, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1529, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.6252
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1530, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2385
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1531, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2968
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1532, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1533, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.8514
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1534, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2213
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1535, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1536, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1537, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1801
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1538, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1539, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1540, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1541, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3598
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1542, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1543, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2746
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1544, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1545, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4416
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1546, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.5326
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1546
Update 1547, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1548, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1549, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.8297
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1550, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1551, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4881
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1552, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1553, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2386
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1554, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4895
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1555, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1556, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1557, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2352
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1558, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1849
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1559, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3751
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1560, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4967
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1561, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1562, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1563, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1564, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1266
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1565, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1566, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.6461
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1567, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1568, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1568
Update 1569, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1570, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1571, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1572, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2143
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1573, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1574, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4503
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1575, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1576, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0724
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1577, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3537
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1578, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1579, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1580, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2141
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1581, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4575
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1582, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2397
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1583, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1584, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4571
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1585, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3728
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1586, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.7003
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1587, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1588, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1589, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1500
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1590, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1590
Update 1591, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3181
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1592, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.6830
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1593, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1594, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1595, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.9202
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1596, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.6131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1597, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1598, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1599, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2764
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1600, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1601, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3752
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1602, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4949
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1603, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1604, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1605, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1606, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1607, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1608, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3299
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1609, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1610, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1611, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1612, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1612
Update 1613, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1614, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1615, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.5067
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1616, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1972
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1617, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1618, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4819
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1619, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1620, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1856
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1621, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1622, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3827
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1623, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1624, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1625, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1626, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1627, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.6105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1628, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1775
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1629, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1630, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1631, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3094
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1632, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1633, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3617
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1634, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.9540
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1634
Update 1635, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.7477
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1636, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.8849
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1637, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1638, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3221
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1639, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1640, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.8569
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1641, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1642, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1643, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1644, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0726
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1645, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1646, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1647, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1648, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1770
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1649, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0791
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1650, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1651, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2179
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1652, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4627
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1653, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4515
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1654, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1655, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1656, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2311
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1656
Update 1657, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1658, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1659, num samples collected 5500, FPS 134
  Algorithm: train_loss 1.0011
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1660, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.6529
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1661, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1662, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1663, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3103
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1664, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1665, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1666, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1667, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1668, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1669, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1670, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1671, num samples collected 5500, FPS 134
  Algorithm: train_loss 1.2540
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1672, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1673, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4281
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1674, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3321
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1675, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1676, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1677, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1678, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1678
Update 1679, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3178
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1680, num samples collected 5500, FPS 133
  Algorithm: train_loss 1.0048
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1681, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1682, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0818
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1683, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.7444
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1684, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1685, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3236
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1686, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.5678
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1687, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1688, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1689, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1690, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3330
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1691, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2738
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1692, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1693, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1548
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1694, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1695, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3545
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1696, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1697, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1698, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1699, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1700, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1700
Update 1701, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1702, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.5289
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1703, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1704, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1705, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1706, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2886
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1707, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1708, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.8159
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1709, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1710, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1711, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0679
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1712, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1713, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1714, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3107
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1715, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3255
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1716, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.5386
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1717, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3202
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1718, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.6687
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1719, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1720, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1200
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1721, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2354
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1722, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1722
Update 1723, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1724, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1725, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1937
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1726, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.5107
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1727, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1529
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1728, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.8206
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1729, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3946
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1730, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1731, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1903
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1732, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1733, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2007
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1734, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3095
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1735, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.5081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1736, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2338
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1737, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1738, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2842
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1739, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1740, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1741, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1742, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.5166
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1743, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1744, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1744
Update 1745, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4830
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1746, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4579
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1747, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4465
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1748, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3492
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1749, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3282
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1750, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1751, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.6438
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1752, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1753, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.5539
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1754, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1755, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1756, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2537
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1757, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1758, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4658
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1759, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1760, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1029
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1761, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1762, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1469
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1763, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1764, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0734
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1765, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0520
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1766, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.7020
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1766
Update 1767, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.5853
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1768, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1769, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1770, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1771, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1772, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1773, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1098
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1774, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1775, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2617
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1776, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1777, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1496
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1778, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.6801
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1779, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4766
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1780, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1781, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4033
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1782, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1783, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1784, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3123
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1785, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1786, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2383
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1787, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3401
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1788, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1788
Update 1789, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0727
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1790, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1540
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1791, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.6623
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1792, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3525
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1793, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1794, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3359
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1795, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0460
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1796, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4037
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1797, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1798, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1799, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1800, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1801, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3150
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1802, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2068
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1803, num samples collected 5500, FPS 132
  Algorithm: train_loss 1.0264
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1804, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1805, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1806, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3246
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1807, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1808, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1855
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1809, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1810, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1810
Update 1811, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4764
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1812, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4205
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1813, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1496
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1814, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.6199
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1815, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1183
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1816, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1837
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1817, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1818, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3828
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1819, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1820, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.5670
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1821, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.7060
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1822, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1823, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1824, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2681
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1825, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1826, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1827, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2967
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1828, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1829, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1947
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1830, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1831, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1832, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1832
Update 1833, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3229
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1834, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1835, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3494
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1836, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1396
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1837, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.5004
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1838, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4672
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1839, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1840, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1841, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1530
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1842, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1843, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1844, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1845, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3912
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1846, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1847, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.7964
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1848, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1849, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1850, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1851, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3091
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1852, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.6150
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1853, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1810
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1854, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1854
Update 1855, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2742
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1856, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.6191
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1857, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4756
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1858, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1859, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2838
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1860, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1861, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.5031
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1862, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3199
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1863, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1864, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1865, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3856
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1866, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2065
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1867, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1528
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1868, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4673
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1869, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1870, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1871, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1872, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1873, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1874, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2657
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1875, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1876, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4950
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1876
Update 1877, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1878, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.5761
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1879, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3544
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1880, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.5127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1881, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4609
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1882, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3157
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1883, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1884, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1025
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1885, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1886, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4390
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1887, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0834
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1888, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1971
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1889, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1890, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1891, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1892, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1893, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1894, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1895, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1896, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1897, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1898, num samples collected 5500, FPS 130
  Algorithm: train_loss 1.4974
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1898
Update 1899, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.5259
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1900, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3568
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1901, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.7325
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1902, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.5861
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1903, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1904, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1822
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1905, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1906, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1907, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1908, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1909, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4191
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1910, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1911, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1503
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1912, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1913, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2362
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1914, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1915, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1916, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1917, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2627
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1918, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3616
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1919, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3618
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1920, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1920
Update 1921, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3970
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1922, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1695
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1923, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1146
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1924, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2891
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1925, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1926, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4986
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1927, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1928, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1929, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1930, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1931, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3452
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1932, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2737
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1933, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4113
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1934, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3000
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1935, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3274
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1936, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1937, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1938, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.9983
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1939, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1960
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1940, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1941, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1942, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0959
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1942
Update 1943, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4027
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1944, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1945, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2311
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1946, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1947, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3608
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1948, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.8513
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1949, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1950, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2983
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1951, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1952, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3251
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1953, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4490
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1954, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1955, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4736
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1956, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1957, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1958, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1959, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1960, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1961, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1962, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1963, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4880
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1964, num samples collected 5500, FPS 129
  Algorithm: train_loss 1.2531
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1964
Update 1965, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1966, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1967, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1933
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1968, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4221
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1969, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4180
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1970, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1971, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3659
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1972, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1973, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.6654
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1974, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1975, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4215
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1976, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1977, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2929
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1978, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1979, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1966
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1980, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4426
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1981, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3262
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1982, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.5036
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1983, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1984, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3555
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1985, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1986, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 1986
Update 1987, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3146
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1988, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1989, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3942
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1990, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1991, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1992, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1908
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1993, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1994, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1995, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1996, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.5752
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1997, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3649
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1998, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1647
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 1999, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4244
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2000, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2001, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.5005
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2002, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2688
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2003, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3102
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2004, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2005, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4505
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2006, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2007, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3234
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2008, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2008
Update 2009, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2333
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2010, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.7685
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2011, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3526
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2012, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3159
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2013, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4241
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2014, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3652
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2015, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2183
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2016, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2017, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2018, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3031
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2019, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2020, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2021, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2022, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2023, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2024, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2025, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3371
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2026, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2027, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4691
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2028, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.6234
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2029, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1917
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2030, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2030
Update 2031, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2032, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2063
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2033, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1943
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2034, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3720
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2035, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4887
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2036, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2037, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2038, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3130
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2039, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3674
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2040, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.7783
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2041, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2042, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3593
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2043, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2044, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2045, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2046, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2047, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.6090
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2048, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2049, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2050, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2051, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2052, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.6080
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2052
Update 2053, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.8530
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2054, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2055, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2056, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2057, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2058, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2059, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2060, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2061, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1878
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2062, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0734
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2063, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2064, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2065, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3457
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2066, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2067, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3031
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2068, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2069, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4310
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2070, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2071, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.6057
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2072, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.7801
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2073, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3408
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2074, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2074
Update 2075, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2076, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2077, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2078, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.5136
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2079, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2080, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.6830
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2081, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.7345
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2082, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2083, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2084, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2834
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2085, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2086, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3522
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2087, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3506
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2088, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2089, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2090, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2091, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4289
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2092, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1757
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2093, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2094, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2095, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2096, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.9939
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2096
Update 2097, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2098, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2099, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3076
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2100, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2101, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2102, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2103, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.5148
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2104, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2105, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2947
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2106, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.5027
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2107, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2108, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2967
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2109, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1523
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2110, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2111, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2112, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.4490
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2113, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2114, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.4557
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2115, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.4703
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2116, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3158
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2117, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2118, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.8542
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2118
Update 2119, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.5933
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2120, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2121, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2175
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2122, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2123, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1962
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2124, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2244
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2125, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3197
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2126, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2127, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2128, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1166
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2129, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2130, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0681
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2131, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2132, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.6502
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2133, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2074
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2134, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2135, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1654
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2136, num samples collected 5500, FPS 127
  Algorithm: train_loss 1.0269
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2137, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3592
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2138, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2139, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2140, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2140
Update 2141, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2142, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.5511
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2143, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2047
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2144, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2145, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2693
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2146, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2147, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2148, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2149, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3386
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2150, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3827
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2151, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2152, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.5877
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2153, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5497
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2154, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1908
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2155, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3062
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2156, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2157, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3212
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2158, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1181
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2159, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1860
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2160, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2161, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1178
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2162, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2162
Update 2163, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5985
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2164, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1557
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2165, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4579
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2166, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1483
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2167, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2168, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2169, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5777
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2170, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2042
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2171, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3208
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2172, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2173, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3056
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2174, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2175, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.7023
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2176, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2177, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2178, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2179, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2180, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2181, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3138
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2182, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1801
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2183, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2184, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.6532
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2184
Update 2185, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2186, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2187, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2188, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2189, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1625
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2190, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5266
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2191, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2192, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2193, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5288
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2194, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3218
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2195, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2196, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2197, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2198, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3544
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2199, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4229
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2200, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.6551
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2201, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2202, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2203, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2204, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.6020
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2205, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2206, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3009
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2206
Update 2207, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2208, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2209, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2210, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0808
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2211, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3087
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2212, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2213, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2214, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0700
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2215, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4617
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2216, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2725
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2217, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2218, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2260
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2219, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1906
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2220, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2221, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5783
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2222, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.5392
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2223, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2895
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2224, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3560
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2225, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2226, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2949
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2227, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2228, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2228
Update 2229, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2969
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2230, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2231, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2232, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1445
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2233, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2234, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.8365
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2235, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4203
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2236, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1801
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2237, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1532
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2238, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1098
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2239, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2240, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2241, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2242, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2803
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2243, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0672
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2244, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.5394
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2245, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2246, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2247, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2248, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3995
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2249, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2250, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2250
Update 2251, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2252, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2253, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3514
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2254, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2255, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4577
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2256, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3882
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2257, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2258, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2259, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2260, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2261, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2262, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3509
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2263, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2264, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2265, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4224
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2266, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.5683
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2267, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2268, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2269, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4075
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2270, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3138
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2271, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3043
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2272, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2272
Update 2273, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4767
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2274, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2099
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2275, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0383
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2276, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4281
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2277, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2278, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2279, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3082
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2280, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2281, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3196
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2282, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4793
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2283, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4481
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2284, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3269
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2285, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2711
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2286, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2287, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3417
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2288, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2289, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2290, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2291, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4049
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2292, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2293, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3137
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2294, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2294
Update 2295, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3342
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2296, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3286
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2297, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2298, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2299, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2300, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2301, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2302, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2303, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2304, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3442
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2305, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2111
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2306, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.7345
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2307, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2972
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2308, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1711
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2309, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2310, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.7347
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2311, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2312, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2313, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2314, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2315, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3129
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2316, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.6130
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2316
Update 2317, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2318, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2319, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2710
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2320, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2321, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3483
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2322, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2323, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.6927
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2324, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.5141
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2325, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1354
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2326, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2786
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2327, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3142
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2328, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2329, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2330, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4072
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2331, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2656
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2332, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.7043
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2333, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2334, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2335, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2336, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2337, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2338, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2338
Update 2339, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2340, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2427
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2341, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.5290
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2342, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1537
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2343, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.5803
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2344, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2345, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4526
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2346, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2347, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2902
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2348, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4214
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2349, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.5420
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2350, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2351, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2352, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2353, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2354, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2355, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2356, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1530
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2357, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3520
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2358, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.5204
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2359, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0700
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2360, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2360
Update 2361, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2964
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2362, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2363, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4612
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2364, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4051
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2365, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2366, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2367, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3463
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2368, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5009
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2369, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2370, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3484
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2371, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1224
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2372, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0496
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2373, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2374, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2375, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2376, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3533
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2377, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3070
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2378, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2911
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2379, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2380, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1997
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2381, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.4166
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2382, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2382
Update 2383, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2189
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2384, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1591
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2385, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1739
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2386, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0783
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2387, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2388, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2987
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2389, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2641
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2390, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2899
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2391, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0737
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2392, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2393, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2394, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.7876
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2395, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3767
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2396, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2397, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2838
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2398, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1976
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2399, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3500
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2400, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2401, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5011
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2402, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2403, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2478
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2404, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2404
Update 2405, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2406, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2407, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2408, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2409, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0679
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2410, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1918
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2411, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.6555
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2412, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2413, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3928
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2414, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2415, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2416, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2417, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.6184
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2418, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2419, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0713
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2420, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2421, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.8573
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2422, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2423, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0885
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2424, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5083
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2425, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.6410
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2426, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.4394
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2426
Update 2427, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2428, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5227
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2429, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5367
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2430, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2883
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2431, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2432, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2433, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3057
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2434, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2435, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2900
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2436, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1350
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2437, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2582
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2438, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2439, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3455
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2440, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2441, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5984
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2442, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2443, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.5932
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2444, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2445, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2446, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2447, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3426
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2448, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2448
Update 2449, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0763
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2450, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4486
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2451, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1341
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2452, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2453, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2454, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2455, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2456, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3016
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2457, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4035
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2458, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2459, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2460, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3358
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2461, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1890
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2462, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2463, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2464, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2465, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2466, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3502
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2467, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2468, num samples collected 5500, FPS 122
  Algorithm: train_loss 1.1663
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2469, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3845
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2470, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.5472
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2470
Update 2471, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.5538
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2472, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.5026
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2473, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2474, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2475, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2476, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2477, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2478, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0744
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2479, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2480, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2481, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.7683
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2482, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2150
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2483, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.5774
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2484, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2288
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2485, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2486, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2147
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2487, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2488, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4160
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2489, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2490, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1388
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2491, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2492, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2492
Update 2493, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2494, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2495, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.6513
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2496, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2497, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.5602
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2498, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3849
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2499, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2563
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2500, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2962
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2501, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2369
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2502, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2503, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2504, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2505, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2506, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2507, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2508, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2082
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2509, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2510, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0713
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2511, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2512, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2513, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1192
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2514, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2514
Update 2515, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2516, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.9039
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2517, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3025
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2518, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2650
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2519, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2511
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2520, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3148
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2521, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2522, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2523, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2524, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0766
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2525, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2526, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3050
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2527, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2528, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4145
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2529, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3471
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2530, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2531, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2532, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1892
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2533, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2534, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2797
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2535, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3183
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2536, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3419
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2536
Update 2537, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3433
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2538, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2539, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2540, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4419
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2541, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1152
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2542, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2543, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2544, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2545, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4505
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2546, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4843
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2547, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.7306
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2548, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2549, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2550, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3050
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2551, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2664
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2552, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2553, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2554, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2555, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2201
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2556, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.5592
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2557, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2558, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2558
Finished MB training, ran for 60 epochs
Update 2559, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1785
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2560, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2561, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2562, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2563, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2564, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.5442
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2565, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2566, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1896
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2567, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2568, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1450
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2569, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2570, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0753
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2571, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2889
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2572, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2573, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3667
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2574, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1484
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2575, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2576, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.8274
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2577, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4692
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2578, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3049
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2579, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4162
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
Update 2580, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3216
  Episodes: TrainReward -582.8028, l 200.0000, t 124.3581, TestReward -671.6174
New EPOCH! 2580
Update 2581, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2582, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.3191
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2583, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2584, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4124
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2585, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2586, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2887
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2587, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2588, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1819
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2589, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0875
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2590, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2591, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1495
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2592, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2593, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.7134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2594, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.3904
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2595, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.7698
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2596, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2597, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2598, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2599, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2600, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2601, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.3836
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2602, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2603, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0706
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2603
Update 2604, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2921
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2605, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2606, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2607, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2608, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2649
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2609, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2570
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2610, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4303
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2611, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.3531
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2612, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2613, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.3317
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2614, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2615, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2616, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4226
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2617, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2618, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4423
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2619, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.3915
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2620, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2621, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.5107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2622, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0927
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2623, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2624, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4695
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2625, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0841
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2626, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2626
Update 2627, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2622
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2628, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.5536
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2629, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2630, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2631, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.4491
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2632, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2633, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2634, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.1532
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2635, num samples collected 5750, FPS 92
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2636, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2637, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2638, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2639, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5173
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2640, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2076
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2641, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2642, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2643, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5600
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2644, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2645, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2646, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1937
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2647, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5550
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2648, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4999
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2649, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2307
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2649
Update 2650, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4625
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2651, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4295
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2652, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5076
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2653, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2654, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2655, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2776
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2656, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2657, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2658, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2659, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2660, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.6502
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2661, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2662, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5525
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2663, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2664, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2665, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.7696
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2666, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2667, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2668, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2669, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1291
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2670, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4544
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2671, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2672, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2672
Update 2673, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2674, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5215
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2675, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2676, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2677, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2678, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3345
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2679, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2680, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1290
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2681, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2685
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2682, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0679
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2683, num samples collected 5750, FPS 91
  Algorithm: train_loss 1.0451
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2684, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2685, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2686, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2965
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2687, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2955
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2688, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2263
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2689, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2690, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4203
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2691, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1180
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2692, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2954
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2693, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2694, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2695, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3473
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2695
Update 2696, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2697, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4514
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2698, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2699, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2700, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2596
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2701, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2014
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2702, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2703, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2704, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2705, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3932
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2706, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5294
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2707, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2708, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5017
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2709, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.6948
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2710, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2711, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2712, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2856
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2713, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3243
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2714, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1171
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2715, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2716, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3088
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2717, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1400
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2718, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2718
Update 2719, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1518
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2720, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2721, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2722, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1814
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2723, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2724, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2883
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2725, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.8466
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2726, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5346
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2727, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2793
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2728, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2729, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2730, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2977
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2731, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2732, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2358
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2733, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2734, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2072
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2735, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2839
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2736, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2551
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2737, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2738, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2739, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5738
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2740, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2741, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5715
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2741
Update 2742, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2743, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2744, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2745, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2760
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2746, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4755
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2747, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.7491
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2748, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2749, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2750, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3888
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2751, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1654
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2752, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2753, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4037
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2754, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2755, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0773
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2756, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2757, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2758, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5686
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2759, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2760, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5055
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2761, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1500
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2762, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1318
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2763, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3257
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2764, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2604
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2764
Update 2765, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.8232
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2766, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1472
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2767, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2768, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2769, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4275
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2770, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2771, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0703
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2772, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3966
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2773, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2774, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1194
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2775, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3457
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2776, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2777, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1423
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2778, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1870
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2779, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5295
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2780, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3126
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2781, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2782, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3825
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2783, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.7894
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2784, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2785, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2786, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2787, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2787
Update 2788, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6881
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2789, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1942
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2790, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2791, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2792, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2793, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2794, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0700
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2795, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2796, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1960
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2797, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3570
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2798, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2799, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3318
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2800, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4612
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2801, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2802, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2803, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3117
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2804, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2805, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2880
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2806, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2807, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2808, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6153
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2809, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2810, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5809
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2810
Update 2811, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2812, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4302
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2813, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2814, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2815, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2816, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3258
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2817, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2818, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5725
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2819, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2820, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4761
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2821, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2822, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4756
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2823, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2921
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2824, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2825, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2826, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2827, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.8403
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2828, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2977
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2829, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2830, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1183
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2831, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2832, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3461
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2833, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2833
Update 2834, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2835, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2836, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2837, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2901
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2838, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1814
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2839, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4429
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2840, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2841, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2842, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2843, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2844, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1469
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2845, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1128
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2846, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2847, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3278
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2848, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5890
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2849, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2850, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2851, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2852, num samples collected 5750, FPS 90
  Algorithm: train_loss 1.0335
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2853, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2854, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4118
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2855, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2856, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2856
Update 2857, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2858, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2859, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3740
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2860, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2861, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2862, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2542
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2863, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2864, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2865, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2866, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2867, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2185
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2868, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5426
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2869, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3949
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2870, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5930
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2871, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2872, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2873, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2874, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2875, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3825
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2876, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6751
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2877, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0519
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2878, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2343
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2879, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2879
Update 2880, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2881, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1782
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2882, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1067
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2883, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2884, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2885, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3209
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2886, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3169
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2887, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2888, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2889, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1979
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2890, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2891, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6037
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2892, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2893, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2894, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3412
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2895, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2896, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2843
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2897, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2898, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5942
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2899, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5065
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2900, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2901, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1831
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2902, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4307
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2902
Update 2903, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2904, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2905, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2906, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2907, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5233
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2908, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2909, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1773
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2910, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2911, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2912, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2913, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2914, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6158
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2915, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0691
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2916, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2917, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5628
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2918, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2692
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2919, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4666
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2920, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2921, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1055
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2922, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2923, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2924, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2925, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.9802
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2925
Update 2926, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2827
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2927, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0696
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2928, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1850
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2929, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2035
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2930, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2409
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2931, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.7209
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2932, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2933, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5139
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2934, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0954
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2935, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2936, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2937, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2938, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2939, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.7349
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2940, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2941, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2942, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2943, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2940
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2944, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0818
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2945, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.7008
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2946, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2947, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2560
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2948, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2948
Update 2949, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2950, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.7058
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2951, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1878
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2952, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3495
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2953, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2954, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2955, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2956, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2957, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2958, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1291
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2959, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2960, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3846
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2961, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4254
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2962, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2963, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6470
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2964, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2965, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6402
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2966, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2967, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3906
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2968, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1917
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2969, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2970, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0941
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2971, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2971
Update 2972, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0690
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2973, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2974, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2975, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2976, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2977, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2978, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2979, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3859
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2980, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2981, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2982, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1879
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2983, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3300
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2984, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.8128
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2985, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2986, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2987, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5498
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2988, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2989, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3603
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2990, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0978
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2991, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2727
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2992, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5358
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2993, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2867
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2994, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0919
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 2994
Update 2995, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2996, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6493
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2997, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2998, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0838
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 2999, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3000, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3001, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3834
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3002, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0772
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3003, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2251
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3004, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3005, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2943
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3006, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3007, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3008, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3009, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3294
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3010, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1651
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3011, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4287
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3012, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3013, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.9863
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3014, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0744
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3015, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2901
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3016, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3017, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2343
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3017
Update 3018, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3019, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2835
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3020, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2455
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3021, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3022, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3023, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3024, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2777
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3025, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3026, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2565
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3027, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3028, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0374
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3029, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4700
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3030, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3031, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3887
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3032, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6123
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3033, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6083
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3034, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3250
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3035, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2484
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3036, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2110
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3037, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3038, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3039, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3040, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3040
Update 3041, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6808
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3042, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5048
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3043, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3044, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1561
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3045, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3046, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3047, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.8990
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3048, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3049, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3050, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3051, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3052, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0844
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3053, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3054, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3055, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3056, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2789
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3057, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1198
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3058, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2849
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3059, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2660
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3060, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3061, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3062, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4610
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3063, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3063
Update 3064, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3065, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4188
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3066, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0736
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3067, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5695
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3068, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3820
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3069, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1520
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3070, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2882
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3071, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3072, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3073, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3074, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3075, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2850
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3076, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2512
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3077, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3078, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3079, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1146
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3080, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5878
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3081, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3082, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4373
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3083, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3084, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4205
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3085, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3086, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3086
Update 3087, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5218
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3088, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4872
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3089, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3109
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3090, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3091, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3092, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3573
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3093, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3094, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3095, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3096, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4230
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3097, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3123
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3098, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3439
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3099, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3100, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3101, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3102, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3103, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2984
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3104, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.6516
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3105, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3106, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2792
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3107, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3108, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3109, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3109
Update 3110, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3111, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4970
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3112, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3113, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3114, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2832
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3115, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3116, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3117, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3118, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3119, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3120, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.6093
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3121, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4416
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3122, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3123, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.6606
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3124, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3125, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2680
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3126, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2582
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3127, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4774
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3128, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3129, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3130, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3131, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5339
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3132, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3132
Update 3133, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4764
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3134, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3135, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0765
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3136, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3137, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2924
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3138, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3139, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5882
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3140, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3141, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3770
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3142, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3143, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3144, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4777
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3145, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3146, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3147, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2743
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3148, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3149, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3060
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3150, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5300
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3151, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3152, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3153, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3154, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3155, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1059
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3155
Update 3156, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3157, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4891
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3158, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3159, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3160, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1845
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3161, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3488
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3162, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2894
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3163, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3164, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1718
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3165, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3219
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3166, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4903
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3167, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3168, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3169, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3170, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3171, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3172, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3559
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3173, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2231
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3174, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3926
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3175, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3176, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4950
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3177, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3178, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.6497
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3178
Update 3179, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3180, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3181, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3182, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4495
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3183, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3184, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3185, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3477
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3186, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3187, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3188, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2853
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3189, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3190, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3191, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4349
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3192, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3193, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4052
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3194, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3195, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3196, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3197, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.6900
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3198, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2578
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3199, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3422
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3200, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4064
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3201, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3201
Update 3202, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1411
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3203, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3204, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3205, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4339
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3206, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0730
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3207, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3577
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3208, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4946
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3209, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4187
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3210, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5802
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3211, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4575
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3212, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3213, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3214, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3215, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3216, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3466
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3217, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3218, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3219, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3220, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3221, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3222, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3223, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2322
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3224, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3224
Update 3225, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3226, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3227, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0659
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3228, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5962
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3229, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3715
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3230, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3231, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.8170
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3232, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.6837
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3233, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4579
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3234, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3235, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1651
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3236, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3237, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3238, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2894
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3239, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3240, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3241, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2452
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3242, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3243, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1944
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3244, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3245, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3246, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3247, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3247
Update 3248, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6225
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3249, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0903
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3250, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3251, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3252, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1173
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3253, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3254, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3255, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3256, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5167
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3257, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3258, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1265
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3259, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3260, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3261, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3262, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3752
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3263, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.7607
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3264, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2689
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3265, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1864
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3266, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4505
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3267, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3268, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3654
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3269, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3270, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2828
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3270
Update 3271, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1386
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3272, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0838
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3273, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3274, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3275, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4604
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3276, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3277, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3278, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3529
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3279, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2589
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3280, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3238
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3281, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3282, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3198
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3283, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3284, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3285, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4141
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3286, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3287, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1576
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3288, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3289, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6464
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3290, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3291, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3292, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3293, num samples collected 5750, FPS 87
  Algorithm: train_loss 1.0602
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3293
Update 3294, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3295, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4420
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3296, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3297, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3298, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3299, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2173
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3300, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3301, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3302, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3303, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1683
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3304, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3305, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3306, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2021
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3307, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3308, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1175
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3309, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1025
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3310, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3208
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3311, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3312, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.7611
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3313, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.9096
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3314, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2139
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3315, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2915
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3316, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3316
Update 3317, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3318, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3319, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1058
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3320, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3321, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.8693
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3322, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1904
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3323, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3324, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2974
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3325, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3326, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4701
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3327, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3328, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0372
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3329, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1580
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3330, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3769
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3331, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3332, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3333, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0975
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3334, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3335, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3240
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3336, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3337, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3338, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5468
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3339, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3339
Update 3340, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2675
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3341, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3342, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3200
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3343, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3344, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2662
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3345, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1542
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3346, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1361
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3347, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3348, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3349, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3350, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2918
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3351, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4793
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3352, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3353, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5034
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3354, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5951
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3355, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0357
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3356, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3357, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2763
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3358, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3359, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3780
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3360, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3361, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3362, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2484
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3362
Update 3363, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3364, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.9390
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3365, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.7090
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3366, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3367, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3368, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3369, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3745
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3370, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3371, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1926
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3372, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3373, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3374, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1111
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3375, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4628
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3376, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3377, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5315
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3378, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3379, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3380, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3381, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2605
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3382, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3064
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3383, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2722
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3384, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3385, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3385
Update 3386, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2758
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3387, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3388, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3389, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3390, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3091
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3391, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3392, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5217
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3393, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2487
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3394, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3395, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3396, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6823
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3397, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3398, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3222
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3399, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3400, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1203
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3401, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3402, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2210
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3403, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3404, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3405, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2751
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3406, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6680
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3407, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3408, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3408
Update 3409, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1835
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3410, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3411, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3412, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3413, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3414, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6760
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3415, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0848
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3416, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3417, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6840
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3418, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3419, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3420, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1634
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3421, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3422, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3423, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.9692
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3424, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3425, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2459
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3426, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3427, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3428, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3429, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1228
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3430, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2561
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3431, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3431
Update 3432, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3433, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2524
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3434, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3435, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1868
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3436, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0829
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3437, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3438, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2007
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3439, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2909
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3440, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5090
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3441, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3442, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3443, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3444, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3341
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3445, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4731
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3446, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3447, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3448, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3449, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3450, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3451, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3452, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3453, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1092
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3454, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3454
Update 3455, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3456, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2445
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3457, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1238
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3458, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4981
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3459, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3460, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0864
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3461, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3462, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3463, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3464, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2043
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3465, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2487
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3466, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1744
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3467, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3468, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1820
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3469, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.6006
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3470, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3471, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3472, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3473, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3474, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4298
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3475, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5994
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3476, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4372
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3477, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3054
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3477
Update 3478, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3479, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3480, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3481, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2171
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3482, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.7381
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3483, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3484, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3485, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3486, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3487, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0723
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3488, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3489, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2647
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3490, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4721
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3491, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3455
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3492, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3493, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.7592
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3494, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3495, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3649
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3496, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3497, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2895
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3498, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3499, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1880
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3500, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3500
Update 3501, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5306
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3502, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2657
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3503, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1269
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3504, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3505, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3506, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.8890
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3507, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3508, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3509, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3510, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3511, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3512, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3513, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3514, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3515, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3516, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0827
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3517, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3518, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3962
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3519, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3613
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3520, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2463
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3521, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3522, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4717
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3523, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3523
Update 3524, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3525, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4426
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3526, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3527, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3486
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3528, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4467
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3529, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3530, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3531, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3532, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3533, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3534, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5581
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3535, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3274
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3536, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0396
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3537, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3538, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4890
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3539, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2733
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3540, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3541, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3542, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2119
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3543, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2834
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3544, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3545, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2370
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3546, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3546
Update 3547, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0737
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3548, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0376
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3549, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2763
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3550, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3551, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3552, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4117
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3553, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3554, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3555, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4799
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3556, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3237
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3557, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3558, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4396
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3559, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3560, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3561, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3562, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2429
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3563, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3193
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3564, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2115
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3565, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.7601
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3566, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3567, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5874
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3568, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3569, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3569
Update 3570, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3571, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3572, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3573, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.7636
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3574, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3575, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5095
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3576, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3577, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4626
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3578, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3997
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3579, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3580, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3581, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3582, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3583, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3584, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3585, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5261
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3586, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1705
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3587, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3588, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3589, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2421
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3590, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1086
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3591, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3592, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3495
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3592
Update 3593, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.8856
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3594, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3595, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5185
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3596, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3597, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3598, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3599, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1520
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3600, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4811
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3601, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3602, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3603, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4051
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3604, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1472
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3605, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.8867
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3606, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3607, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3608, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3609, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3610, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3611, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0689
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3612, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3613, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4310
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3614, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3615, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3615
Update 3616, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3617, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5567
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3618, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3619, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3620, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2826
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3621, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3622, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0541
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3623, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2909
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3624, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5768
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3625, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3626, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4788
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3627, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3628, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3629, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3630, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1986
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3631, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1510
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3632, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1667
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3633, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4266
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3634, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3635, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3218
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3636, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3637, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3638, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3638
Update 3639, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3640, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3641, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3167
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3642, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3643, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3644, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3645, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3646, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2884
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3647, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4795
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3648, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3649, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1989
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3650, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3651, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0820
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3652, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3297
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3653, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.6644
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3654, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3336
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3655, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2827
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3656, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1875
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3657, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1523
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3658, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0970
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3659, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4042
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3660, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3661, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3661
Update 3662, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0378
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3663, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.8199
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3664, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3665, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3666, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3667, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4960
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3668, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3669, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3238
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3670, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2275
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3671, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1951
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3672, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1743
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3673, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3674, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0712
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3675, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1106
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3676, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1095
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3677, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5264
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3678, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3679, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2760
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3680, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3433
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3681, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3682, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3683, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0724
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3684, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3684
Update 3685, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2527
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3686, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3687, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3688, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3689, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3690, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3691, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3692, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3693, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3694, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1162
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3695, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.8176
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3696, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3697, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2223
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3698, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3699, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3854
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3700, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1358
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3701, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3702, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3703, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3894
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3704, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4850
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3705, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2604
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3706, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3707, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3707
Update 3708, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3709, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0784
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3710, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3711, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3712, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3713, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4816
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3714, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3614
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3715, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3716, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3717, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3718, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2442
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3719, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3720, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5034
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3721, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1311
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3722, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3723, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4504
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3724, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3725, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4273
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3726, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3727, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3728, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2536
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3729, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2809
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3730, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2461
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3730
Update 3731, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3732, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1862
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3733, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3734, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3735, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3736, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1953
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3737, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3738, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1397
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3739, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3740, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3741, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1523
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3742, num samples collected 5750, FPS 85
  Algorithm: train_loss 1.0148
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3743, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.7169
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3744, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1919
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3745, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3395
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3746, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3458
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3747, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3748, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3749, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4893
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3750, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3751, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3752, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2617
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3753, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3753
Update 3754, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.7907
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3755, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3756, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4954
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3757, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3758, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3759, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3760, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3761, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3762, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3763, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3764, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2028
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3765, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2698
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3766, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3767, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3619
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3768, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3769, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3028
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3770, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1900
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3771, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3772, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5879
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3773, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3774, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2717
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3775, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3776, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3776
Update 3777, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.8750
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3778, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3779, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3131
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3780, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1998
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3781, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1684
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3782, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2923
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3783, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3784, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1790
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3785, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2514
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3786, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1152
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3787, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3788, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3789, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3790, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3791, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1202
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3792, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3793, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3794, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4868
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3795, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2899
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3796, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3797, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1853
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3798, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2371
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3799, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3799
Update 3800, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0677
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3801, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3802, num samples collected 5750, FPS 84
  Algorithm: train_loss 1.1228
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3803, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0408
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3804, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3805, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1650
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3806, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2878
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3807, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1441
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3808, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3809, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3810, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3811, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2584
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3812, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3813, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3814, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3857
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3815, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3816, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1632
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3817, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3217
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3818, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4288
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3819, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3820, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3821, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3822, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3822
Update 3823, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3824, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3700
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3825, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2868
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3826, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4095
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3827, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3147
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3828, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3829, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1718
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3830, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3831, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3832, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3833, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3834, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3835, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1579
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3836, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5644
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3837, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5389
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3838, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3839, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3840, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3841, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3842, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3843, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2785
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3844, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3088
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3845, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3832
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3845
Update 3846, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1878
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3847, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3848, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3849, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3171
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3850, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3852
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3851, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0696
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3852, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0519
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3853, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3854, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5705
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3855, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2815
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3856, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3857, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2755
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3858, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3859, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3860, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5286
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3861, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3862, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0346
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3863, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2389
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3864, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3865, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3866, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4947
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3867, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3868, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2354
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3868
Update 3869, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4253
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3870, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2462
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3871, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2084
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3872, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1967
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3873, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3874, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3875, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1429
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3876, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0983
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3877, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1947
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3878, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3879, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1173
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3880, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3881, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3882, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2456
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3883, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3884, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3885, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2953
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3886, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3887, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1443
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3888, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3208
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3889, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3897
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3890, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.6961
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3891, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3739
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3891
Update 3892, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1838
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3893, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3894, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3473
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3895, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3126
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3896, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3897, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3898, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3899, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3900, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3901, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3902, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4204
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3903, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3904, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3905, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3906, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3907, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.6678
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3908, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3909, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3910, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.7437
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3911, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2088
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3912, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3913, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3914, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3914
Update 3915, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3916, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3917, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3918, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1935
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3919, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3920, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3921, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0679
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3922, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3923, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3924, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.7023
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3925, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3926, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3927, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5622
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3928, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.5312
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3929, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1201
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3930, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3931, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.5374
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3932, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3933, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3126
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3934, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3935, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2148
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3936, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3937, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4777
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3937
Finished MB training, ran for 60 epochs
Update 3938, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4940
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3939, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4188
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3940, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3941, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3942, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3943, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3010
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3944, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3945, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2102
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3946, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1302
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3947, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3948, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3949, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3950, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3951, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3432
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3952, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3953, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3531
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3954, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2737
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3955, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3956, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3957, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4434
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3958, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3959, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
Update 3960, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -386.9723, l 200.0000, t 146.3662, TestReward -1101.7343
New EPOCH! 3960
Update 3961, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3962, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.3560
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3963, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3964, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.5253
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3965, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2554
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3966, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.3057
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3967, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.1198
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3968, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3969, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3970, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3971, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3972, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3973, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.1362
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3974, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.4886
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3975, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.6692
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3976, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.3424
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3977, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3978, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3979, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.8650
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3980, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3981, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3982, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3983, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0775
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3984, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 3984
Update 3985, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.1053
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3986, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3987, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2313
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3988, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2788
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3989, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3990, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.1596
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3991, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3992, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.6012
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3993, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3994, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.2556
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3995, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3996, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.3052
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3997, num samples collected 6000, FPS 70
  Algorithm: train_loss 0.5344
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3998, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 3999, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4000, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3281
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4001, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4002, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4003, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4004, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1264
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4005, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4006, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4007, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4622
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4008, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4008
Update 4009, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4010, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4011, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1791
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4012, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4013, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4014, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4738
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4015, num samples collected 6000, FPS 69
  Algorithm: train_loss 1.0047
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4016, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2745
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4017, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4018, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4979
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4019, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4020, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4021, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4022, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4023, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4024, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4025, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4026, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4027, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4028, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4029, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3401
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4030, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4264
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4031, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4032, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2061
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4032
Update 4033, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4034, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4035, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.8553
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4036, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0307
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4037, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7938
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4038, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3437
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4039, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4040, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4041, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4042, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3595
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4043, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4044, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4045, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4046, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5177
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4047, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5263
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4048, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4049, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4050, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4051, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4052, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1848
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4053, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0754
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4054, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0806
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4055, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4056, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4056
Update 4057, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4058, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1788
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4059, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4060, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4061, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4773
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4062, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0644
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4063, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4064, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3170
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4065, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2281
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4066, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4067, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.6394
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4068, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1986
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4069, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3125
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4070, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4071, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0670
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4072, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4073, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0446
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4074, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.6553
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4075, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4076, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4077, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3785
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4078, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4079, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4080, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4080
Update 4081, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7049
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4082, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0775
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4083, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1989
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4084, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4085, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4086, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4087, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1641
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4088, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4089, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3638
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4090, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4091, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4092, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0870
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4093, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2714
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4094, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1880
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4095, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4096, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4097, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4098, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4099, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1793
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4100, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1544
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4101, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4102, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4283
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4103, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5412
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4104, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4104
Update 4105, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4106, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2815
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4107, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4108, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7447
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4109, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1532
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4110, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4111, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4502
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4112, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4113, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4114, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4115, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4116, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1131
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4117, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2058
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4118, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4119, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4120, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4121, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5624
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4122, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7620
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4123, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4124, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4125, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5678
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4126, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4127, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4128, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4128
Update 4129, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4130, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0691
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4131, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4132, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4133, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4028
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4134, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2185
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4135, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3200
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4136, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4137, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4138, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.6762
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4139, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1817
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4140, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4141, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4142, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4143, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4144, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4145, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4146, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4147, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1341
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4148, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4523
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4149, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3687
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4150, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4151, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3496
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4152, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1460
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4152
Update 4153, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1010
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4154, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3599
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4155, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4156, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4157, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1770
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4158, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4159, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1930
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4160, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4161, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4162, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4163, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4164, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3692
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4165, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4166, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4167, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4105
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4168, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4169, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4972
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4170, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2018
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4171, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5037
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4172, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5965
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4173, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4174, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2747
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4175, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3310
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4176, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4176
Update 4177, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4178, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4179, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.9788
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4180, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3257
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4181, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4182, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4183, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4184, num samples collected 6000, FPS 69
  Algorithm: train_loss 1.0828
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4185, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4186, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4389
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4187, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4188, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4189, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4190, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4191, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0953
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4192, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7936
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4193, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4194, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4195, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4196, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1039
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4197, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4198, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4199, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4200, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0937
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4200
Update 4201, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4202, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3021
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4203, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4204, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4205, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2298
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4206, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4207, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3983
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4208, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1362
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4209, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4210, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4211, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3859
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4212, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4213, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4674
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4214, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4215, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2456
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4216, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4217, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4218, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4219, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4220, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4221, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5075
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4222, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4223, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4224, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4223
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4224
Update 4225, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4066
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4226, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4227, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4228, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4229, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0913
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4230, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4937
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4231, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4232, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3605
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4233, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4234, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.6019
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4235, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4236, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4237, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4238, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4239, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3490
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4240, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4241, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3782
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4242, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4928
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4243, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4244, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2212
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4245, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4246, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4247, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2623
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4248, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4530
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4248
Update 4249, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4250, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4251, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4804
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4252, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4253, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4254, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0979
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4255, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4256, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4257, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4258, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1522
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4259, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1441
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4260, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5442
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4261, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1087
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4262, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4263, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4264, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4334
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4265, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.8472
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4266, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0297
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4267, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5316
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4268, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4269, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4270, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4271, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4272, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4272
Update 4273, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4274, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4275, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4276, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4277, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2852
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4278, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4279, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4280, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1989
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4281, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4282, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0812
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4283, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4284, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4285, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4286, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.8361
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4287, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6761
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4288, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0692
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4289, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3695
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4290, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3463
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4291, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1025
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4292, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0764
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4293, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4294, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4295, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4296, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0734
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4296
Update 4297, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1995
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4298, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2729
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4299, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3601
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4300, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4301, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2908
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4302, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0881
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4303, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4304, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4305, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4306, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4307, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4308, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4309, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1714
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4310, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4311, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7532
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4312, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4313, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0446
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4314, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4315, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4316, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4160
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4317, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4318, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4319, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4320, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4320
Update 4321, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4322, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1261
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4323, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4324, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4325, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1941
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4326, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4327, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0366
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4328, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2670
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4329, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4330, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4331, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3466
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4332, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0949
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4333, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1241
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4334, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4335, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1785
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4336, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2574
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4337, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1000
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4338, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2830
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4339, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2717
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4340, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4341, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2575
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4342, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1537
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4343, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5432
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4344, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4404
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4344
Update 4345, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3697
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4346, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4347, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6267
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4348, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1364
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4349, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3064
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4350, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1809
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4351, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1869
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4352, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0689
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4353, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4354, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4355, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4356, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4357, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4358, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1990
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4359, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4360, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4361, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1004
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4362, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3475
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4363, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0362
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4364, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4365, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4366, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4367, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7224
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4368, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4368
Update 4369, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4370, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6038
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4371, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4372, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4373, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1987
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4374, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4375, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2708
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4376, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4377, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3201
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4378, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3388
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4379, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6643
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4380, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2327
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4381, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4382, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4383, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4384, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4385, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4386, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4387, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4388, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2829
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4389, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4390, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4391, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2320
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4392, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4392
Update 4393, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4394, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1827
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4395, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4396, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4397, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4398, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0940
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4399, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4400, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4401, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5111
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4402, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0960
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4403, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2948
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4404, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4025
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4405, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4406, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4407, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4408, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1855
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4409, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2493
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4410, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4411, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7360
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4412, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4413, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4414, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4415, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4089
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4416, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4416
Update 4417, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4418, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4419, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4420, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2192
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4421, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3326
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4422, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1016
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4423, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4424, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2235
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4425, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4269
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4426, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4427, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4419
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4428, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3551
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4429, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4430, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3437
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4431, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4432, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4433, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4434, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4435, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4436, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5054
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4437, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4567
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4438, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1835
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4439, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5464
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4440, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4440
Update 4441, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4442, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4443, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4654
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4444, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4445, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3637
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4446, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2697
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4447, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0791
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4448, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4449, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2014
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4450, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4451, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5799
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4452, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4453, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4454, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4000
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4455, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4456, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4457, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1827
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4458, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4459, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2849
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4460, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3350
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4461, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4462, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4463, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3547
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4464, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.8029
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4464
Update 4465, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4466, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3300
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4467, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6457
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4468, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4469, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4470, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4471, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4472, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1989
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4473, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5162
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4474, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4475, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1845
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4476, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4477, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4478, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3381
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4479, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4043
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4480, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4481, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5670
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4482, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4483, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4484, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4485, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4486, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5143
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4487, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4488, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4488
Update 4489, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4490, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4491, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4492, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0917
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4493, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4494, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4495, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1452
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4496, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2428
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4497, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3581
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4498, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2015
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4499, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3896
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4500, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4719
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4501, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1670
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4502, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4503, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0869
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4504, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1319
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4505, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4506, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4507, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2812
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4508, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4075
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4509, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5110
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4510, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4511, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4512, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5554
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4512
Update 4513, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3838
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4514, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4515, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4516, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2157
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4517, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4518, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4900
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4519, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4520, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4521, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0719
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4522, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3242
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4523, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4524, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5380
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4525, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4526, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3142
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4527, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4528, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1442
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4529, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2429
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4530, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4531, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4532, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4533, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4534, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1840
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4535, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4536, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4536
Update 4537, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1361
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4538, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4539, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2890
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4540, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3053
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4541, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4542, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7395
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4543, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4544, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3733
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4545, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4546, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4547, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4548, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2445
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4549, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4550, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1696
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4551, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4552, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3410
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4553, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4554, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1657
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4555, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4556, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4557, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1859
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4558, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4559, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3311
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4560, num samples collected 6000, FPS 67
  Algorithm: train_loss 1.1100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4560
Update 4561, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0959
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4562, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4563, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2462
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4564, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4565, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4566, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1825
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4567, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4568, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4569, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1831
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4570, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4571, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4572, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1359
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4573, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5917
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4574, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6871
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4575, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3425
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4576, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4577, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4578, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4579, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4745
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4580, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4581, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2335
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4582, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4583, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4584, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0951
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4584
Update 4585, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2286
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4586, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1858
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4587, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4588, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4589, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3477
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4590, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4591, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2966
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4592, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6200
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4593, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1625
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4594, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4595, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2681
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4596, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4597, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4598, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4599, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5699
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4600, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4601, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4602, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4603, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4604, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1579
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4605, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3874
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4606, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2615
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4607, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4608, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4980
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4608
Update 4609, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4610, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4611, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2719
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4612, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4613, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.9496
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4614, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2922
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4615, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4133
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4616, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4617, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4949
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4618, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4619, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1740
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4620, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0670
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4621, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1228
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4622, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4623, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4624, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4625, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2120
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4626, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4627, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2717
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4628, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4629, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4630, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4631, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1642
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4632, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4632
Update 4633, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4634, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3292
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4635, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3112
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4636, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2174
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4637, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4638, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4639, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4640, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4641, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4642, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2080
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4643, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4644, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.8345
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4645, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1344
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4646, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4647, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4574
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4648, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4649, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0965
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4650, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4467
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4651, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2732
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4652, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1615
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4653, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1802
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4654, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4655, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4656, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4656
Update 4657, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4658, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2009
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4659, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4660, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5261
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4661, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4662, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2375
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4663, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2699
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4664, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2773
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4665, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4666, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4667, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4668, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4782
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4669, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4670, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2686
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4671, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2037
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4672, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2391
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4673, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4674, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4675, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2069
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4676, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4677, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1196
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4678, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3690
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4679, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4680, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4680
Update 4681, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5005
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4682, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4683, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4684, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2759
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4685, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6928
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4686, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4687, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4688, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1390
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4689, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4690, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4691, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0859
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4692, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1024
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4693, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4694, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4695, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4696, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4697, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1986
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4698, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5110
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4699, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5112
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4700, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4701, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4702, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2682
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4703, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4704, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4704
Update 4705, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3300
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4706, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1354
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4707, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4708, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3538
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4709, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4710, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5357
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4711, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2661
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4712, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4713, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4714, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4715, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1027
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4716, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3257
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4717, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4718, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2825
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4719, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0805
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4720, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4721, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5018
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4722, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1949
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4723, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3711
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4724, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4725, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4726, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4727, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4728, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3539
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4728
Update 4729, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4253
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4730, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4372
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4731, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4732, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2315
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4733, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3346
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4734, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4735, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4736, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0833
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4737, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4738, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4739, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1266
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4740, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1744
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4741, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4956
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4742, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4743, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4216
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4744, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4745, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0690
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4746, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4747, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0898
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4748, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4749, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4750, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6247
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4751, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1798
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4752, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2507
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4752
Update 4753, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3985
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4754, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2389
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4755, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4756, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2035
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4757, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0862
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4758, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4759, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4894
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4760, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2860
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4761, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6798
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4762, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4763, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0790
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4764, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4765, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4766, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4767, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4768, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2847
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4769, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4770, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4771, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4772, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2755
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4773, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4774, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4775, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0835
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4776, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4776
Update 4777, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4778, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2921
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4779, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4780, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0481
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4781, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4782, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1907
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4783, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4784, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1322
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4785, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4786, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4787, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3596
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4788, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2281
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4789, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4790, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4791, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4792, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1610
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4793, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0695
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4794, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4795, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4036
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4796, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4841
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4797, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5238
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4798, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4799, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4741
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4800, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4800
Update 4801, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4802, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0982
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4803, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4804, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4805, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4969
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4806, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4807, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0798
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4808, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4809, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4810, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4811, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1020
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4812, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2903
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4813, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4814, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4815, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6062
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4816, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5879
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4817, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3236
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4818, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3655
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4819, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4820, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2868
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4821, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3495
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4822, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4823, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0773
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4824, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4824
Update 4825, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4826, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1901
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4827, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4828, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1645
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4829, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4830, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1568
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4831, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2911
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4832, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4833, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4834, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0932
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4835, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2362
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4836, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4598
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4837, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4838, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4839, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4840, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4841, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4842, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4843, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4844, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4066
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4845, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4846, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4847, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.9618
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4848, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0350
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4848
Update 4849, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1885
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4850, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4851, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1919
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4852, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4853, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1985
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4854, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2327
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4855, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2198
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4856, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4857, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4858, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2803
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4859, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4860, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.5107
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4861, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4862, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1215
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4863, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4864, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4865, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4866, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4867, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2518
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4868, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.7571
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4869, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4870, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4871, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4872, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3616
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4872
Update 4873, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4874, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0932
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4875, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2919
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4876, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2605
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4877, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2466
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4878, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.8662
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4879, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4880, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4881, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4882, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.5141
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4883, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2298
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4884, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4885, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4886, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1702
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4887, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4888, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.8689
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4889, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4890, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0859
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4891, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4892, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4893, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4894, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4895, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4896, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3447
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4896
Update 4897, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1973
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4898, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2719
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4899, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4900, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3648
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4901, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4902, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1537
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4903, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0824
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4904, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2670
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4905, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4906, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1840
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4907, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4908, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4909, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1397
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4910, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4911, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0828
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4912, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4913, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4914, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3011
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4915, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4916, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4265
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4917, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.5510
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4918, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4919, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.7392
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4920, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4606
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4920
Update 4921, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4922, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0319
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4923, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4924, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4925, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3515
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4926, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4927, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4928, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0929
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4929, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4495
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4930, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4931, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1368
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4932, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4933, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.5243
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4934, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2404
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4935, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1007
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4936, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0744
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4937, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4938, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4970
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4939, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4940, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4941, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3767
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4942, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1091
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4943, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4944, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2735
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4944
Update 4945, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4946, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4947, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2435
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4948, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4949, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4950, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0383
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4951, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4952, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4953, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3389
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4954, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4955, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1913
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4956, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3884
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4957, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4958, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.6910
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4959, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4960, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4961, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4962, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.7105
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4963, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0945
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4964, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0894
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4965, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4966, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4967, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2303
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4968, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1335
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4968
Update 4969, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4970, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4971, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4972, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4973, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4974, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4975, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4976, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4666
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4977, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4978, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4979, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4980, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.6364
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4981, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.7299
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4982, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4983, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3787
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4984, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4985, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0874
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4986, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4987, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4717
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4988, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2171
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4989, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4990, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1268
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4991, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4992, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 4992
Update 4993, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4994, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4995, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4996, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4997, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2398
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4998, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4589
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 4999, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4944
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5000, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5001, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5002, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3874
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5003, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2946
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5004, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5005, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5006, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5007, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5008, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1021
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5009, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1727
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5010, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.6406
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5011, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1139
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5012, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5013, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5014, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5015, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5016, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5016
Update 5017, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5018, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5019, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5020, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4596
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5021, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5022, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1803
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5023, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5024, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5025, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5026, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5027, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.6365
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5028, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5029, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5030, num samples collected 6000, FPS 66
  Algorithm: train_loss 1.1439
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5031, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3096
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5032, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5033, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5034, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5035, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5036, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5037, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5038, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0942
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5039, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5040, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4291
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5040
Update 5041, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5042, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1355
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5043, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5044, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5045, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5046, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5047, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3657
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5048, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5049, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3777
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5050, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2991
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5051, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4776
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5052, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2334
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5053, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1732
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5054, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5055, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1711
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5056, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.6718
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5057, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5058, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5059, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2058
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5060, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3249
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5061, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5062, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5063, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5064, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5064
Update 5065, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5066, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5067, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5068, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4205
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5069, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5070, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5071, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5072, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3864
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5073, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1994
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5074, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5075, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.7409
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5076, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5077, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5078, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1256
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5079, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5080, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5081, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5082, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4675
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5083, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5084, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2375
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5085, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5086, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5087, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1793
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5088, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5088
Update 5089, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5090, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1028
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5091, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5092, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0678
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5093, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5094, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1309
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5095, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5096, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5097, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2791
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5098, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5099, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4299
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5100, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0762
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5101, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2905
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5102, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5103, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5104, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5105, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5106, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6463
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5107, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5108, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3788
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5109, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5110, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4535
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5111, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5112, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6499
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5112
Update 5113, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5114, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1730
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5115, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5116, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5117, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5118, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2569
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5119, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5120, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1639
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5121, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4498
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5122, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5152
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5123, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5124, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3837
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5125, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3051
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5126, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5127, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5128, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2376
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5129, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2168
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5130, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4710
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5131, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5132, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5133, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5134, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2527
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5135, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5136, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5136
Update 5137, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2848
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5138, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5139, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1353
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5140, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1152
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5141, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6454
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5142, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5143, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5144, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5145, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2870
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5146, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5147, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5148, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5149, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5150, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4083
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5151, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4490
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5152, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5153, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0715
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5154, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5155, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1035
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5156, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5157, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3751
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5158, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4382
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5159, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5160, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5160
Update 5161, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5694
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5162, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5163, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2573
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5164, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5165, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5166, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5167, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.7019
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5168, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5169, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5170, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5171, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5172, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3489
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5173, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5174, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0752
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5175, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5176, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4282
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5177, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2627
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5178, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2662
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5179, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1893
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5180, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5181, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1167
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5182, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3192
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5183, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5184, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5184
Update 5185, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5186, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1995
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5187, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5188, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5189, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1875
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5190, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5191, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5192, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5193, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5194, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6228
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5195, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0659
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5196, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6688
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5197, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2056
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5198, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4036
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5199, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5200, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5201, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0632
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5202, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0414
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5203, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4884
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5204, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5205, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5206, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5207, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5208, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.7772
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5208
Update 5209, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0975
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5210, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2303
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5211, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5212, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5213, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1228
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5214, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5215, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5216, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6216
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5217, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4490
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5218, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4872
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5219, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5220, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5221, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5222, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3741
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5223, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2281
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5224, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1992
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5225, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2291
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5226, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5227, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0756
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5228, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5229, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5230, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5231, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2965
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5232, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1504
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5232
Update 5233, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5234, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3414
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5235, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2974
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5236, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5237, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6384
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5238, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1576
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5239, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5240, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5241, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5242, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5105
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5243, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2695
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5244, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1117
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5245, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2438
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5246, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3210
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5247, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5248, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1323
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5249, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5250, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5251, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0826
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5252, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1175
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5253, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2691
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5254, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0796
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5255, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1288
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5256, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5256
Update 5257, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6753
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5258, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0790
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5259, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.9609
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5260, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5261, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5262, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5263, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3857
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5264, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5265, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4541
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5266, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5267, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5268, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5269, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5270, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5271, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5272, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5273, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5274, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0519
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5275, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5276, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5277, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5278, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2333
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5279, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5280, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5280
Update 5281, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1618
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5282, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2605
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5283, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5284, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5285, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5286, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5287, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0946
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5288, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0968
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5289, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6290
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5290, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5291, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5257
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5292, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5293, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5294, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5295, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5296, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5297, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1063
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5298, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5299, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1325
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5300, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4078
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5301, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5497
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5302, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5303, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5304, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.7099
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5304
Update 5305, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5306, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.7036
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5307, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5308, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5309, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5310, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5311, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5312, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0945
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5313, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3453
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5314, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5315, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4777
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5316, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0653
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5317, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0765
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5318, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5319, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3533
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5320, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5321, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4434
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5322, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5759
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5323, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1291
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5324, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5325, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0713
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5326, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5327, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0991
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5328, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5328
Update 5329, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5330, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5331, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5332, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5333, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2767
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5334, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3311
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5335, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0684
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5336, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5337, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5338, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3320
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5339, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5340, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1361
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5341, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5342, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5343, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5344, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5217
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5345, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5346, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3728
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5347, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.7494
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5348, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3278
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5349, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5350, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3984
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5351, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2611
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5352, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5352
Update 5353, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.7467
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5354, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6647
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5355, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2452
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5356, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5357, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5358, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5359, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4175
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5360, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1719
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5361, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5362, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5363, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5364, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5365, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5366, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2620
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5367, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5368, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3190
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5369, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2289
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5370, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5371, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5372, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5373, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5374, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5375, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5376, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5376
Finished MB training, ran for 60 epochs
Update 5377, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5378, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5379, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4248
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5380, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5381, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2343
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5382, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1817
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5383, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5384, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5385, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.7833
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5386, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5387, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5388, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5389, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5390, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5391, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5392, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2543
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5393, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1976
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5394, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5395, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5396, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1987
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5397, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.6238
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5398, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5399, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
Update 5400, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3657
  Episodes: TrainReward -1073.5359, l 200.0000, t 170.6524, TestReward -1466.9416
New EPOCH! 5400
Update 5401, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5402, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5403, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1624
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5404, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5405, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.4533
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5406, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2289
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5407, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3619
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5408, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1918
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5409, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3222
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5410, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1318
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5411, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5412, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5413, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5414, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5415, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3363
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5416, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.4554
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5417, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5418, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3139
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5419, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5420, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2875
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5421, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5422, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0388
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5423, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5424, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5425, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.6558
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5425
Update 5426, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5427, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1966
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5428, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5429, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.4042
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5430, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5431, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5432, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5433, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3823
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5434, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5435, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5436, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.4546
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5437, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5438, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1798
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5439, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5440, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0768
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5441, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1486
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5442, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5443, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5444, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3543
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5445, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5446, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3378
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5447, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.7696
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5448, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5449, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1751
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5450, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5450
Update 5451, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.4749
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5452, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5453, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.4505
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5454, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0963
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5455, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0353
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5456, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1873
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5457, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5458, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0749
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5459, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.1007
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5460, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5461, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.3031
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5462, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.0937
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5463, num samples collected 6250, FPS 57
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5464, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5465, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4690
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5466, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5467, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5468, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5469, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0788
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5470, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1902
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5471, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5472, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1252
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5473, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4163
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5474, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2110
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5475, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5475
Update 5476, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3670
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5477, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5478, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1510
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5479, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2851
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5480, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5481, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5482, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5483, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5484, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5485, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5486, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5487, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5488, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3840
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5489, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1242
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5490, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5491, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5492, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3372
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5493, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5494, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4882
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5495, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3796
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5496, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5497, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5651
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5498, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1662
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5499, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2833
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5500, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5500
Update 5501, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5502, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0553
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5503, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5504, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5505, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5506, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5507, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5508, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5509, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1151
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5510, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.7169
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5511, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3606
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5512, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.8816
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5513, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5514, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2068
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5515, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1728
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5516, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5517, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5518, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6272
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5519, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0795
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5520, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5521, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5522, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5523, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5524, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5525, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1318
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5525
Update 5526, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5527, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5528, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5529, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5543
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5530, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5531, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2633
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5532, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2869
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5533, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5534, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5908
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5535, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5536, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4444
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5537, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5786
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5538, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0356
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5539, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0481
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5540, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5541, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1537
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5542, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5543, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5544, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5545, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5546, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5547, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5548, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5549, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5550, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5550
Update 5551, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2769
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5552, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0824
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5553, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5554, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5555, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5119
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5556, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5557, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4648
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5558, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1434
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5559, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0739
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5560, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2417
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5561, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3979
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5562, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5563, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5564, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5565, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5566, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5567, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5568, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0374
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5569, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5570, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5804
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5571, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1648
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5572, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5573, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5951
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5574, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5575, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5575
Update 5576, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5577, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3349
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5578, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5579, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5580, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1952
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5581, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5582, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2175
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5583, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4540
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5584, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2934
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5585, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5586, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5587, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5588, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4784
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5589, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5590, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5591, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2478
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5592, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5593, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2702
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5594, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2181
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5595, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3817
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5596, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5597, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5598, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4599
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5599, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0712
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5600, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5600
Update 5601, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1970
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5602, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5603, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4410
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5604, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5605, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2275
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5606, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5607, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5608, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5609, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4491
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5610, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5611, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3617
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5612, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5613, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5614, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5615, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6174
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5616, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5617, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5618, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5619, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1660
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5620, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5621, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5622, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0403
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5623, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3305
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5624, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1466
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5625, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6007
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5625
Update 5626, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5627, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0971
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5628, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5629, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4703
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5630, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5631, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5299
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5632, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5633, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5634, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5635, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5636, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1618
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5637, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0356
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5638, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5639, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5640, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5641, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2291
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5642, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.8567
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5643, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2562
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5644, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3778
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5645, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5646, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5647, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3055
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5648, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5649, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3485
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5650, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2935
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5650
Update 5651, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5652, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0489
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5653, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5654, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4887
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5655, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3172
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5656, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5657, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1884
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5658, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3638
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5659, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5660, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5661, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2632
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5662, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5663, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5664, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0961
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5665, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5666, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4310
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5667, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5668, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4663
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5669, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5243
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5670, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2628
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5671, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5672, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0797
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5673, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5674, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2141
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5675, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5675
Update 5676, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2353
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5677, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4371
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5678, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5679, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6390
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5680, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3485
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5681, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5682, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5683, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5684, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1266
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5685, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5686, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5687, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5688, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2383
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5689, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3705
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5690, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5691, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5692, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1757
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5693, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5694, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2719
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5695, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5696, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1288
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5697, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3259
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5698, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5699, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5700, num samples collected 6250, FPS 56
  Algorithm: train_loss 1.1776
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5700
Update 5701, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1625
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5702, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5703, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5704, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0366
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5705, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5706, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2941
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5707, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5708, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5709, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5710, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5711, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5712, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0383
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5713, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5328
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5714, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5715, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5716, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5717, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5718, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5719, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3530
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5720, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.8314
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5721, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5722, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6483
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5723, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2982
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5724, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0907
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5725, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0928
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5725
Update 5726, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5727, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3634
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5728, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.5194
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5729, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5730, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5731, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2918
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5732, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5733, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4158
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5734, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5735, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5736, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5737, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3424
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5738, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5739, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1555
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5740, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5741, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5742, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5743, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3631
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5744, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5745, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0980
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5746, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3192
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5747, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5748, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5749, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5750, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5750
Update 5751, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5752, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5753, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1396
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5754, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2867
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5755, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1259
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5756, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3039
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5757, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3679
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5758, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5759, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.8171
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5760, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5761, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5762, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5763, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2210
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5764, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5765, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5766, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5767, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1767
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5768, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5769, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6749
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5770, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5771, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5772, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2333
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5773, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0775
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5774, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0873
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5775, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1730
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5775
Update 5776, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5777, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5778, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2407
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5779, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4501
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5780, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5781, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2773
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5782, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5783, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5784, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1285
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5785, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5786, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5787, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5788, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5789, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3407
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5790, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5791, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5792, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5793, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4591
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5794, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5795, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5796, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0717
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5797, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4028
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5798, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.7633
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5799, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5800, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5800
Update 5801, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5802, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5803, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5804, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5805, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5806, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1215
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5807, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3287
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5808, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5809, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.6225
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5810, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3418
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5811, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5812, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1667
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5813, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.3106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5814, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2242
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5815, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1356
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5816, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5817, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5818, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.4753
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5819, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.2022
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5820, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5821, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5822, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0926
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5823, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5824, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5825, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5825
Update 5826, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5827, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5828, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1775
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5829, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5830, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5831, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5832, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5833, num samples collected 6250, FPS 56
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5834, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0919
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5835, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5836, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2310
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5837, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6562
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5838, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5724
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5839, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5840, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2205
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5841, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6278
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5842, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5843, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2516
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5844, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5845, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5846, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4157
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5847, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2427
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5848, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5849, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5850, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5850
Update 5851, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5852, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5853, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2518
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5854, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1881
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5855, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5856, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5857, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.9298
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5858, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1435
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5859, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1702
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5860, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0541
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5861, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5862, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5863, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1775
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5864, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2533
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5865, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5866, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3515
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5867, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5868, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5869, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5870, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2874
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5871, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5872, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3734
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5873, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5874, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5875, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5875
Update 5876, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5877, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5878, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4207
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5879, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5880, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5881, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5882, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5883, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5884, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2652
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5885, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1410
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5886, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5887, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5888, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5889, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1622
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5890, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3007
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5891, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5892, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.8634
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5893, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3344
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5894, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5895, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5896, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5897, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5898, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2269
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5899, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2231
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5900, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1367
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5900
Update 5901, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1003
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5902, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5903, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.7178
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5904, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5905, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2026
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5906, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1518
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5907, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5908, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5909, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5910, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5911, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5912, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5913, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1802
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5914, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1565
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5915, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5916, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5917, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2868
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5918, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2263
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5919, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5920, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5921, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4386
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5922, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5923, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5924, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4243
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5925, num samples collected 6250, FPS 55
  Algorithm: train_loss 1.1386
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5925
Update 5926, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5927, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5928, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5929, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5930, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5931, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5932, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2861
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5933, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2098
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5934, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5935, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5936, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1164
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5937, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1017
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5938, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0722
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5939, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5940, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5941, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2818
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5942, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1652
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5943, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6818
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5944, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1353
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5945, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0790
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5946, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6487
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5947, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5948, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5949, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5950, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5950
Update 5951, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5952, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5953, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5954, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5955, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3166
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5956, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5957, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5958, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5959, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5960, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5961, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5962, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3046
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5963, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3024
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5964, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5965, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4311
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5966, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0404
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5967, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3869
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5968, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5969, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5970, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1856
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5971, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5972, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.7015
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5973, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4209
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5974, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3598
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5975, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 5975
Update 5976, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5977, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5978, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5979, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5980, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1580
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5981, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5982, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5983, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6368
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5984, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0729
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5985, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5986, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5987, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5988, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5989, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2823
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5990, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1869
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5991, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5895
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5992, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5993, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5994, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2064
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5995, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5996, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1682
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5997, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2993
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5998, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 5999, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6000, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6000
Update 6001, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2238
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6002, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1995
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6003, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6004, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6005, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4752
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6006, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6007, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2009
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6008, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3415
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6009, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6329
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6010, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6011, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2188
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6012, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6013, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6014, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3177
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6015, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6016, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6017, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6018, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1319
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6019, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6020, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6021, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6022, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6023, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6024, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6025, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6025
Update 6026, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3708
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6027, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6028, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3795
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6029, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6030, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6031, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6032, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6033, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6034, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3159
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6035, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6036, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6037, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.8142
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6038, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6039, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6040, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6041, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2776
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6042, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2389
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6043, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.8346
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6044, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0717
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6045, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6046, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0967
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6047, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6048, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6049, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6050, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6050
Update 6051, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4433
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6052, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6053, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6054, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4979
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6055, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6056, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2356
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6057, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4427
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6058, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6059, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1633
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6060, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1811
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6061, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6062, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6063, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6064, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6065, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1420
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6066, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6067, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6068, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4696
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6069, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6070, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0662
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6071, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2931
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6072, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0695
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6073, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6074, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3805
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6075, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6075
Update 6076, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3451
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6077, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6078, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2734
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6079, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6080, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6081, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6082, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1896
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6083, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6084, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6085, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6086, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6087, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6516
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6088, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6089, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5887
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6090, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1763
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6091, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6092, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6093, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3209
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6094, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6095, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1381
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6096, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6097, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6098, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6099, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6100, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6100
Update 6101, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4850
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6102, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6103, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2372
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6104, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.7789
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6105, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6106, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6107, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6108, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6109, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6110, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6111, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6112, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1771
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6113, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6114, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3737
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6115, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6116, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6117, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6118, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3198
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6119, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6120, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4237
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6121, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6122, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6123, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6124, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6125, num samples collected 6250, FPS 55
  Algorithm: train_loss 1.3104
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6125
Update 6126, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6127, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6128, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6129, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6130, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6131, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6132, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6133, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6134, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6135, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6136, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4402
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6137, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2639
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6138, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1901
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6139, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6140, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2702
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6141, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2849
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6142, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6143, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6144, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6298
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6145, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6146, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6546
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6147, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6148, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6188
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6149, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6150, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6150
Update 6151, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1276
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6152, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2231
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6153, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0811
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6154, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6155, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6156, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1474
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6157, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6158, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.8054
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6159, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4343
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6160, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3140
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6161, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6162, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6163, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3830
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6164, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6165, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6166, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3248
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6167, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6168, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3889
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6169, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6170, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0416
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6171, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6172, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6173, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2915
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6174, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6175, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6175
Update 6176, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2611
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6177, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6178, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6179, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3746
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6180, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6181, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6182, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6183, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2718
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6184, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6185, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6186, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6187, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6188, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6189, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5236
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6190, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6191, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6192, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1624
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6193, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3909
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6194, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0886
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6195, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6196, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4321
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6197, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6198, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6199, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3389
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6200, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.8882
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6200
Update 6201, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1761
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6202, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6203, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3374
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6204, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6205, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6206, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3444
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6207, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6208, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5145
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6209, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6210, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6211, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6212, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1465
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6213, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6214, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6215, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4012
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6216, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3086
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6217, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6218, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6564
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6219, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6220, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2361
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6221, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6222, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6223, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0772
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6224, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6225, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6225
Update 6226, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4200
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6227, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2171
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6228, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6229, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2666
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6230, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2897
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6231, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6232, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6233, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6234, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6235, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6236, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6419
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6237, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6238, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6239, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6240, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6158
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6241, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0351
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6242, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6243, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6244, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1094
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6245, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2032
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6246, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6247, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6248, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3579
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6249, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6250, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6250
Update 6251, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6252, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6253, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3266
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6254, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5537
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6255, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6256, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6257, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6258, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6259, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6260, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6261, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6262, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2269
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6263, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6264, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4702
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6265, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3496
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6266, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0806
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6267, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6268, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6269, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6270, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.6991
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6271, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6272, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5673
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6273, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2970
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6274, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6275, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6275
Update 6276, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6277, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6278, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6279, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2618
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6280, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6281, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6282, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3673
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6283, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2013
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6284, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1599
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6285, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3986
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6286, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6287, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5166
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6288, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6289, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6290, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6291, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6292, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6293, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6294, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6295, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1255
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6296, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6297, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2511
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6298, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6299, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5252
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6300, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6300
Update 6301, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6302, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1683
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6303, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1749
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6304, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6305, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6306, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6307, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6308, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4640
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6309, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.6274
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6310, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6311, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0747
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6312, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6313, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6314, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6315, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2371
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6316, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6317, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6318, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6319, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4982
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6320, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5636
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6321, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6322, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6323, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6324, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6325, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.6703
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6325
Update 6326, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6327, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6328, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6329, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6330, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6331, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5994
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6332, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6333, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2931
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6334, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.7464
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6335, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6336, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3756
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6337, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6338, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1871
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6339, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0953
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6340, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6341, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3376
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6342, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6343, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4256
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6344, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6345, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6346, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1374
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6347, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6348, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0992
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6349, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6350, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6350
Update 6351, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6352, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6353, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2695
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6354, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5544
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6355, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6356, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6357, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6358, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6359, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6360, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3122
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6361, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6362, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0901
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6363, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6364, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6365, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6366, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3789
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6367, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5379
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6368, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6369, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2078
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6370, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6371, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6372, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5735
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6373, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6374, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1010
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6375, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2843
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6375
Update 6376, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6377, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3334
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6378, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6379, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6380, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0686
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6381, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6382, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0820
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6383, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4496
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6384, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6385, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6386, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6387, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6388, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6389, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6390, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1791
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6391, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3669
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6392, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6393, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6394, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6395, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6396, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5626
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6397, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5703
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6398, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6399, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6400, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6400
Update 6401, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4570
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6402, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2735
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6403, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.7936
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6404, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6405, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6406, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6407, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6408, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4167
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6409, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4435
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6410, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6411, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1344
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6412, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6413, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6414, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6415, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0662
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6416, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6417, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0659
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6418, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6419, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6420, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6421, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1559
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6422, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6423, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6424, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6425, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6425
Update 6426, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6427, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6428, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6429, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6430, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1965
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6431, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5745
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6432, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6433, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6434, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6435, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1261
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6436, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4537
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6437, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6438, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6439, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2463
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6440, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6441, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6442, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6443, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6444, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4421
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6445, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6446, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6447, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2231
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6448, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0702
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6449, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6450, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4244
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6450
Update 6451, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6452, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1697
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6453, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6454, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0855
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6455, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4771
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6456, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0784
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6457, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6458, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6459, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0853
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6460, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5635
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6461, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4505
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6462, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1935
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6463, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6464, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0356
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6465, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2948
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6466, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6467, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4075
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6468, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2726
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6469, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6470, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6471, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6472, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6473, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6474, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0995
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6475, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6475
Update 6476, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.7845
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6477, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2803
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6478, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6479, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6480, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6481, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6482, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6483, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6484, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6485, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6486, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6487, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4285
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6488, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3219
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6489, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6490, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3465
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6491, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6492, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6493, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6494, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6495, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0673
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6496, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3632
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6497, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6498, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6499, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6500, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6500
Update 6501, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5572
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6502, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6503, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1440
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6504, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6505, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4633
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6506, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6507, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1429
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6508, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2046
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6509, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6510, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6511, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6512, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1433
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6513, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6514, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6515, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6516, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6517, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3695
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6518, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6519, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6520, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6521, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2950
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6522, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6523, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6524, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6525, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5548
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6525
Update 6526, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6527, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2520
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6528, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6529, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5851
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6530, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0484
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6531, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6532, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6533, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2609
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6534, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6535, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6536, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4670
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6537, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6538, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0704
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6539, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6540, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6541, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3058
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6542, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.7806
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6543, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0812
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6544, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6545, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6546, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2472
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6547, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6548, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1314
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6549, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6550, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3593
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6550
Update 6551, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6552, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6553, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6554, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6555, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4559
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6556, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0481
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6557, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6558, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6559, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6560, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6561, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6562, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6563, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1516
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6564, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6565, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6566, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1345
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6567, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.6866
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6568, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6569, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.9759
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6570, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0899
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6571, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6572, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6573, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1541
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6574, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6575, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.6028
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6575
Update 6576, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5128
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6577, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6578, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3116
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6579, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1347
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6580, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6581, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0922
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6582, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6583, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6584, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6585, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3513
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6586, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4894
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6587, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6588, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6589, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5915
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6590, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6591, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6592, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6593, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4188
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6594, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1737
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6595, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1197
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6596, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6597, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6598, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6599, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2337
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6600, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1680
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6600
Update 6601, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6602, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2074
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6603, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6604, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2195
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6605, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5506
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6606, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6607, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6608, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3400
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6609, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6610, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4218
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6611, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6612, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2622
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6613, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0732
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6614, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3329
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6615, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4202
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6616, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6617, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2563
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6618, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6619, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6620, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6621, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1728
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6622, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6623, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6624, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6625, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6625
Update 6626, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6627, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4100
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6628, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6629, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1660
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6630, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3511
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6631, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6632, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1311
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6633, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6634, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0427
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6635, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1987
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6636, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6637, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6638, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6639, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6640, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6641, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6642, num samples collected 6250, FPS 54
  Algorithm: train_loss 1.0306
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6643, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2949
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6644, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2229
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6645, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1367
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6646, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6647, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0736
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6648, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6649, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6650, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6650
Update 6651, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6652, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6653, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6654, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3964
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6655, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6656, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1709
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6657, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6658, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6659, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2353
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6660, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6661, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6662, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4484
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6663, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6664, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6665, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6666, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.8741
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6667, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3428
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6668, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6669, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6670, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0838
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6671, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6672, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1677
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6673, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6674, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2816
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6675, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6675
Update 6676, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6677, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6678, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6679, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6680, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6681, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3404
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6682, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6683, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2393
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6684, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6685, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6686, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1434
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6687, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4598
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6688, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6689, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4255
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6690, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3465
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6691, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6692, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1595
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6693, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6694, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6695, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6696, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6697, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6698, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6699, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6700, num samples collected 6250, FPS 54
  Algorithm: train_loss 1.3842
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6700
Update 6701, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6702, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6703, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6704, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6705, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6706, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4087
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6707, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6708, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1653
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6709, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2303
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6710, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5103
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6711, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6712, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2767
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6713, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1684
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6714, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6715, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6716, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6717, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5879
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6718, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6719, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3265
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6720, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6721, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6722, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6723, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3514
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6724, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1455
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6725, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6725
Update 6726, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6727, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5073
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6728, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3367
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6729, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6730, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4260
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6731, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6732, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6733, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6734, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.8199
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6735, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6736, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6737, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6738, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4787
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6739, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2487
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6740, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6741, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6742, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6743, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6744, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6745, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0940
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6746, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6747, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6748, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6749, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6750, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6750
Update 6751, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6752, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5411
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6753, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6754, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6755, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6756, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3460
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6757, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4419
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6758, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6759, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6760, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0685
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6761, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6762, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3005
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6763, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2778
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6764, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2224
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6765, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0783
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6766, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6767, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6768, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6769, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6731
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6770, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0890
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6771, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6772, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6773, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2294
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6774, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2286
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6775, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6775
Update 6776, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6777, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6778, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6779, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6780, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6781, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6782, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6783, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6784, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2788
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6785, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4285
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6786, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6787, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0683
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6788, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4703
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6789, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6790, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.8438
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6791, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6792, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6793, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6794, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3613
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6795, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6796, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6797, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6798, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6799, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1668
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6800, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6089
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6800
Update 6801, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6802, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6803, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6804, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2912
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6805, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5436
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6806, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5543
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6807, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6808, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6809, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1871
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6810, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5199
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6811, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6812, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3067
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6813, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6814, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6815, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6816, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2245
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6817, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6818, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6819, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2735
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6820, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6821, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6822, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6823, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6824, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6825, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6825
Update 6826, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6827, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2257
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6828, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6829, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1616
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6830, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4064
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6831, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1024
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6832, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6833, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6834, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6835, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1920
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6836, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6851
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6837, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6838, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6839, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0987
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6840, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0792
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6841, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6842, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1659
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6843, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5649
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6844, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6845, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4799
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6846, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0640
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6847, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2218
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6848, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6849, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6850, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6850
Update 6851, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0833
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6852, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4590
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6853, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6854, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6855, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6856, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6857, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6858, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1793
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6859, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5300
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6860, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6861, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2053
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6862, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6863, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6864, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6865, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6866, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6867, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1250
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6868, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6869, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6870, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2455
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6871, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6872, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1838
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6873, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6874, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6546
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6875, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6875
Finished MB training, ran for 60 epochs
Update 6876, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6877, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6878, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4655
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6879, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6880, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6881, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6882, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1838
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6883, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3853
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6884, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5152
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6885, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0720
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6886, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3327
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6887, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6888, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6889, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1902
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6890, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1874
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6891, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5922
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6892, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1349
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6893, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1522
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6894, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0513
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6895, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6896, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6897, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6898, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6899, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
Update 6900, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -991.1547, l 200.0000, t 195.4640, TestReward -1225.7271
New EPOCH! 6900
Update 6901, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6902, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2937
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6903, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1191
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6904, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6905, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.7254
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6906, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6907, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3718
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6908, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3049
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6909, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.9040
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6910, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6911, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6912, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6913, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6914, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6915, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1225
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6916, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6917, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6918, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0664
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6919, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6920, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6921, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1734
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6922, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6923, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0350
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6924, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6925, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6926, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 6926
Update 6927, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3626
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6928, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1490
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6929, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6930, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6931, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6932, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.6602
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6933, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6934, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6935, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6936, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6937, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0435
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6938, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1994
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6939, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6940, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4195
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6941, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1518
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6942, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6943, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5286
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6944, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2041
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6945, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1068
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6946, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6947, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0346
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6948, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3529
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6949, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3927
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6950, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2177
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6951, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6952, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0694
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 6952
Update 6953, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6954, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6955, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6956, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.6109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6957, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6958, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6959, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6960, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1440
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6961, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6962, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6963, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5861
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6964, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6965, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.6381
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6966, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6967, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6968, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5426
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6969, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6970, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4958
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6971, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6972, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6973, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6974, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1227
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6975, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6976, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6977, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6978, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 6978
Update 6979, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5700
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6980, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6981, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6982, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6983, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6984, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6985, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6986, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6987, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2240
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6988, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6989, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6990, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6991, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4998
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6992, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6993, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.6521
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6994, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0855
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6995, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2659
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6996, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6997, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.7840
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6998, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 6999, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7000, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7001, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7002, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7003, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1428
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7004, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5721
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7004
Update 7005, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7006, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7007, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5778
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7008, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7009, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7010, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3409
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7011, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7012, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7013, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7014, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7015, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7016, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0875
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7017, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4467
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7018, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7019, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2122
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7020, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1101
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7021, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7022, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3022
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7023, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7024, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7025, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7026, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1808
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7027, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1855
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7028, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7029, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4170
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7030, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7030
Update 7031, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7032, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7033, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7034, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1442
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7035, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7036, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2641
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7037, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7038, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1725
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7039, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7040, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3566
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7041, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0960
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7042, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7043, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.9994
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7044, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2796
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7045, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7046, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7047, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7048, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3941
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7049, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7050, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7051, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3705
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7052, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1935
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7053, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7054, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7055, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7056, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7056
Update 7057, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7058, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7059, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4183
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7060, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4729
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7061, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1645
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7062, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2183
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7063, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1652
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7064, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7065, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7066, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7067, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7068, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7069, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3586
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7070, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7071, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7072, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3620
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7073, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2250
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7074, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7075, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3027
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7076, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7077, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7078, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7079, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7080, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4402
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7081, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7082, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7082
Update 7083, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7084, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7085, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7086, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7087, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7088, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7089, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7090, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1967
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7091, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7092, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4020
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7093, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7094, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7095, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0479
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7096, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4775
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7097, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2609
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7098, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7099, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3146
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7100, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7101, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7102, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7103, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7104, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.6246
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7105, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7106, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3149
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7107, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3843
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7108, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7108
Update 7109, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3293
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7110, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7111, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7112, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3644
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7113, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1356
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7114, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7115, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7116, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3734
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7117, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7118, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0377
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7119, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7120, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7121, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7122, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7123, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5997
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7124, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7125, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7126, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7127, num samples collected 6500, FPS 48
  Algorithm: train_loss 1.0516
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7128, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2778
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7129, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7130, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7131, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7132, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1079
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7133, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7134, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7134
Update 7135, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3551
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7136, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7137, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7138, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2572
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7139, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7140, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1707
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7141, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7142, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7143, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4113
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7144, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7145, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0992
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7146, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7147, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7148, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.7421
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7149, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7150, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7151, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2996
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7152, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7153, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4997
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7154, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7155, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7156, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2754
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7157, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7158, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1974
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7159, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7160, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7160
Update 7161, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0316
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7162, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7163, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7164, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7165, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4004
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7166, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0890
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7167, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7168, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0908
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7169, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7170, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7171, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5848
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7172, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7173, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2785
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7174, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0932
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7175, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7176, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7177, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7178, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7179, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7180, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7181, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.9153
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7182, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7183, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7184, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7185, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1484
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7186, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7186
Update 7187, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7188, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7189, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0844
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7190, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7191, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7192, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.4547
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7193, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7194, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7195, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7196, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1247
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7197, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.8008
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7198, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3573
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7199, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7200, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1846
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7201, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7202, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7203, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2701
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7204, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7205, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7206, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0443
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7207, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1739
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7208, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2352
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7209, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7210, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2869
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7211, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7212, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7212
Update 7213, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7214, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.9038
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7215, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7216, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7217, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7218, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2708
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7219, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7220, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.8667
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7221, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7222, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7223, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3834
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7224, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1486
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7225, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1998
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7226, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7227, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7228, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3375
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7229, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7230, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7231, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1439
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7232, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7233, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7234, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7235, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7236, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1667
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7237, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0951
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7238, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7238
Update 7239, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7240, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7241, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1070
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7242, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7243, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3488
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7244, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7245, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.5629
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7246, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7247, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7248, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7249, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2294
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7250, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2742
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7251, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7252, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0916
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7253, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3356
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7254, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7255, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2660
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7256, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.7491
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7257, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7258, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2963
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7259, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7260, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7261, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.2340
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7262, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0951
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7263, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.3053
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7264, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7264
Update 7265, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7266, num samples collected 6500, FPS 48
  Algorithm: train_loss 0.1434
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7267, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7268, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7269, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7270, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7271, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7272, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7273, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7274, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7275, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7276, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5212
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7277, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7278, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4743
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7279, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7280, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5215
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7281, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7282, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7283, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0878
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7284, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2294
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7285, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6162
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7286, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7287, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7288, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7289, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7290, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7290
Update 7291, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2780
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7292, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7293, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1737
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7294, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4366
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7295, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7296, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7297, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7298, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1867
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7299, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2816
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7300, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0837
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7301, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7302, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1678
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7303, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7304, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0708
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7305, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.9692
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7306, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7307, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0848
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7308, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7309, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7310, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2338
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7311, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7312, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7313, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1335
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7314, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7315, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7316, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7316
Update 7317, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7318, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7319, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7320, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4955
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7321, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1027
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7322, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7323, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7324, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1430
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7325, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7326, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3789
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7327, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7328, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0383
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7329, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7330, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7331, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5270
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7332, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2550
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7333, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7334, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1896
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7335, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5587
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7336, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1240
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7337, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4328
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7338, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7339, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1902
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7340, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7341, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2177
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7342, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0959
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7342
Update 7343, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7344, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3792
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7345, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7346, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7347, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3823
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7348, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7349, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7350, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7351, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0943
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7352, num samples collected 6500, FPS 47
  Algorithm: train_loss 1.0553
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7353, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7354, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7355, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1139
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7356, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0481
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7357, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2022
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7358, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7359, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2209
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7360, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4313
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7361, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1695
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7362, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7363, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7364, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7365, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7366, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7367, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7368, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0738
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7368
Update 7369, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0821
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7370, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7371, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7372, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7373, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1705
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7374, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3481
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7375, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7376, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7377, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4231
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7378, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4457
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7379, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7380, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1204
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7381, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7382, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7383, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3793
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7384, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1629
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7385, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7386, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4446
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7387, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4698
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7388, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7389, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4316
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7390, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7391, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0877
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7392, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7393, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7394, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7394
Update 7395, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7396, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2959
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7397, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2359
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7398, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7399, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7400, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2762
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7401, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1146
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7402, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3507
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7403, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1600
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7404, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3196
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7405, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4516
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7406, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7407, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7408, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7409, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1985
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7410, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7411, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7412, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1625
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7413, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3421
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7414, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7415, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7416, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5792
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7417, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7418, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7419, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7420, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7420
Update 7421, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7422, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7423, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4315
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7424, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3571
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7425, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7426, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2834
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7427, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7428, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7429, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1369
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7430, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0919
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7431, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5739
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7432, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7433, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7434, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7435, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2916
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7436, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3356
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7437, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7438, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7439, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7440, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7441, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7442, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7443, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1388
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7444, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7445, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.7998
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7446, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7446
Update 7447, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7448, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7449, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1390
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7450, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7451, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7452, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7453, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7454, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3366
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7455, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0708
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7456, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7457, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1255
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7458, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7459, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7460, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7461, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7462, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7463, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7464, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7465, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7466, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7467, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7468, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.7733
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7469, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7470, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7471, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4435
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7472, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.7846
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7472
Update 7473, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7474, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7475, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7476, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7477, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3705
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7478, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7479, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7480, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7481, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7482, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7483, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7484, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7485, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7486, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7487, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7488, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7489, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7490, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7491, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2267
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7492, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7493, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3597
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7494, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7495, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7496, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3535
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7497, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.9836
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7498, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7498
Update 7499, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1014
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7500, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7501, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7502, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7503, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2196
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7504, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7505, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2186
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7506, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7507, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7508, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7509, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7510, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7511, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7512, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7513, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2645
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7514, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3486
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7515, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7516, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1439
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7517, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5314
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7518, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7519, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1405
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7520, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.8194
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7521, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7522, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7523, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4984
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7524, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7524
Update 7525, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7526, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7527, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0919
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7528, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3877
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7529, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7530, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7531, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7532, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2580
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7533, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5934
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7534, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4604
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7535, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7536, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1838
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7537, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1600
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7538, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7539, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7540, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4389
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7541, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7542, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7543, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1613
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7544, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7545, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7546, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7547, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7548, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4686
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7549, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7550, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7550
Update 7551, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7552, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0763
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7553, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1412
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7554, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7555, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7556, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7557, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7558, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2401
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7559, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5824
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7560, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0864
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7561, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3200
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7562, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7563, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7564, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7565, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5288
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7566, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3941
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7567, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7568, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3977
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7569, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7570, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7571, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3505
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7572, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7573, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0739
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7574, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7575, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7576, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7576
Update 7577, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0731
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7578, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4288
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7579, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7580, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7581, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7582, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7583, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7584, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7585, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7586, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7587, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7588, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3770
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7589, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7590, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2683
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7591, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3694
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7592, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7593, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1752
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7594, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3985
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7595, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4209
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7596, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7597, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3561
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7598, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1650
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7599, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7600, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2919
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7601, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7602, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7602
Update 7603, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7604, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2743
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7605, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2759
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7606, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7607, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0937
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7608, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7609, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1014
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7610, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7611, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7612, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4653
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7613, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1077
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7614, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7615, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7616, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7617, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7618, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7619, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5167
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7620, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7621, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7622, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7623, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3619
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7624, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3256
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7625, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7626, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2565
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7627, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3533
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7628, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4372
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7628
Update 7629, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1530
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7630, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7631, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7632, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7633, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7634, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7635, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7636, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7637, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2781
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7638, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7639, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6028
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7640, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2222
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7641, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7642, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7643, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2695
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7644, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7645, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4553
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7646, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7647, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2221
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7648, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1289
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7649, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7650, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0745
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7651, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3305
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7652, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7653, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3539
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7654, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2854
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7654
Update 7655, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1653
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7656, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7657, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7658, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2074
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7659, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1783
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7660, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7661, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2688
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7662, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7663, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3469
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7664, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4751
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7665, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7666, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6496
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7667, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1133
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7668, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6083
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7669, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0416
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7670, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7671, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1504
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7672, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7673, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3767
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7674, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7675, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7676, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7677, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7678, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2148
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7679, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7680, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7680
Update 7681, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7682, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1937
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7683, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7684, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2061
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7685, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7686, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7687, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7688, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7689, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1496
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7690, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6250
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7691, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3497
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7692, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4935
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7693, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7694, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7695, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7696, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7697, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4112
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7698, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1911
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7699, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7700, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2189
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7701, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3240
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7702, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7703, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7704, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7705, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7706, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7706
Update 7707, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7708, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0404
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7709, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0741
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7710, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7711, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7712, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1585
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7713, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3069
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7714, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2465
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7715, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7716, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6943
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7717, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7718, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7719, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7720, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7721, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4426
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7722, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4306
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7723, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7724, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0737
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7725, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7726, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7727, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4911
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7728, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7729, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7730, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7731, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7732, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7732
Update 7733, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7734, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7735, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7736, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7737, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2823
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7738, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7739, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7740, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7741, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7742, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1940
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7743, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7744, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3222
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7745, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7746, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7747, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7748, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3989
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7749, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.9127
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7750, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7751, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7752, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7753, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7754, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7755, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5044
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7756, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7757, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0907
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7758, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7758
Update 7759, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7760, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7761, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7762, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7763, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0737
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7764, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7765, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3933
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7766, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1974
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7767, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7768, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7769, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7770, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7771, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7772, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6440
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7773, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7774, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4816
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7775, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7776, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4598
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7777, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0685
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7778, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2182
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7779, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7780, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7781, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3309
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7782, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3161
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7783, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2450
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7784, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7784
Update 7785, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3024
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7786, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7787, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3990
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7788, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1913
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7789, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7790, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5768
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7791, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7792, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7793, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4182
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7794, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7795, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1926
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7796, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7797, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4151
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7798, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7799, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7800, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7801, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0910
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7802, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7803, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7804, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7805, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7806, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7807, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4557
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7808, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7809, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7810, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7810
Update 7811, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7812, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1863
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7813, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5442
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7814, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7815, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1016
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7816, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7817, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7818, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7819, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3052
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7820, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1323
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7821, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7822, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5022
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7823, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1212
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7824, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7825, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5496
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7826, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2757
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7827, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7828, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7829, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7830, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7831, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.6066
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7832, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7833, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7834, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7835, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7836, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1063
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7836
Update 7837, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7838, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7839, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0720
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7840, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7841, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7842, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7843, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7844, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7845, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3638
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7846, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7847, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7848, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7849, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7850, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7851, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7852, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7853, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7854, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5195
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7855, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0788
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7856, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7857, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7858, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3595
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7859, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2427
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7860, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.8641
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7861, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.7564
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7862, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7862
Update 7863, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7864, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4002
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7865, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7866, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7867, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7868, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7869, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7870, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7871, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0792
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7872, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7873, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2473
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7874, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7875, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7876, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7877, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7878, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.4388
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7879, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7880, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2672
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7881, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.3830
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7882, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7883, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0960
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7884, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.1409
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7885, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.5850
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7886, num samples collected 6500, FPS 47
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7887, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0867
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7888, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7888
Update 7889, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7890, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0919
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7891, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7892, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7893, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7894, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7147
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7895, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7896, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7897, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7898, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2227
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7899, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7900, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0793
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7901, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2559
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7902, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7903, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3628
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7904, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7905, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7906, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4129
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7907, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3595
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7908, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7909, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7910, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1919
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7911, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7912, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0790
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7913, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7914, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3371
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7914
Update 7915, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7872
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7916, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5275
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7917, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7918, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7919, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2365
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7920, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0748
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7921, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7922, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7923, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7924, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7925, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7926, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4340
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7927, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7928, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7929, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1196
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7930, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7931, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7932, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7933, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0797
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7934, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7935, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2987
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7936, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7937, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7097
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7938, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7939, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7940, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1428
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7940
Update 7941, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7942, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7943, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7944, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7945, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7946, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7947, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2691
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7948, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3456
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7949, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7950, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7951, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7952, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2170
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7953, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0745
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7954, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7955, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0750
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7956, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7957, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7958, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4422
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7959, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7960, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4563
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7961, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7962, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0941
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7963, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1775
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7964, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7965, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7966, num samples collected 6500, FPS 46
  Algorithm: train_loss 1.5707
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7966
Update 7967, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7968, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7969, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7970, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4189
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7971, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2851
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7972, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7973, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7974, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7975, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7976, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7977, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7978, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2035
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7979, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7980, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1408
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7981, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7982, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7983, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7984, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1331
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7985, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7986, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6024
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7987, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7988, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7989, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7990, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7991, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7992, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 7992
Update 7993, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7994, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7995, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7996, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7997, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7998, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 7999, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3058
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8000, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4201
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8001, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3476
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8002, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8003, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3464
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8004, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8005, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5690
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8006, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8007, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8008, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8009, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8010, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8011, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2387
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8012, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8013, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8014, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3587
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8015, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8016, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1536
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8017, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2230
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8018, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8018
Update 8019, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8020, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8021, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0856
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8022, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0785
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8023, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8024, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8025, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7807
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8026, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8027, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2046
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8028, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2270
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8029, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8030, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8031, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5617
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8032, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8033, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8034, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8035, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8036, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8037, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8038, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4808
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8039, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2664
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8040, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8041, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1433
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8042, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8043, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8044, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8044
Update 8045, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8046, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8047, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4437
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8048, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8049, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8050, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8051, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5412
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8052, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8053, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3990
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8054, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8055, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8056, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8057, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4559
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8058, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8059, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8060, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8061, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8062, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0686
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8063, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8064, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1288
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8065, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8066, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4593
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8067, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8068, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8069, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4068
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8070, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4983
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8070
Update 8071, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8072, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2850
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8073, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8074, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0940
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8075, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3450
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8076, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3482
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8077, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1226
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8078, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5038
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8079, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8080, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5156
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8081, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8082, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8083, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3298
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8084, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8085, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8086, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0349
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8087, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8088, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1459
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8089, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4453
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8090, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8091, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2074
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8092, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0817
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8093, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8094, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8095, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1907
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8096, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8096
Update 8097, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2030
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8098, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8099, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8100, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8101, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8102, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8103, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0869
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8104, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7278
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8105, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8106, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1461
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8107, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.9222
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8108, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8109, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8110, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4268
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8111, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8112, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8113, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8114, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8115, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8116, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8117, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3465
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8118, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8119, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8120, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8121, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8122, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8122
Update 8123, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2936
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8124, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1637
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8125, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8126, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0739
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8127, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8128, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8129, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8130, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8131, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8132, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8133, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8134, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8135, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8136, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1762
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8137, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8138, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8139, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8140, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3664
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8141, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4955
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8142, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0826
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8143, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0781
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8144, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3464
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8145, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8146, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8147, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6820
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8148, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8148
Update 8149, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8150, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2035
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8151, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8152, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8153, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8154, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8155, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1219
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8156, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8157, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8158, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8159, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8160, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6636
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8161, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2692
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8162, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2499
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8163, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1496
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8164, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0897
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8165, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3248
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8166, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8167, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4890
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8168, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8169, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7353
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8170, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0346
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8171, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8172, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8173, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8174, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8174
Update 8175, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5737
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8176, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4165
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8177, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8178, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8179, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1758
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8180, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2389
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8181, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3549
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8182, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0844
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8183, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2637
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8184, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8185, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8186, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8187, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8188, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8189, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8190, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2759
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8191, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1463
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8192, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8193, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0362
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8194, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8195, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8196, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8197, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8198, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2089
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8199, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8200, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.9865
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8200
Update 8201, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8202, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1434
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8203, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8204, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1788
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8205, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2457
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8206, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8207, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6458
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8208, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8209, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8210, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8211, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2605
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8212, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1952
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8213, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8214, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8215, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8216, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4604
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8217, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8218, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2233
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8219, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1610
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8220, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1101
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8221, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6410
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8222, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8223, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8224, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8225, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1392
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8226, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8226
Update 8227, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8228, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8229, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8230, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5113
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8231, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1931
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8232, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8233, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2400
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8234, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8235, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8236, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8237, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8238, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8239, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8240, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0744
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8241, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6194
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8242, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0991
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8243, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8244, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8245, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4179
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8246, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8247, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8248, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5414
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8249, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4516
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8250, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8251, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2665
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8252, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8252
Update 8253, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0923
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8254, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8255, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2648
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8256, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2624
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8257, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8258, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8259, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4502
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8260, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1579
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8261, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8262, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8263, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6930
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8264, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8265, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8266, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2141
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8267, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5326
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8268, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8269, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8270, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2155
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8271, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8272, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1777
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8273, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8274, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8275, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8276, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1318
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8277, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8278, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0745
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8278
Update 8279, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8280, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8281, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1639
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8282, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8283, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8284, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8285, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8286, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8287, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2989
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8288, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0866
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8289, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8290, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8291, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1572
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8292, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1801
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8293, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6827
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8294, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8295, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1362
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8296, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2373
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8297, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1238
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8298, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2960
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8299, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8300, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8301, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4612
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8302, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8303, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8304, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8304
Update 8305, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8306, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1241
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8307, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8308, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2210
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8309, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8310, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8311, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8312, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8313, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1689
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8314, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8315, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8316, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8317, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8318, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8319, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5122
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8320, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3843
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8321, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4056
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8322, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8323, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8324, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1171
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8325, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8326, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4483
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8327, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8328, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4858
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8329, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8330, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8330
Update 8331, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8332, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8333, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1276
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8334, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8335, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8336, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8337, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8338, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2706
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8339, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8340, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0769
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8341, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8342, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1883
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8343, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0870
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8344, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4543
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8345, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2192
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8346, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1196
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8347, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1201
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8348, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8349, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2222
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8350, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8351, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1672
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8352, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8353, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4264
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8354, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8355, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4798
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8356, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0879
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8356
Update 8357, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8358, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8359, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3404
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8360, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8361, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8362, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2196
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8363, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8364, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8365, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8366, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2447
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8367, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8368, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8369, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8370, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2766
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8371, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8372, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8373, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3938
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8374, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3303
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8375, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8376, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8377, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8378, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1460
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8379, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6924
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8380, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8381, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8382, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8382
Update 8383, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8384, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8385, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8386, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1098
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8387, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8388, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2615
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8389, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2596
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8390, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3259
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8391, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8392, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8393, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2435
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8394, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8395, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3479
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8396, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1995
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8397, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8398, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8399, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6586
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8400, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8401, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3498
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8402, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8403, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0840
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8404, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8405, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5030
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8406, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0374
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8407, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8408, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8408
Update 8409, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3804
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8410, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1760
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8411, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3161
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8412, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3974
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8413, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8414, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8415, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3390
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8416, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8417, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8418, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8419, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8420, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1349
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8421, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8422, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8423, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8424, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6162
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8425, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0356
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8426, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8427, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8428, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2496
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8429, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0908
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8430, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2671
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8431, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0382
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8432, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8433, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8434, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8434
Finished MB training, ran for 60 epochs
Update 8435, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8436, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3961
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8437, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8438, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8439, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8440, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0316
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8441, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2043
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8442, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5045
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8443, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1890
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8444, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8445, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8446, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8447, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0846
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8448, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8449, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1568
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8450, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5936
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8451, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8452, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8453, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8454, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3885
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8455, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8456, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4402
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8457, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0496
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8458, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8459, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
Update 8460, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3402
  Episodes: TrainReward -823.3773, l 200.0000, t 218.8600, TestReward -1421.0738
New EPOCH! 8460
Update 8461, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8462, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8463, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8464, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8465, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4402
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8466, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8467, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8468, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3475
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8469, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8470, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2845
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8471, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8472, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8473, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8474, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1878
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8475, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2578
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8476, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8477, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8478, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2805
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8479, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3186
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8480, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8481, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2026
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8482, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8483, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8484, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8485, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0977
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8486, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1961
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8487, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8487
Update 8488, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0785
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8489, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8490, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0691
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8491, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8492, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0827
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8493, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0640
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8494, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8495, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3017
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8496, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8497, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4505
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8498, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3469
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8499, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8500, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8501, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8502, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8503, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0795
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8504, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8505, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2173
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8506, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8507, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8508, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8509, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0972
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8510, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8511, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8512, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4020
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8513, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8514, num samples collected 6750, FPS 42
  Algorithm: train_loss 1.1734
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8514
Update 8515, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4460
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8516, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8517, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3394
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8518, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8519, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8520, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3748
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8521, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8522, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8523, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8524, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1821
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8525, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8526, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8527, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2398
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8528, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8529, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5867
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8530, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8531, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8532, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0415
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8533, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8534, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1982
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8535, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1212
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8536, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2158
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8537, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8538, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8539, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1836
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8540, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8541, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8541
Update 8542, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4731
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8543, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1599
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8544, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8545, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8546, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8547, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5688
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8548, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8549, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1237
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8550, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0796
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8551, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8552, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8553, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8554, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8555, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8556, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8557, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4215
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8558, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8559, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4791
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8560, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1660
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8561, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8562, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8563, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8564, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8565, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0729
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8566, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4484
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8567, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8568, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8568
Update 8569, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2084
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8570, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8571, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8572, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4906
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8573, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4462
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8574, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8575, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2645
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8576, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8577, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1995
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8578, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8579, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3511
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8580, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0700
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8581, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8582, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8583, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8584, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8585, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8586, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8587, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3627
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8588, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8589, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8590, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8591, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2473
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8592, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1800
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8593, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8594, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8595, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8595
Update 8596, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8597, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0733
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8598, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8599, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8600, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8601, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4215
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8602, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8603, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8604, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8605, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8606, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8607, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2598
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8608, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8609, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1522
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8610, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8611, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8612, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3539
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8613, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8614, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8615, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8616, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5316
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8617, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8618, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0959
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8619, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8620, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4725
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8621, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4307
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8622, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2219
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8622
Update 8623, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8624, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0937
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8625, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1282
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8626, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8627, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8628, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8629, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8630, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8631, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8632, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1846
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8633, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8634, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4356
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8635, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8636, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1929
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8637, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8638, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3958
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8639, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2584
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8640, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8641, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1261
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8642, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1836
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8643, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5309
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8644, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8645, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4974
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8646, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8647, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8648, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8649, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8649
Update 8650, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8651, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8652, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8653, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5808
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8654, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8655, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3480
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8656, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4263
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8657, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8658, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8659, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8660, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8661, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2792
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8662, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8663, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2594
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8664, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8665, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8666, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3655
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8667, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1050
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8668, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8669, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8670, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8671, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3493
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8672, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8673, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8674, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8675, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8676, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8676
Update 8677, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3378
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8678, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8679, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1677
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8680, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8681, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8682, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3938
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8683, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1486
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8684, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8685, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8686, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1664
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8687, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0378
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8688, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1288
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8689, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8690, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8691, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8692, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.6053
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8693, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3267
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8694, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3164
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8695, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8696, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1960
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8697, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8698, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0901
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8699, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2380
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8700, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8701, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8702, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8703, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.7979
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8703
Update 8704, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8705, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8706, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8707, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8708, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5453
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8709, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8710, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8711, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8712, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8713, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0827
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8714, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1557
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8715, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8716, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.7559
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8717, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8718, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5275
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8719, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1991
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8720, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8721, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8722, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4928
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8723, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2065
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8724, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8725, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0484
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8726, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8727, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8728, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8729, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0971
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8730, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8730
Update 8731, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8732, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8733, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1453
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8734, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3768
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8735, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3023
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8736, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8737, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8738, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1876
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8739, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5285
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8740, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8741, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8742, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8743, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8744, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0805
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8745, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4110
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8746, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8747, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8748, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8749, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8750, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8751, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8752, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3294
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8753, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4435
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8754, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8755, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8756, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1901
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8757, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8757
Update 8758, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0819
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8759, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8760, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8761, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5701
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8762, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1819
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8763, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8764, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8765, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8766, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8767, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8768, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1735
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8769, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8770, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8771, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8772, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8773, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8774, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8775, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8776, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2300
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8777, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8778, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2416
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8779, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4229
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8780, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3814
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8781, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8782, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3817
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8783, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8784, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8784
Update 8785, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8786, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1495
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8787, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8788, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1392
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8789, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8790, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2975
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8791, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8792, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8793, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8794, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4855
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8795, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0689
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8796, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8797, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4657
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8798, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8799, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4031
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8800, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4791
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8801, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1940
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8802, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8803, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8804, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8805, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3853
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8806, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8807, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2565
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8808, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1181
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8809, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8810, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8811, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8811
Update 8812, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8813, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8814, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8815, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8816, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8817, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8818, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1883
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8819, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1696
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8820, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3273
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8821, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0726
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8822, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2099
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8823, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1731
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8824, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8825, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8826, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8827, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8828, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8829, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8830, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8831, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8832, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8833, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8834, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8835, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4539
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8836, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4060
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8837, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1645
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8838, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8838
Update 8839, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0871
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8840, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8841, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8842, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8843, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8844, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8845, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3216
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8846, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4188
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8847, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4589
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8848, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8849, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8850, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8851, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8852, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8853, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8854, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8855, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2582
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8856, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1795
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8857, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1697
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8858, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8859, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8860, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8861, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8862, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8863, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8864, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2218
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8865, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8865
Update 8866, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8867, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4357
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8868, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3396
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8869, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2294
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8870, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3396
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8871, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8872, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8873, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1691
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8874, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8875, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3703
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8876, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1279
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8877, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1857
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8878, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4529
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8879, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8880, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8881, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8882, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8883, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8884, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0803
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8885, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8886, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8887, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8888, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8889, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8890, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8891, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8892, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8892
Update 8893, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4030
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8894, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8895, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8896, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8897, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8898, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8899, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8900, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8901, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0946
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8902, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1825
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8903, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8904, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0443
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8905, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8906, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8907, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0644
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8908, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3441
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8909, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.7637
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8910, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2487
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8911, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1719
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8912, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8913, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0977
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8914, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8915, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8916, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8917, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8918, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3960
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8919, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8919
Update 8920, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8921, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1625
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8922, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3904
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8923, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8924, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8925, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.6313
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8926, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8927, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8928, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8929, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1611
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8930, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1529
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8931, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8932, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8933, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0937
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8934, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8935, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2492
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8936, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8937, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8938, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8939, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8940, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8941, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8942, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4291
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8943, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3609
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8944, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8945, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8946, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5770
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8946
Update 8947, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1721
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8948, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2121
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8949, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8950, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4487
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8951, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8952, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8953, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8954, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8955, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1599
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8956, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2064
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8957, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4634
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8958, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3366
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8959, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8960, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8961, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8962, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8963, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4503
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8964, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0928
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8965, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8966, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1850
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8967, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8968, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1095
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8969, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3990
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8970, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8971, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8972, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8973, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.9951
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 8973
Update 8974, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8975, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8976, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8977, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8978, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8979, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1816
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8980, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8981, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4144
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8982, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8983, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.6964
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8984, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8985, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8986, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1858
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8987, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1951
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8988, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0912
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8989, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8990, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1739
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8991, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8992, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1219
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8993, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8994, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4099
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8995, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3116
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8996, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8997, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8998, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 8999, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9000, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9000
Update 9001, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9002, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9003, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9004, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1224
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9005, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9006, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9007, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9008, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5578
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9009, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2527
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9010, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9011, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1383
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9012, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9013, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9014, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9015, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3891
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9016, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9017, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9018, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.5666
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9019, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9020, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9021, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1709
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9022, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9023, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.3088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9024, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9025, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9026, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9027, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.9190
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9027
Update 9028, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.6224
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9029, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9030, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2013
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9031, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9032, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.4236
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9033, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9034, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9035, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9036, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9037, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9038, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9039, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0954
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9040, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9041, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9042, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.6289
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9043, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9044, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9045, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1373
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9046, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2330
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9047, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0806
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9048, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9049, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9050, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0838
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9051, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9052, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0753
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9053, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9054, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9054
Update 9055, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0460
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9056, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9057, num samples collected 6750, FPS 42
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9058, num samples collected 6750, FPS 41
  Algorithm: train_loss 1.0559
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9059, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1417
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9060, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9061, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9062, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1759
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9063, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9064, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2694
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9065, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1927
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9066, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9067, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0869
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9068, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9069, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9070, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9071, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9072, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1647
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9073, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0541
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9074, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3824
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9075, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9076, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3412
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9077, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1912
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9078, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9079, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9080, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9081, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9081
Update 9082, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9083, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9084, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9085, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9086, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9087, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9088, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0427
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9089, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9090, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9091, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9092, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9093, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4106
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9094, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0350
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9095, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.7019
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9096, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9097, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5989
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9098, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9099, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9100, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9101, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9102, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9103, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9104, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0322
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9105, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9106, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9107, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4753
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9108, num samples collected 6750, FPS 41
  Algorithm: train_loss 1.6725
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9108
Update 9109, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9110, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4914
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9111, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9112, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9113, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9114, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9115, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9116, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9117, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9118, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9119, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1147
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9120, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9121, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2521
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9122, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4286
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9123, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1459
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9124, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3475
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9125, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9126, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9127, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9128, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0820
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9129, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9130, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4050
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9131, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9132, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9133, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9134, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9135, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9135
Update 9136, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9137, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9138, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4310
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9139, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9140, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1678
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9141, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9142, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9143, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0865
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9144, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9145, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1668
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9146, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9147, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1709
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9148, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4536
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9149, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9150, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.7835
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9151, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9152, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9153, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9154, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1710
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9155, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0430
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9156, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0813
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9157, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9158, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9159, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2090
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9160, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9161, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9162, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.9157
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9162
Update 9163, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9164, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3953
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9165, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1484
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9166, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9167, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9168, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9169, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9170, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9171, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9172, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9173, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9174, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3205
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9175, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5151
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9176, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9177, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4029
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9178, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9179, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.7201
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9180, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9181, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9182, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9183, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9184, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9185, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0760
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9186, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9187, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9188, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1440
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9189, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2205
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9189
Update 9190, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1656
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9191, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3972
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9192, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2219
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9193, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4630
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9194, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2620
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9195, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9196, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1472
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9197, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9198, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9199, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9200, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0734
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9201, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9202, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9203, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0860
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9204, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9205, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9206, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3466
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9207, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9208, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9209, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0520
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9210, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9211, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9212, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9213, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9214, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9215, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1971
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9216, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9216
Update 9217, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1881
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9218, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9219, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9220, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9221, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6825
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9222, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9223, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2404
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9224, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1777
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9225, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0408
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9226, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2997
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9227, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9228, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9229, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3107
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9230, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9231, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9232, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9233, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9234, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5140
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9235, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1821
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9236, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9237, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9238, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9239, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9240, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9241, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9242, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0320
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9243, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9243
Update 9244, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9245, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9246, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9247, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9248, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9249, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9250, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1554
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9251, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9252, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9253, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9254, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9255, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2287
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9256, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.7257
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9257, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9258, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9259, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9260, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5508
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9261, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1770
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9262, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9263, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9264, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9265, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2837
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9266, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9267, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3709
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9268, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9269, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9270, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9270
Update 9271, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9272, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9273, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9274, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4638
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9275, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9276, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0812
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9277, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9278, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6543
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9279, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9280, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9281, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9282, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1799
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9283, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4113
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9284, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9285, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9286, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1420
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9287, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4109
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9288, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9289, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9290, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1247
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9291, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9292, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9293, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9294, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1775
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9295, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9296, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9297, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9297
Update 9298, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9299, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2955
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9300, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9301, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9302, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1725
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9303, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5567
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9304, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9305, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9306, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9307, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9308, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9309, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3998
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9310, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9311, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9312, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9313, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9314, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9315, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9316, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1423
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9317, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9318, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5314
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9319, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4258
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9320, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3580
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9321, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1566
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9322, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9323, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9324, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9324
Update 9325, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9326, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9327, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5316
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9328, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9329, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2223
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9330, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3622
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9331, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9332, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3107
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9333, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9334, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9335, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9336, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9337, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9338, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9339, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9340, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4611
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9341, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0388
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9342, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2889
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9343, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9344, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9345, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1693
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9346, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9347, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9348, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0690
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9349, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3206
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9350, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2028
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9351, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3585
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9351
Update 9352, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5707
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9353, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9354, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9355, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9356, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9357, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9358, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9359, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0887
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9360, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9361, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9362, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9363, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9364, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9365, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9366, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9367, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1243
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9368, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3185
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9369, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9370, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9371, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2348
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9372, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9373, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9374, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1963
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9375, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4103
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9376, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9377, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6063
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9378, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9378
Update 9379, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0870
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9380, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9381, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2794
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9382, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9383, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9384, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3281
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9385, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9386, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1259
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9387, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9388, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5888
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9389, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0754
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9390, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9391, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5353
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9392, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9393, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4106
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9394, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9395, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9396, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0804
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9397, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9398, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9399, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2260
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9400, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9401, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9402, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9403, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9404, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9405, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9405
Update 9406, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9407, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9408, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9409, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3621
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9410, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9411, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1773
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9412, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0346
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9413, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9414, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3846
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9415, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3218
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9416, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9417, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3667
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9418, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9419, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9420, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1634
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9421, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9422, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5261
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9423, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4141
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9424, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9425, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4597
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9426, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9427, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9428, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9429, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9430, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9431, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9432, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9432
Update 9433, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3263
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9434, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9435, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9436, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9437, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0836
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9438, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1293
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9439, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0914
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9440, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1028
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9441, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9442, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4042
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9443, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9444, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9445, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3482
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9446, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2782
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9447, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9448, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9449, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3686
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9450, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9451, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4323
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9452, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3820
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9453, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9454, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9455, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9456, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9457, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3894
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9458, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9459, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9459
Update 9460, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9461, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9462, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9463, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9464, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9465, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1554
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9466, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9467, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9468, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3037
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9469, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4464
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9470, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9471, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9472, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9473, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3744
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9474, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9475, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3908
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9476, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9477, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1358
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9478, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5505
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9479, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9480, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9481, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9482, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9483, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5256
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9484, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9485, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0856
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9486, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9486
Update 9487, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4379
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9488, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9489, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9490, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1445
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9491, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1506
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9492, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1698
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9493, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9494, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9495, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9496, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9497, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3397
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9498, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9499, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2749
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9500, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4893
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9501, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9502, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9503, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9504, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4083
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9505, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9506, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9507, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9508, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1020
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9509, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9510, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1495
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9511, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9512, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9513, num samples collected 6750, FPS 41
  Algorithm: train_loss 1.0410
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9513
Update 9514, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9515, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9516, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4068
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9517, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0831
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9518, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9519, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9520, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9521, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9522, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1485
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9523, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9524, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1246
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9525, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0907
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9526, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3993
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9527, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9528, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1474
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9529, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9530, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1467
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9531, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9532, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9533, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9534, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.7353
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9535, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2604
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9536, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9537, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9538, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9539, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9540, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9540
Update 9541, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9542, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9543, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4345
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9544, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4805
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9545, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9546, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9547, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2715
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9548, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9549, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9550, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9551, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9552, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9553, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9554, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.7040
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9555, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9556, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9557, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9558, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5426
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9559, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9560, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9561, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9562, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9563, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9564, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1884
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9565, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9566, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9567, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9567
Update 9568, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9569, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0790
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9570, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9571, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0770
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9572, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9573, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9574, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9575, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4496
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9576, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4306
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9577, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9578, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9579, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9580, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9581, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9582, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9583, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0996
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9584, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4532
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9585, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9586, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1491
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9587, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9588, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9589, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2483
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9590, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4905
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9591, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0972
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9592, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9593, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3727
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9594, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4565
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9594
Update 9595, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9596, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4140
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9597, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9598, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9599, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1451
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9600, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2269
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9601, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0752
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9602, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1653
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9603, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3677
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9604, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1457
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9605, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9606, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3358
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9607, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9608, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9609, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1158
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9610, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9611, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9612, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3121
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9613, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9614, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9615, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9616, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3887
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9617, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3944
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9618, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1293
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9619, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9620, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2448
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9621, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9621
Update 9622, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2843
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9623, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1663
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9624, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9625, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9626, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9627, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9628, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3444
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9629, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9630, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4805
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9631, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9632, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3827
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9633, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0713
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9634, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5666
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9635, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9636, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4035
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9637, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9638, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9639, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9640, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0903
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9641, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9642, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9643, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9644, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9645, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9646, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9647, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9648, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9648
Update 9649, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9650, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9651, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9652, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9653, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9654, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4485
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9655, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9656, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9657, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9658, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9659, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9660, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1705
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9661, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9662, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3447
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9663, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9664, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9665, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9666, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6652
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9667, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1423
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9668, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9669, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1673
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9670, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9671, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3871
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9672, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2718
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9673, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9674, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9675, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9675
Update 9676, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9677, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3496
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9678, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1846
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9679, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4100
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9680, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2026
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9681, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2254
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9682, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9683, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4985
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9684, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3008
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9685, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9686, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9687, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9688, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9689, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9690, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9691, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9692, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9693, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9694, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9695, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1029
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9696, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9697, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4328
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9698, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9699, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9700, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1835
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9701, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1503
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9702, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9702
Update 9703, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0891
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9704, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9705, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9706, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5685
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9707, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9708, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9709, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9710, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9711, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2453
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9712, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9713, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9714, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3462
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9715, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0244
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9716, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9717, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1314
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9718, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3147
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9719, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0716
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9720, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9721, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4321
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9722, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9723, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3233
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9724, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3824
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9725, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9726, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9727, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9728, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1901
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9729, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9729
Update 9730, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4083
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9731, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9732, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3557
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9733, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9734, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9735, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9736, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1577
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9737, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2617
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9738, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9739, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9740, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9741, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9742, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6224
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9743, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0762
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9744, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9745, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9746, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9747, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9748, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9749, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9750, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3767
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9751, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3345
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9752, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9753, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9754, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9755, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9756, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1299
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9756
Update 9757, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3759
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9758, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9759, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3844
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9760, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9761, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9762, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1288
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9763, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9764, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0931
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9765, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9766, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9767, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1157
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9768, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4363
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9769, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3335
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9770, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9771, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0416
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9772, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9773, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1542
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9774, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0979
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9775, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0899
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9776, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3039
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9777, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9778, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9779, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4259
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9780, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9781, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9782, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9783, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9783
Update 9784, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9785, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1649
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9786, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9787, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4168
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9788, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1989
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9789, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9790, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3377
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9791, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9792, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0916
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9793, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9794, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2171
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9795, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9796, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9797, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9798, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9799, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9800, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9801, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9802, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9803, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9804, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3890
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9805, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0380
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9806, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9807, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9808, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4217
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9809, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9810, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3678
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9810
Update 9811, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9812, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9813, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0408
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9814, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2840
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9815, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9816, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9817, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3745
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9818, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3172
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9819, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4425
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9820, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9821, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9822, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9823, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9824, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9825, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9826, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9827, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9828, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9829, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9830, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9831, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9832, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3430
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9833, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9834, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9835, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3413
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9836, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5611
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9837, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9837
Update 9838, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9839, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4252
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9840, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9841, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9842, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9843, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9844, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9845, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4873
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9846, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9847, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2653
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9848, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3616
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9849, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9850, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9851, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0763
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9852, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3449
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9853, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9854, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1597
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9855, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9856, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3015
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9857, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9858, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9859, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9860, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2002
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9861, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9862, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9863, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2227
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9864, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9864
Update 9865, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9866, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1649
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9867, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9868, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0664
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9869, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9870, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9871, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9872, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9873, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9874, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9875, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9876, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9877, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3426
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9878, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9879, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9880, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9881, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9882, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3163
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9883, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4249
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9884, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0867
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9885, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3226
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9886, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9887, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5363
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9888, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9889, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3095
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9890, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0419
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9891, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0377
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9891
Update 9892, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9893, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9894, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9895, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4234
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9896, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9897, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9898, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9899, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9900, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9901, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9902, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4310
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9903, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9904, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1890
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9905, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.7741
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9906, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9907, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1702
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9908, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0702
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9909, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3695
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9910, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9911, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9912, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4400
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9913, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9914, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2384
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9915, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9916, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9917, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9918, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9918
Update 9919, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3238
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9920, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9921, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9922, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6519
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9923, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9924, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0694
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9925, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9926, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5453
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9927, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9928, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0860
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9929, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0322
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9930, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9931, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0673
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9932, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9933, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9934, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9935, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3567
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9936, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9937, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2916
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9938, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9939, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9940, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9941, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9942, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4431
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9943, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3052
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9944, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9945, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9945
Update 9946, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9947, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5830
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9948, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1435
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9949, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9950, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2456
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9951, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0902
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9952, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9953, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9954, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3809
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9955, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9956, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9957, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6454
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9958, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1867
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9959, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9960, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9961, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3642
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9962, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2126
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9963, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9964, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9965, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9966, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9967, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9968, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9969, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9970, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0513
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9971, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9972, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9972
Update 9973, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9974, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9975, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3823
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9976, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9977, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9978, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9979, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3883
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9980, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9981, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9982, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9983, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9984, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9985, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3982
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9986, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1492
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9987, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9988, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9989, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9990, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3799
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9991, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9992, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9993, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9994, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9995, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2712
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9996, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9997, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9998, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 9999, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 9999
Update 10000, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10001, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0900
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10002, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1467
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10003, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10004, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10005, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1209
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10006, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3812
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10007, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10008, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0426
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10009, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10010, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4881
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10011, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3933
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10012, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10013, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10014, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10015, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3862
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10016, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10017, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10018, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10019, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10020, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0923
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10021, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0731
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10022, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10023, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10024, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10025, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4941
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10026, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4767
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 10026
Update 10027, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10028, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10029, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10030, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10031, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10032, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4152
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10033, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1983
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10034, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10035, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3381
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10036, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10037, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10038, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10039, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10040, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1542
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10041, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10042, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10043, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10044, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10045, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10046, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10047, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10048, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4051
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10049, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4451
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10050, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0758
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10051, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10052, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2238
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10053, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 10053
Finished MB training, ran for 60 epochs
Update 10054, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10055, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2164
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10056, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10057, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10058, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1827
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10059, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5046
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10060, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10061, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10062, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10063, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10064, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10065, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3717
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10066, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10067, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10068, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10069, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0881
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10070, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10071, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10072, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10073, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0859
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10074, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1396
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10075, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2575
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10076, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10077, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10078, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10079, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1944
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
Update 10080, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -870.3158, l 200.0000, t 242.0721, TestReward -1178.2418
New EPOCH! 10080
Update 10081, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10082, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2155
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10083, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3894
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10084, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10085, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10086, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5037
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10087, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10088, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1811
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10089, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.6493
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10090, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3735
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10091, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10092, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10093, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10094, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10095, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10096, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10097, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4492
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10098, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10099, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10100, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10101, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10102, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10103, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10104, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10105, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1865
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10106, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10107, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3105
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10108, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10108
Update 10109, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10110, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10111, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10112, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10113, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10114, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3256
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10115, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10116, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10117, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10118, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10119, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10120, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4726
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10121, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10122, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10123, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0697
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10124, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10125, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4862
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10126, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3044
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10127, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2569
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10128, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1861
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10129, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1540
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10130, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1631
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10131, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10132, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0978
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10133, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5896
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10134, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1237
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10135, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10136, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10136
Update 10137, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10138, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0421
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10139, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10140, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10141, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10142, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2457
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10143, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10144, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10145, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10146, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10147, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10148, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.6834
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10149, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10150, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0716
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10151, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10152, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5306
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10153, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10154, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0914
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10155, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10156, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10157, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10158, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10159, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10160, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10161, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1166
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10162, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10163, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.8142
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10164, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10164
Update 10165, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10166, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3982
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10167, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10168, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10169, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10170, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10171, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3613
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10172, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10173, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10174, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10175, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3906
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10176, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10177, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10178, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10179, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.6530
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10180, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1613
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10181, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10182, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10183, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10184, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10185, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10186, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0755
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10187, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0803
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10188, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10189, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3874
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10190, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10191, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5776
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10192, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10192
Update 10193, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0765
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10194, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10195, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10196, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4112
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10197, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10198, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10199, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10200, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0261
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10201, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10202, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10203, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3066
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10204, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10205, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10206, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10207, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10208, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.6363
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10209, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1804
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10210, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10211, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3330
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10212, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10213, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5878
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10214, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10215, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3528
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10216, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1798
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10217, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2427
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10218, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10219, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10220, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10220
Update 10221, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1433
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10222, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10223, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10224, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10225, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10226, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10227, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.7521
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10228, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10229, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10230, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4394
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10231, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10232, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10233, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3346
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10234, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10235, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10236, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10237, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10238, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10239, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10240, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10241, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10242, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3281
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10243, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10244, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10245, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10246, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10247, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3767
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10248, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10248
Update 10249, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10250, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10251, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10252, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10253, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10254, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10255, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10256, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3903
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10257, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10258, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10259, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0932
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10260, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3685
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10261, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0379
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10262, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3704
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10263, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2286
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10264, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10265, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10266, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4165
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10267, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10268, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.6672
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10269, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10270, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10271, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0874
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10272, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1903
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10273, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0216
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10274, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10275, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3340
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10276, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10276
Update 10277, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.6189
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10278, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5094
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10279, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.3302
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10280, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10281, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10282, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2320
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10283, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10284, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10285, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10286, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10287, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10288, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0689
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10289, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10290, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0488
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10291, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10292, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10293, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10294, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10295, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10296, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10297, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.5247
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10298, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10299, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.2270
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10300, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.1392
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10301, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.4363
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10302, num samples collected 7000, FPS 38
  Algorithm: train_loss 0.0893
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10303, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10304, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10304
Update 10305, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0750
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10306, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10307, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10308, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10309, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2542
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10310, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10311, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10312, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10313, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0541
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10314, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.6570
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10315, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4174
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10316, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10317, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10318, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2745
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10319, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3513
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10320, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3720
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10321, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10322, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10323, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10324, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4241
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10325, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10326, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2710
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10327, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1616
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10328, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10329, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1361
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10330, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10331, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10332, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3943
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10332
Update 10333, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10334, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10335, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10336, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10337, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10338, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3926
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10339, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2247
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10340, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.6820
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10341, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10342, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1007
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10343, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10344, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0779
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10345, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10346, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10347, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10348, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1970
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10349, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1419
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10350, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10351, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.6914
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10352, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10353, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10354, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0451
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10355, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2730
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10356, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1374
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10357, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2031
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10358, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10359, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1909
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10360, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0706
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10360
Update 10361, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3944
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10362, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10363, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10364, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10365, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10366, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1453
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10367, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10368, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10369, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10370, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10371, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1548
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10372, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10373, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10374, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1060
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10375, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10376, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10377, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4530
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10378, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4004
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10379, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10380, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2489
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10381, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10382, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2899
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10383, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10384, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10385, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10386, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3105
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10387, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5297
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10388, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10388
Update 10389, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10390, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10391, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10392, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4164
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10393, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0415
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10394, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10395, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10396, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10397, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5450
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10398, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1490
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10399, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3022
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10400, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10401, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10402, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5325
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10403, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10404, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10405, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10406, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1698
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10407, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10408, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10409, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4314
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10410, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3738
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10411, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10412, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10413, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10414, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0768
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10415, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10416, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10416
Update 10417, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10418, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10419, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10420, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10421, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10422, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10423, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1510
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10424, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10425, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10426, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10427, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10428, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10429, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5619
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10430, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4231
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10431, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10432, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2771
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10433, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10434, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1732
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10435, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0931
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10436, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10437, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10438, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10439, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2727
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10440, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10441, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2507
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10442, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4693
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10443, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3304
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10444, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0644
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10444
Update 10445, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10446, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10447, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10448, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4740
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10449, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10450, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10451, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3219
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10452, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10453, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10454, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10455, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10456, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10457, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10458, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10459, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10460, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10461, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2962
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10462, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10463, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10464, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10465, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10466, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10467, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10468, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10469, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10470, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4078
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10471, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3936
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10472, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10472
Update 10473, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10474, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10475, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10476, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2750
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10477, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3376
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10478, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10479, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10480, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5439
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10481, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10482, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1920
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10483, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10484, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2981
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10485, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10486, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5468
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10487, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10488, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10489, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10490, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3808
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10491, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10492, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10493, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0392
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10494, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0228
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10495, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10496, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10497, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2308
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10498, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3348
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10499, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10500, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10500
Update 10501, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10502, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10503, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0787
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10504, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10505, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2970
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10506, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10507, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3188
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10508, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10509, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0411
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10510, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10511, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10512, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10513, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3298
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10514, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10515, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10516, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1362
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10517, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5688
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10518, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10519, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10520, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0938
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10521, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10522, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2542
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10523, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10524, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10525, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10526, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4125
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10527, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1839
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10528, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10528
Update 10529, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10530, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10531, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10532, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10533, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10534, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1913
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10535, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1597
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10536, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10537, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4429
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10538, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4127
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10539, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2001
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10540, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10541, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1196
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10542, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10543, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10544, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10545, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3524
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10546, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10547, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10548, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10549, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3667
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10550, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5258
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10551, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0753
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10552, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10553, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2453
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10554, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3811
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10555, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
Update 10556, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1089.9815, l 200.0000, t 268.3228, TestReward -1223.7132
New EPOCH! 10556
