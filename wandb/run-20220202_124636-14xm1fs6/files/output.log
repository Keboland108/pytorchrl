Sequential(
  (0): Linear(in_features=4, out_features=500, bias=True)
  (1): ReLU()
  (2): Linear(in_features=500, out_features=500, bias=True)
  (3): ReLU()
  (4): DeterministicMB(
    (output): Linear(in_features=500, out_features=4, bias=True)
  )
)
Training model from scratch
Training model from scratch
Collecting initial samples...
Created CWorker with worker_index 0
Created GWorker with worker_index 0
New EPOCH! 0
Update 1, num samples collected 5250, FPS 324
  Algorithm: train_loss 1.0993
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 2, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.7994
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 3, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.6512
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 4, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.2453
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 5, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.1855
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 6, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 7, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.1364
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 8, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.4510
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 9, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.1455
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 10, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.2778
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 11, num samples collected 5250, FPS 324
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 12, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 13, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.1412
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 14, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.0821
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 15, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 16, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.7283
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 17, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 18, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.5307
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 19, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 20, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 21, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 21
Update 22, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 23, num samples collected 5250, FPS 323
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 24, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 25, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 26, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 27, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.3571
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 28, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 29, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 30, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 31, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.2425
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 32, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.1415
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 33, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 34, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.3607
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 35, num samples collected 5250, FPS 322
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 36, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 37, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.2395
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 38, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 39, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.2161
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 40, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 41, num samples collected 5250, FPS 321
  Algorithm: train_loss 1.3151
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 42, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.2178
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 42
Update 43, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 44, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 45, num samples collected 5250, FPS 321
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 46, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 47, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 48, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 49, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.2144
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 50, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 51, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.1121
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 52, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 53, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.3078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 54, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.2582
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 55, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.4235
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 56, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 57, num samples collected 5250, FPS 320
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 58, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 59, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.6226
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 60, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.6654
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 61, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 62, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.3020
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 63, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 63
Update 64, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 65, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 66, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.3939
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 67, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.1783
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 68, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 69, num samples collected 5250, FPS 319
  Algorithm: train_loss 0.1381
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 70, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.4569
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 71, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 72, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.2826
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 73, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 74, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.3176
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 75, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 76, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 77, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.4927
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 78, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 79, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.1191
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 80, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.0520
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 81, num samples collected 5250, FPS 318
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 82, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 83, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 84, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.4536
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 84
Update 85, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.2468
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 86, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 87, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.1264
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 88, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 89, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.5262
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 90, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 91, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 92, num samples collected 5250, FPS 317
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 93, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2248
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 94, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 95, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 96, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1423
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 97, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 98, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 99, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 100, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.2556
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 101, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 102, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 103, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.3053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 104, num samples collected 5250, FPS 316
  Algorithm: train_loss 0.6191
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 105, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.5435
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 105
Update 106, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.2956
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 107, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 108, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.4158
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 109, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0716
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 110, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.4600
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 111, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 112, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 113, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 114, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 115, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 116, num samples collected 5250, FPS 315
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 117, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 118, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.6116
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 119, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.2533
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 120, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 121, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.1814
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 122, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 123, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 124, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0779
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 125, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.4386
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 126, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 126
Update 127, num samples collected 5250, FPS 314
  Algorithm: train_loss 0.2556
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 128, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2121
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 129, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 130, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 131, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.3780
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 132, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 133, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 134, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 135, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 136, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2624
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 137, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 138, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2824
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 139, num samples collected 5250, FPS 313
  Algorithm: train_loss 0.2260
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 140, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2996
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 141, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 142, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.3538
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 143, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.4638
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 144, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2014
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 145, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 146, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 147, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 147
Update 148, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.5788
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 149, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 150, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 151, num samples collected 5250, FPS 312
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 152, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 153, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 154, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1358
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 155, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 156, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 157, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 158, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 159, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 160, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.0489
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 161, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.2340
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 162, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.4559
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 163, num samples collected 5250, FPS 311
  Algorithm: train_loss 0.1374
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 164, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 165, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 166, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.4912
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 167, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.2391
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 168, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 168
Update 169, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.4035
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 170, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 171, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 172, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 173, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 174, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.1757
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 175, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 176, num samples collected 5250, FPS 310
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 177, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0606
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 178, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.2517
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 179, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 180, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 181, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.3417
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 182, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.4364
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 183, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 184, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.6877
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 185, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 186, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 187, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 188, num samples collected 5250, FPS 309
  Algorithm: train_loss 0.4820
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 189, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 189
Update 190, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 191, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.6249
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 192, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 193, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 194, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 195, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 196, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 197, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 198, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 199, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 200, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 201, num samples collected 5250, FPS 308
  Algorithm: train_loss 0.4853
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 202, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 203, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2781
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 204, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.5103
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 205, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.3080
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 206, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 207, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 208, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 209, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.3243
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 210, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 210
Update 211, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.3453
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 212, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.1885
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 213, num samples collected 5250, FPS 307
  Algorithm: train_loss 0.2277
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 214, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 215, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 216, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 217, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 218, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.1818
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 219, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.1056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 220, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 221, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3803
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 222, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.2052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 223, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.1615
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 224, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 225, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.3028
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 226, num samples collected 5250, FPS 306
  Algorithm: train_loss 0.2794
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 227, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 228, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 229, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.4270
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 230, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 231, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 231
Update 232, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 233, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 234, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 235, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.1743
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 236, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.5756
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 237, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.4521
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 238, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 239, num samples collected 5250, FPS 305
  Algorithm: train_loss 0.1369
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 240, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0644
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 241, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 242, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 243, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.4125
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 244, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 245, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 246, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.2316
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 247, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 248, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 249, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.3777
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 250, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.1050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 251, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 252, num samples collected 5250, FPS 304
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 252
Update 253, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3555
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 254, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.4224
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 255, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 256, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 257, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.3168
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 258, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 259, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 260, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 261, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 262, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.2807
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 263, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 264, num samples collected 5250, FPS 303
  Algorithm: train_loss 0.2933
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 265, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 266, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 267, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 268, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 269, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.1557
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 270, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.1313
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 271, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 272, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 273, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.1947
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 273
Update 274, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2518
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 275, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 276, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2322
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 277, num samples collected 5250, FPS 302
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 278, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 279, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.3143
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 280, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 281, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.2052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 282, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 283, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1759
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 284, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.4426
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 285, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 286, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1108
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 287, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.3428
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 288, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.4356
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 289, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 290, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 291, num samples collected 5250, FPS 301
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 292, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 293, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 294, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 294
Update 295, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.5428
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 296, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1041
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 297, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.3482
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 298, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.2294
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 299, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1941
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 300, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 301, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 302, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 303, num samples collected 5250, FPS 300
  Algorithm: train_loss 0.5610
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 304, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 305, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 306, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 307, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.2170
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 308, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 309, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 310, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 311, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.3559
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 312, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.1647
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 313, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 314, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 315, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 315
Update 316, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 317, num samples collected 5250, FPS 299
  Algorithm: train_loss 0.3358
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 318, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1087
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 319, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 320, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 321, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 322, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1302
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 323, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.3930
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 324, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1806
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 325, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 326, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 327, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.6962
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 328, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1337
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 329, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 330, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 331, num samples collected 5250, FPS 298
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 332, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.4227
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 333, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.2411
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 334, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 335, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 336, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 336
Update 337, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 338, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 339, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.2307
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 340, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.4985
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 341, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.1443
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 342, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 343, num samples collected 5250, FPS 297
  Algorithm: train_loss 0.3076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 344, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 345, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.1818
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 346, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 347, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 348, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.6056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 349, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 350, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 351, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.1097
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 352, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2464
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 353, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2030
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 354, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 355, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 356, num samples collected 5250, FPS 296
  Algorithm: train_loss 0.3639
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 357, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 357
Update 358, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 359, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1102
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 360, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 361, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2439
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 362, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 363, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 364, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.6788
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 365, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2660
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 366, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 367, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 368, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.1820
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 369, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2311
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 370, num samples collected 5250, FPS 295
  Algorithm: train_loss 0.2016
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 371, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 372, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 373, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 374, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 375, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 376, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.4598
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 377, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.1504
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 378, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.4158
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 378
Update 379, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2725
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 380, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.1020
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 381, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 382, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 383, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.3247
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 384, num samples collected 5250, FPS 294
  Algorithm: train_loss 0.4405
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 385, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.5826
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 386, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 387, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 388, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1100
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 389, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 390, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3848
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 391, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 392, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 393, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 394, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 395, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 396, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.3285
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 397, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 398, num samples collected 5250, FPS 293
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 399, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 399
Update 400, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.3807
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 401, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 402, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 403, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 404, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 405, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 406, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1121
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 407, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 408, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.3332
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 409, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 410, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 411, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.3997
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 412, num samples collected 5250, FPS 292
  Algorithm: train_loss 0.4565
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 413, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 414, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 415, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3988
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 416, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.3564
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 417, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.1821
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 418, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 419, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 420, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 420
Update 421, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.6666
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 422, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.2167
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 423, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0332
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 424, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.1605
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 425, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 426, num samples collected 5250, FPS 291
  Algorithm: train_loss 0.0730
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 427, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 428, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 429, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 430, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 431, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.8638
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 432, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 433, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 434, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.2084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 435, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 436, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0609
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 437, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 438, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.1336
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 439, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 440, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.3178
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 441, num samples collected 5250, FPS 290
  Algorithm: train_loss 0.2772
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 441
Update 442, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 443, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 444, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 445, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.1411
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 446, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.3711
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 447, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 448, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.3523
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 449, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 450, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 451, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.6136
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 452, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 453, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.3627
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 454, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.5209
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 455, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.2801
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 456, num samples collected 5250, FPS 289
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 457, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 458, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 459, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.2803
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 460, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 461, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 462, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.2638
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 462
Update 463, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 464, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 465, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.3454
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 466, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 467, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 468, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.2406
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 469, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 470, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 471, num samples collected 5250, FPS 288
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 472, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 473, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 474, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 475, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 476, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2588
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 477, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2539
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 478, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 479, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2089
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 480, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 481, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.2649
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 482, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.5695
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 483, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 483
Update 484, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 485, num samples collected 5250, FPS 287
  Algorithm: train_loss 0.5634
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 486, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 487, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 488, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3080
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 489, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.1576
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 490, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 491, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 492, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 493, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3735
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 494, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3507
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 495, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.4560
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 496, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 497, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 498, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 499, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.1793
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 500, num samples collected 5250, FPS 286
  Algorithm: train_loss 0.3724
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 501, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 502, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 503, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2380
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 504, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 504
Update 505, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 506, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2519
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 507, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 508, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 509, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.6049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 510, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.3749
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 511, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1988
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 512, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1527
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 513, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 514, num samples collected 5250, FPS 285
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 515, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 516, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 517, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.2111
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 518, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 519, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 520, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.1773
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 521, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 522, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.2173
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 523, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 524, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0485
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 525, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 525
Update 526, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 527, num samples collected 5250, FPS 284
  Algorithm: train_loss 0.1058
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 528, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 529, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2104
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 530, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.1027
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 531, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 532, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 533, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 534, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2974
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 535, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.3890
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 536, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 537, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 538, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.5492
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 539, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 540, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 541, num samples collected 5250, FPS 283
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 542, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.5839
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 543, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.1732
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 544, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 545, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.4783
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 546, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0836
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 546
Update 547, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 548, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 549, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.2913
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 550, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.4690
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 551, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.4349
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 552, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 553, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 554, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 555, num samples collected 5250, FPS 282
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 556, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 557, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1821
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 558, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1009
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 559, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 560, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 561, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.4252
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 562, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.4445
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 563, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 564, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 565, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 566, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.2244
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 567, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 567
Update 568, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 569, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 570, num samples collected 5250, FPS 281
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 571, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 572, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 573, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 574, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 575, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1572
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 576, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2161
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 577, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 578, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2032
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 579, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1540
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 580, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.6355
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 581, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 582, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 583, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 584, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 585, num samples collected 5250, FPS 280
  Algorithm: train_loss 0.1733
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 586, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 587, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 588, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 588
Update 589, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.1117
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 590, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2399
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 591, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.2277
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 592, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.0947
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 593, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.3675
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 594, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.3755
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 595, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 596, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.6332
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 597, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.1827
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 598, num samples collected 5250, FPS 279
  Algorithm: train_loss 0.1056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 599, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 600, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 601, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 602, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 603, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 604, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1337
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 605, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 606, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 607, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 608, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.3917
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 609, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 609
Update 610, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 611, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 612, num samples collected 5250, FPS 278
  Algorithm: train_loss 0.3348
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 613, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 614, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.2676
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 615, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 616, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 617, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 618, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 619, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.4883
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 620, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 621, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 622, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 623, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 624, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.2026
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 625, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.7012
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 626, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 627, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 628, num samples collected 5250, FPS 277
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 629, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 630, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0821
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 630
Update 631, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 632, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.3682
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 633, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2031
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 634, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 635, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.5135
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 636, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 637, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 638, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 639, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 640, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.1511
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 641, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 642, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 643, num samples collected 5250, FPS 276
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 644, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1020
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 645, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 646, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 647, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 648, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2816
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 649, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.5189
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 650, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 651, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2761
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 651
Update 652, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 653, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 654, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 655, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.4458
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 656, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.4356
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 657, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.1739
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 658, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 659, num samples collected 5250, FPS 275
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 660, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.1795
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 661, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 662, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 663, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.4167
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 664, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 665, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 666, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 667, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.3245
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 668, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 669, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 670, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 671, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.4486
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 672, num samples collected 5250, FPS 274
  Algorithm: train_loss 0.3014
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 672
Update 673, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 674, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 675, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 676, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 677, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1352
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 678, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 679, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 680, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.4261
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 681, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 682, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 683, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 684, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1760
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 685, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 686, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.1091
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 687, num samples collected 5250, FPS 273
  Algorithm: train_loss 0.3049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 688, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 689, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2429
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 690, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2492
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 691, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2429
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 692, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2038
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 693, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.4159
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 693
Update 694, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 695, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 696, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2773
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 697, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 698, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 699, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 700, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.1107
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 701, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.1577
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 702, num samples collected 5250, FPS 272
  Algorithm: train_loss 0.3010
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 703, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.5642
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 704, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 705, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 706, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 707, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 708, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.4838
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 709, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 710, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 711, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 712, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 713, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 714, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 714
Update 715, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 716, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 717, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 718, num samples collected 5250, FPS 271
  Algorithm: train_loss 0.1015
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 719, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 720, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 721, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 722, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 723, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 724, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.3115
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 725, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.4367
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 726, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.8280
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 727, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 728, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 729, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 730, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 731, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 732, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 733, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2422
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 734, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.2327
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 735, num samples collected 5250, FPS 270
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 735
Update 736, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.3325
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 737, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 738, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 739, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 740, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.3470
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 741, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 742, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 743, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.6828
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 744, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 745, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2545
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 746, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.4287
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 747, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1379
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 748, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2016
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 749, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 750, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 751, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.2829
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 752, num samples collected 5250, FPS 269
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 753, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1739
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 754, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 755, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 756, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.4383
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 756
Update 757, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1548
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 758, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.3078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 759, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 760, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 761, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 762, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 763, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 764, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2658
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 765, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2378
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 766, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 767, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2726
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 768, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.2781
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 769, num samples collected 5250, FPS 268
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 770, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.4381
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 771, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.3751
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 772, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 773, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 774, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 775, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.4796
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 776, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 777, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 777
Update 778, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 779, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 780, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.1060
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 781, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.1318
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 782, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.3163
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 783, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 784, num samples collected 5250, FPS 267
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 785, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 786, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.6280
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 787, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.3507
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 788, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 789, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 790, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0955
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 791, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 792, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 793, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.4178
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 794, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 795, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 796, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 797, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 798, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.9642
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 798
Update 799, num samples collected 5250, FPS 266
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 800, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.4395
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 801, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 802, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 803, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 804, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 805, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2353
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 806, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 807, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 808, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 809, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3820
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 810, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2322
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 811, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.3357
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 812, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 813, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 814, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 815, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 816, num samples collected 5250, FPS 265
  Algorithm: train_loss 0.6656
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 817, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 818, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.3137
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 819, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 819
Update 820, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1339
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 821, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 822, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 823, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1452
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 824, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1241
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 825, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 826, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 827, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 828, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 829, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2978
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 830, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.3432
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 831, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.4783
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 832, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.2588
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 833, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 834, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.3970
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 835, num samples collected 5250, FPS 264
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 836, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 837, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 838, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 839, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.3088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 840, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 840
Update 841, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2247
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 842, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 843, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 844, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.1356
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 845, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 846, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2406
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 847, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 848, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2402
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 849, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 850, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.1788
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 851, num samples collected 5250, FPS 263
  Algorithm: train_loss 0.2386
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 852, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.5722
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 853, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 854, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 855, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.4424
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 856, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 857, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.2900
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 858, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 859, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.2061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 860, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 861, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 861
Update 862, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 863, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 864, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 865, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 866, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 867, num samples collected 5250, FPS 262
  Algorithm: train_loss 0.1783
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 868, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.7201
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 869, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 870, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 871, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 872, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 873, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 874, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.4848
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 875, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 876, num samples collected 5250, FPS 261
  Algorithm: train_loss 0.4613
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 877, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.1094
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 878, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 879, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.1441
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 880, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 881, num samples collected 5250, FPS 260
  Algorithm: train_loss 0.1355
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 882, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 882
Update 883, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 884, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 885, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5610
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 886, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5331
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 887, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 888, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 889, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 890, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.1987
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 891, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 892, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 893, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.5901
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 894, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.3633
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 895, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 896, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 897, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 898, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.1740
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 899, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.3019
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 900, num samples collected 5250, FPS 259
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 901, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2706
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 902, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 903, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 903
Update 904, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 905, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1325
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 906, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 907, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2238
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 908, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.2491
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 909, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 910, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 911, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.4163
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 912, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 913, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 914, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 915, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.1310
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 916, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.4357
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 917, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.3508
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 918, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.3902
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 919, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 920, num samples collected 5250, FPS 258
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 921, num samples collected 5250, FPS 257
  Algorithm: train_loss 0.0741
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 922, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.1016
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 923, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 924, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.5332
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 924
Update 925, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 926, num samples collected 5250, FPS 256
  Algorithm: train_loss 0.3229
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 927, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 928, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.4093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 929, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1457
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 930, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 931, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 932, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 933, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.6756
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 934, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 935, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 936, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 937, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1322
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 938, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0856
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 939, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 940, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1343
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 941, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.1696
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 942, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 943, num samples collected 5250, FPS 255
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 944, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 945, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 945
Update 946, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0488
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 947, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1831
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 948, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 949, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 950, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 951, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 952, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 953, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2547
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 954, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 955, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2466
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 956, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2665
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 957, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 958, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1792
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 959, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.1068
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 960, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.5094
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 961, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 962, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.3027
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 963, num samples collected 5250, FPS 254
  Algorithm: train_loss 0.2417
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 964, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 965, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 966, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 966
Update 967, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.3557
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 968, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 969, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.3706
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 970, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 971, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.2008
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 972, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 973, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.4468
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 974, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 975, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 976, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 977, num samples collected 5250, FPS 253
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 978, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2363
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 979, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1297
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 980, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 981, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 982, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3349
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 983, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 984, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 985, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 986, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1591
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 987, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3484
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 987
Update 988, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 989, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1023
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 990, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 991, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 992, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 993, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.4746
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 994, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.4378
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 995, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.3514
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 996, num samples collected 5250, FPS 252
  Algorithm: train_loss 0.1942
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 997, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 998, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1126
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 999, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.2409
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1000, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1024
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1001, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1002, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1583
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1003, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1004, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.3369
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1005, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1006, num samples collected 5250, FPS 251
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1007, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1008, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1008
Update 1009, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2029
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1010, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.3166
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1011, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.3732
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1012, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1013, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.5449
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1014, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1015, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1016, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1017, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.2099
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1018, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1352
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1019, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1353
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1020, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1021, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.3550
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1022, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.3371
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1023, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1751
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1024, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.1550
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1025, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1026, num samples collected 5250, FPS 250
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1027, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1028, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1029, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1029
Update 1030, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1031, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1032, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2881
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1033, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1034, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1035, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1036, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3332
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1037, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1038, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1039, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2690
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1040, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1041, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1042, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1043, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3867
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1044, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.3701
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1045, num samples collected 5250, FPS 249
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1046, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1047, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.6101
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1048, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3003
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1049, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1050, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1050
Update 1051, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.6449
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1052, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2934
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1053, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1054, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1055, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1056, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.3398
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1057, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1058, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1059, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1060, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.2916
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1061, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.1064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1062, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1063, num samples collected 5250, FPS 248
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1064, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.3058
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1065, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.2855
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1066, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1067, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1092
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1068, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0912
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1069, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1708
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1070, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.4066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1071, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1071
Update 1072, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1073, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.4514
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1074, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.5029
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1075, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1076, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1077, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1078, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.2483
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1079, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1080, num samples collected 5250, FPS 247
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1081, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1082, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.5090
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1083, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1084, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1085, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.6663
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1086, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1087, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1088, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1089, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2411
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1090, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1091, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2749
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1092, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.4699
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1092
Update 1093, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1094, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.0443
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1095, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.2017
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1096, num samples collected 5250, FPS 246
  Algorithm: train_loss 0.3791
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1097, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1098, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2706
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1099, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1100, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2902
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1101, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1102, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1103, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1104, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1105, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.4258
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1106, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2727
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1107, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.6187
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1108, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1109, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.2178
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1110, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1111, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1550
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1112, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1113, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1113
Update 1114, num samples collected 5250, FPS 245
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1115, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2444
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1116, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2268
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1117, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1118, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3271
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1119, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1120, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1121, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1122, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1123, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1124, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1777
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1125, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1126, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.4514
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1127, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1128, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3344
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1129, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1130, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.3604
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1131, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1132, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1133, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1134, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.5555
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1134
Update 1135, num samples collected 5250, FPS 244
  Algorithm: train_loss 0.1530
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1136, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1137, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.4833
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1138, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1139, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1335
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1140, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2466
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1141, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1142, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1758
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1143, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1003
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1144, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.3081
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1145, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1146, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1147, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1869
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1148, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1149, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1150, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.5947
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1151, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1152, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.1911
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1153, num samples collected 5250, FPS 243
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1154, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1155, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1155
Update 1156, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1611
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1157, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2385
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1158, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1159, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1981
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1160, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2109
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1161, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.4459
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1162, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.3307
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1163, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1164, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2537
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1165, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1166, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1016
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1167, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1168, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1169, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.1405
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1170, num samples collected 5250, FPS 242
  Algorithm: train_loss 0.2263
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1171, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1172, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.5179
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1173, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.1803
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1174, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.1105
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1175, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1176, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.4021
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1176
Update 1177, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3994
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1178, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1179, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1180, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1181, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3571
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1182, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1183, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.1355
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1184, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.2252
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1185, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1186, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.3906
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1187, num samples collected 5250, FPS 241
  Algorithm: train_loss 0.0459
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1188, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1189, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1190, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.3505
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1191, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1192, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1193, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1194, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1195, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.5759
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1196, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1197, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.2744
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1197
Update 1198, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.4118
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1199, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1365
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1200, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1102
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1201, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.1110
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1202, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1203, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.4209
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1204, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.5373
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1205, num samples collected 5250, FPS 240
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1206, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1207, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2695
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1208, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1209, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1210, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.4218
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1211, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1212, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1213, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1214, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1215, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2605
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1216, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1217, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1218, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1218
Update 1219, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1220, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.3927
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1221, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.5887
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1222, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1223, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.1577
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1224, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1225, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.7800
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1226, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1227, num samples collected 5250, FPS 239
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1228, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1229, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1230, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1231, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1232, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3739
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1233, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1017
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1234, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1235, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1236, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1051
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1237, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3356
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1238, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1239, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1239
Finished MB training, ran for 60 epochs
Update 1240, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1107
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1241, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1242, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1799
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1243, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1244, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.4489
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1245, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1246, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1247, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.3398
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1248, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1249, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2923
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1250, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2245
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1251, num samples collected 5250, FPS 238
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1252, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.4102
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1253, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1254, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1255, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1256, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1257, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1258, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.3342
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1259, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.2007
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
Update 1260, num samples collected 5250, FPS 237
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -1266.6523, l 200.0000, t 100.1285, TestReward -1420.5623
New EPOCH! 1260
Update 1261, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1262, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.3580
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1263, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1264, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.3917
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1265, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0878
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1266, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.1397
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1267, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.2128
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1268, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1269, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.4103
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1270, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1271, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1272, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.3458
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1273, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1274, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1275, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1276, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.1639
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1277, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1278, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1279, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.5684
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1280, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1281, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.2370
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1282, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1282
Update 1283, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1284, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.4629
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1285, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1286, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.5878
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1287, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.4087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1288, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1289, num samples collected 5500, FPS 141
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1290, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.4561
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1291, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1292, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1293, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.3197
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1294, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.4027
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1295, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1296, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1297, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1298, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1299, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1166
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1300, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1301, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1818
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1302, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1303, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1304, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1304
Update 1305, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1306, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1307, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1880
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1308, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1309, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1310, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1311, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1312, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1322
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1313, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1314, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.5420
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1315, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1316, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1336
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1317, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1318, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1319, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.6574
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1320, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.4360
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1321, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1322, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1323, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1324, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.5590
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1325, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1326, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1326
Update 1327, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1793
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1328, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1329, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1330, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1331, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1332, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1333, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1334, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.3603
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1335, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1336, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1337, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.3457
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1338, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1339, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1340, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.3230
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1341, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1342, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.2859
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1343, num samples collected 5500, FPS 140
  Algorithm: train_loss 0.4122
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1344, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1345, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3354
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1346, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1347, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3258
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1348, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1348
Update 1349, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1350, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1351, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1352, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1585
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1353, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2574
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1354, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1121
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1355, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1356, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.6146
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1357, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1761
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1358, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1359, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1360, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1361, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2132
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1362, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1324
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1363, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.6666
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1364, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1809
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1365, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1366, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1367, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0831
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1368, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1369, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2849
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1370, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1370
Update 1371, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1372, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3156
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1373, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1018
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1374, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1343
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1375, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2594
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1376, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2397
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1377, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3016
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1378, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2132
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1379, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1380, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1381, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1382, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.4270
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1383, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1384, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.5330
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1385, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1386, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1387, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1388, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1389, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1390, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1391, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1392, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1392
Update 1393, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1000
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1394, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3426
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1395, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1396, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2120
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1397, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1398, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1399, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1400, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.2732
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1401, num samples collected 5500, FPS 139
  Algorithm: train_loss 0.3996
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1402, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3530
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1403, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1477
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1404, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1405, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4086
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1406, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1407, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1408, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1409, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1410, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1309
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1411, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3784
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1412, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1413, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2138
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1414, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1414
Update 1415, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1416, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1417, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2962
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1418, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3014
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1419, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2639
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1420, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3238
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1421, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1422, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1423, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1424, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1425, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1426, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4190
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1427, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2841
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1428, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2281
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1429, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2378
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1430, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3073
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1431, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1432, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1433, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1434, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1401
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1435, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1436, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1436
Update 1437, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2252
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1438, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1439, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1440, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3476
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1441, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1442, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4432
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1443, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1444, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1445, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1446, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3116
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1447, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4209
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1448, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.4457
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1449, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1450, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1451, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1452, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1453, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.3092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1454, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1455, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1456, num samples collected 5500, FPS 138
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1457, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1108
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1458, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1458
Update 1459, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1460, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1461, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2213
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1462, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4936
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1463, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1464, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1465, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2138
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1466, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2641
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1467, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4296
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1468, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1469, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1470, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1471, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1111
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1472, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4680
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1473, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1474, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1475, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1476, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1477, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1478, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1479, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1480, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1480
Update 1481, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0446
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1482, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1483, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3286
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1484, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1485, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1304
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1486, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1487, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4925
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1488, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1489, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1490, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1491, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1492, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1493, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4801
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1494, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1495, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3284
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1496, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.3438
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1497, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1498, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2370
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1499, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1500, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.5183
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1501, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1502, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1502
Update 1503, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.4119
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1504, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0936
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1505, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1506, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.2230
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1507, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1508, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1509, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.6844
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1510, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1511, num samples collected 5500, FPS 137
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1512, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1513, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1514, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1515, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1516, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1517, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1518, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1519, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.6492
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1520, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1521, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1522, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1523, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.5169
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1524, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1524
Update 1525, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1876
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1526, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1527, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1528, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1376
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1529, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1530, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0930
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1531, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1532, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.5738
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1533, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1534, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1535, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1536, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.5441
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1537, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1538, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1539, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1540, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1541, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1542, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2539
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1543, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3028
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1544, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1545, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1546, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3557
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1546
Update 1547, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1548, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1549, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1550, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3800
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1551, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1552, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1553, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1554, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1951
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1555, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2628
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1556, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1557, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1558, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1559, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1560, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.2978
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1561, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1562, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1563, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1564, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1565, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.3374
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1566, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1567, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1568, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.4201
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1568
Update 1569, num samples collected 5500, FPS 136
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1570, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1571, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1572, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.5853
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1573, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1400
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1574, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2144
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1575, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1361
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1576, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1577, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1578, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2960
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1579, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1580, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1581, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3764
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1582, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1583, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2682
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1584, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4385
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1585, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1586, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1587, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1588, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1589, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4938
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1590, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1590
Update 1591, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3660
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1592, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1593, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1735
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1594, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2239
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1595, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1596, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1509
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1597, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1598, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4318
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1599, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1600, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3001
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1601, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2627
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1602, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1603, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3338
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1604, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1605, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1606, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1607, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1608, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2954
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1609, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1354
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1610, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1611, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1612, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1612
Update 1613, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1614, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1615, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1616, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.1822
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1617, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1618, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3610
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1619, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1620, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2358
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1621, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1622, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1623, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1624, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.4176
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1625, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.5401
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1626, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1627, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2866
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1628, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1629, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1630, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2996
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1631, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.3262
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1632, num samples collected 5500, FPS 135
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1633, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3209
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1634, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1634
Update 1635, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1510
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1636, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4213
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1637, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3562
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1638, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1639, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1640, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1641, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1642, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1643, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4022
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1644, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1645, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1646, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3403
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1647, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3675
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1648, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1649, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4464
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1650, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1651, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4001
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1652, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1653, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1654, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1655, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1656, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1656
Update 1657, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.3025
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1658, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1659, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1660, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1661, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1743
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1662, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1663, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1664, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1218
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1665, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1666, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1667, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2143
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1668, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1669, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1670, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1671, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1672, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.8926
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1673, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.5728
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1674, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2082
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1675, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1583
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1676, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2148
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1677, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1678, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.4282
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1678
Update 1679, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1680, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1681, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1682, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.6983
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1683, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1684, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1685, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1686, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1687, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1688, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1689, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1690, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1691, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1692, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2737
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1693, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1694, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1695, num samples collected 5500, FPS 134
  Algorithm: train_loss 0.2556
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1696, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1697, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4127
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1698, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1699, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1700, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.7150
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1700
Update 1701, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4329
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1702, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1703, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1704, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1106
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1705, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1706, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2053
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1707, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1708, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3122
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1709, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4851
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1710, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1711, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1028
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1712, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1713, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1714, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1715, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1716, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1740
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1717, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1718, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1719, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1720, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1721, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3352
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1722, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.8879
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1722
Update 1723, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1724, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1725, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1817
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1726, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1727, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1121
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1728, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1729, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3015
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1730, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1731, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1732, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2128
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1733, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1734, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4301
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1735, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1364
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1736, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3878
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1737, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1738, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1739, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1740, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1197
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1741, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4520
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1742, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3946
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1743, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.4093
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1744, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1744
Update 1745, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1746, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1747, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1748, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2901
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1749, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1750, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1751, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1752, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3184
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1753, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1798
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1754, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1755, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1756, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.5687
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1757, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1758, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1759, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2501
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1760, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2232
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1761, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1762, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.3185
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1763, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1764, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1783
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1765, num samples collected 5500, FPS 133
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1766, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4232
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1766
Update 1767, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4111
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1768, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1831
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1769, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1770, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1771, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1772, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1773, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3800
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1774, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1775, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1776, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4796
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1777, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1778, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1779, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3553
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1780, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1091
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1781, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1782, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1783, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1784, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1785, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1786, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.5075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1787, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1788, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0868
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1788
Update 1789, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2661
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1790, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1791, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1792, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1793, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1794, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3753
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1795, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1796, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1797, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1332
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1798, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1336
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1799, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1800, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2912
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1801, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1802, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1803, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.6430
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1804, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0931
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1805, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1806, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1807, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4584
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1808, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.4119
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1809, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1810, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1810
Update 1811, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3515
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1812, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1813, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2244
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1814, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1095
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1815, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1816, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1095
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1817, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1818, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3749
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1819, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3832
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1820, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1821, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1822, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1823, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2939
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1824, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1825, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1826, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1827, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1828, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.2130
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1829, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.3984
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1830, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1831, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.1381
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1832, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1832
Update 1833, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1834, num samples collected 5500, FPS 132
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1835, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3468
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1836, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1837, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1838, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1839, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1840, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2511
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1841, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3807
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1842, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1843, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.6900
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1844, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1845, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3831
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1846, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4632
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1847, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1848, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1849, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4311
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1850, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1212
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1851, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1852, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1853, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1854, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1854
Update 1855, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1856, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1857, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4136
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1858, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1859, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3217
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1860, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0434
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1861, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3375
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1862, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2503
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1863, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1457
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1864, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1865, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3038
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1866, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1867, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1868, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2115
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1869, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1870, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4910
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1871, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1872, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1873, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1874, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1875, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1876, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4837
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1876
Update 1877, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1878, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1879, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1880, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1881, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1882, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1883, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.5873
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1884, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1885, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2706
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1886, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3790
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1887, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.3055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1888, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2794
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1889, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.4181
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1890, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1891, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1892, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.2089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1893, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1894, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1895, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1896, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1897, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1898, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1898
Update 1899, num samples collected 5500, FPS 131
  Algorithm: train_loss 0.0857
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1900, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1901, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1902, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1903, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3003
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1904, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2866
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1905, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2260
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1906, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1748
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1907, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1908, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1909, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1910, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1911, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3485
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1912, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4137
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1913, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2902
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1914, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1915, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1916, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3524
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1917, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1918, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1919, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4272
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1920, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1920
Update 1921, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1922, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.6176
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1923, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.5344
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1924, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2797
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1925, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1401
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1926, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1927, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1928, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1929, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1930, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3759
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1931, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1932, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1035
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1933, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1934, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1935, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1936, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1937, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1938, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1054
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1939, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1940, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1941, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2258
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1942, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1942
Update 1943, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1944, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1945, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1946, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3820
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1947, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1948, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1949, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1950, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1310
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1951, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1952, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1953, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3739
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1954, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1955, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1504
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1956, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1961
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1957, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.6440
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1958, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1959, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1097
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1960, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2434
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1961, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.3455
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1962, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1963, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1964, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1964
Update 1965, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2864
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1966, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1967, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1012
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1968, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1969, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1970, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.2383
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1971, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1972, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.4561
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1973, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.1111
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1974, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.5276
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1975, num samples collected 5500, FPS 130
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1976, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1977, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1978, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1979, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1980, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2668
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1981, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3710
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1982, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0609
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1983, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1984, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2168
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1985, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1986, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 1986
Update 1987, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3693
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1988, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4411
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1989, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1990, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3593
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1991, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1992, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1993, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1994, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0886
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1995, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1996, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1997, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3284
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1998, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 1999, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2104
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2000, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2001, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3210
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2002, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2003, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2004, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2005, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2006, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3598
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2007, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2008, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2008
Update 2009, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2010, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4096
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2011, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1737
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2012, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2013, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2730
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2014, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2015, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2016, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2017, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3808
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2018, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2019, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2487
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2020, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4154
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2021, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2705
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2022, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1035
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2023, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2024, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2025, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2026, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2027, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2028, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2029, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1273
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2030, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1128
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2030
Update 2031, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4260
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2032, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3556
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2033, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2034, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2035, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2036, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2037, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3738
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2038, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2039, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2040, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.3217
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2041, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2042, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2043, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2044, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2045, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1397
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2046, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2047, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2048, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2049, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1415
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2050, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.4969
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2051, num samples collected 5500, FPS 129
  Algorithm: train_loss 0.1444
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2052, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2052
Update 2053, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4855
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2054, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2055, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4500
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2056, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2057, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2058, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2059, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2060, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2061, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2062, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1133
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2063, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2064, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2065, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3018
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2066, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2067, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2068, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2069, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1680
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2070, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.6467
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2071, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1450
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2072, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2073, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2074, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2074
Update 2075, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0835
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2076, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1839
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2077, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2078, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4053
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2079, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2080, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1491
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2081, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2082, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2083, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2084, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2547
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2085, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0853
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2086, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2087, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2088, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2089, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2090, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4476
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2091, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2092, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2093, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2094, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2095, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2096, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2096
Update 2097, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2841
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2098, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2636
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2099, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2100, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2101, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2102, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4852
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2103, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2104, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2105, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2106, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2107, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3924
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2108, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0879
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2109, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2110, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2111, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2612
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2112, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2113, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2114, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0553
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2115, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.4466
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2116, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2117, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2118, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2118
Update 2119, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2120, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2121, num samples collected 5500, FPS 128
  Algorithm: train_loss 0.3358
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2122, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1354
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2123, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2124, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2125, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2359
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2126, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2127, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0728
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2128, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2129, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1561
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2130, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2292
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2131, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1224
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2132, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2133, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2053
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2134, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2135, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2218
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2136, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2137, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2138, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2139, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3100
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2140, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2140
Update 2141, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.4252
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2142, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2143, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3198
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2144, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2145, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0871
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2146, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.4981
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2147, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2148, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2149, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2150, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2151, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2152, num samples collected 5500, FPS 127
  Algorithm: train_loss 0.3531
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2153, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1105
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2154, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0738
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2155, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2835
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2156, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2157, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2158, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1009
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2159, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1368
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2160, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3546
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2161, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2162, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2162
Update 2163, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2164, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2621
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2165, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2166, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2167, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2287
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2168, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2169, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2170, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2171, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1910
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2172, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2173, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2174, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4160
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2175, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3565
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2176, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2177, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2178, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2019
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2179, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2180, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.6809
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2181, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2182, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2183, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2184, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2184
Update 2185, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2186, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2187, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3029
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2188, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4559
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2189, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2190, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2191, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2192, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2193, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0798
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2194, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2195, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2196, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2197, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1404
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2198, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2836
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2199, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4552
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2200, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2201, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.4244
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2202, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2203, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1401
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2204, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1848
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2205, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2206, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2019
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2206
Update 2207, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2208, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2209, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2210, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2790
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2211, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3003
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2212, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2213, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.1110
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2214, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.0835
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2215, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2216, num samples collected 5500, FPS 126
  Algorithm: train_loss 0.3344
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2217, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2230
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2218, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2219, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1792
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2220, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4210
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2221, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2222, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2223, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.5829
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2224, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2191
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2225, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2226, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2227, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2228, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2680
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2228
Update 2229, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2230, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2013
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2231, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2232, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2233, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2234, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2662
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2235, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2236, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0541
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2237, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4964
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2238, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3432
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2239, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2240, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4563
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2241, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2540
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2242, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2243, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2244, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2245, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2246, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.4964
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2247, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1428
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2248, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2249, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2250, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2250
Update 2251, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2033
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2252, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2253, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2254, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2255, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2656
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2256, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3630
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2257, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2258, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2259, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2260, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2261, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2262, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.2793
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2263, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2264, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2265, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2266, num samples collected 5500, FPS 125
  Algorithm: train_loss 0.3168
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2267, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2268, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3827
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2269, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2270, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2874
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2271, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2931
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2272, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2272
Update 2273, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2274, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.4814
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2275, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1867
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2276, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2277, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2278, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2279, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2280, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2281, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2282, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2874
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2283, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2524
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2284, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2285, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2286, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2287, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2288, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2217
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2289, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2290, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.5325
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2291, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2292, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2293, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2294, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2294
Update 2295, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2296, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2688
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2297, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.3344
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2298, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0766
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2299, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2300, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2301, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2302, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2303, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1982
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2304, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2455
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2305, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2306, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2307, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2308, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2086
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2309, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2217
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2310, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2311, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2334
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2312, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.1906
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2313, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2314, num samples collected 5500, FPS 124
  Algorithm: train_loss 0.2430
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2315, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2316, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2171
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2316
Update 2317, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2318, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3089
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2319, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1801
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2320, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2321, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2322, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1027
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2323, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1599
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2324, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1525
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2325, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2882
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2326, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2327, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2328, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2329, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2330, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2331, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2332, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2394
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2333, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2334, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3401
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2335, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2574
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2336, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3636
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2337, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2338, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.2001
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2338
Update 2339, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.3375
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2340, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2341, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2342, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.4610
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2343, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2344, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.4245
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2345, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2346, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2347, num samples collected 5500, FPS 123
  Algorithm: train_loss 0.0964
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2348, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2349, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2350, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2351, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1693
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2352, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3765
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2353, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2354, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0258
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2355, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2356, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3135
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2357, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3619
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2358, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2359, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2360, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4184
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2360
Update 2361, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2362, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1111
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2363, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1312
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2364, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4969
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2365, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2183
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2366, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3001
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2367, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2368, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2369, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2370, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1825
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2371, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2372, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2373, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4421
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2374, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2375, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2376, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2377, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2378, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3972
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2379, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2380, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2381, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0999
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2382, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2741
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2382
Update 2383, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2384, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2385, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2386, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4280
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2387, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2482
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2388, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1390
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2389, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2390, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2709
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2391, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2392, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2393, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2394, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2158
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2395, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.5732
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2396, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2397, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3171
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2398, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2186
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2399, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2400, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2401, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2402, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4404
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2403, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1190
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2404, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2404
Update 2405, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1317
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2406, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.4656
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2407, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3900
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2408, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2409, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.3651
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2410, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2411, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2412, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2413, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2414, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1351
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2415, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2416, num samples collected 5500, FPS 122
  Algorithm: train_loss 0.1358
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2417, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2418, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1105
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2419, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4044
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2420, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1107
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2421, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2422, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2741
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2423, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4106
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2424, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2425, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2426, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2426
Update 2427, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2428, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2429, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2430, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2431, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3759
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2432, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2810
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2433, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2434, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2435, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2436, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3469
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2437, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2438, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4134
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2439, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2440, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2441, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2442, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2443, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4181
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2444, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4495
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2445, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2446, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2447, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.5184
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2448, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2448
Update 2449, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4252
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2450, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2451, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.2088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2452, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.4080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2453, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1727
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2454, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2455, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2456, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1791
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2457, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.1334
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2458, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0784
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2459, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3154
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2460, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2461, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2462, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.7660
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2463, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.3427
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2464, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2465, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2466, num samples collected 5500, FPS 121
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2467, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2468, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2469, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2470, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2470
Update 2471, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2472, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2473, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2474, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2475, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2476, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2477, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1719
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2478, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.4439
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2479, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1310
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2480, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2481, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2482, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1845
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2483, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2484, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1388
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2485, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2486, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.2517
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2487, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.4078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2488, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.4028
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2489, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2490, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.4212
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2491, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1151
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2492, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2492
Update 2493, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1771
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2494, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2495, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.2176
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2496, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2497, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2498, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2499, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.3010
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2500, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2501, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2502, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2503, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2504, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.3740
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2505, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.2768
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2506, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2507, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.2596
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2508, num samples collected 5500, FPS 120
  Algorithm: train_loss 0.4906
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2509, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2742
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2510, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2511, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1757
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2512, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3312
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2513, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2514, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1566
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2514
Update 2515, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2516, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1117
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2517, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2518, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2519, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2520, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.4721
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2521, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.4277
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2522, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2523, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.4466
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2524, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2525, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2526, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2527, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2528, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3976
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2529, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3163
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2530, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1333
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2531, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2532, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2533, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2534, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2535, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2536, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2536
Update 2537, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2538, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2539, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2540, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2212
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2541, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2542, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2543, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2095
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2544, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2545, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1565
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2546, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2547, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2548, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2549, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2550, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2782
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2551, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2552, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2972
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2553, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2554, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2555, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2233
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2556, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2091
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2557, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1966
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2558, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.4401
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2558
Finished MB training, ran for 60 epochs
Update 2559, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.4614
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2560, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3375
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2561, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1997
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2562, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2563, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2564, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2565, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2566, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2567, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2568, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2569, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2570, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1881
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2571, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.9226
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2572, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1328
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2573, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.1102
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2574, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2575, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.3900
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2576, num samples collected 5500, FPS 119
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2577, num samples collected 5500, FPS 118
  Algorithm: train_loss 0.1770
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2578, num samples collected 5500, FPS 118
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2579, num samples collected 5500, FPS 118
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
Update 2580, num samples collected 5500, FPS 118
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -602.7968, l 200.0000, t 121.3694, TestReward -712.1314
New EPOCH! 2580
Update 2581, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4733
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2582, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2583, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4468
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2584, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2585, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2586, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2587, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2444
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2588, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2589, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2590, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2591, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2592, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1392
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2593, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2594, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1828
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2595, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1433
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2596, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2597, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2598, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2599, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2600, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2601, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2602, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0939
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2603, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2603
Update 2604, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2605, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2606, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2607, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.5899
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2608, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3786
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2609, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3572
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2610, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2611, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2612, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1212
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2613, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3565
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2614, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2615, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2616, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2617, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2618, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2619, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2933
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2620, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.3454
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2621, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2622, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2472
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2623, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2624, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2625, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2626, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1341
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2626
Update 2627, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.4374
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2628, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2629, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2630, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1967
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2631, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1527
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2632, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2633, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2634, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1913
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2635, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2636, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2637, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2638, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.0606
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2639, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.1379
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2640, num samples collected 5750, FPS 91
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2641, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1440
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2642, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2643, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2644, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2298
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2645, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2646, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5567
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2647, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2648, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1100
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2649, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4747
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2649
Update 2650, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2651, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4013
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2652, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2624
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2653, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2654, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2655, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2656, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2657, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0843
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2658, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3423
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2659, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4577
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2660, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2661, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2223
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2662, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1112
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2663, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4160
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2664, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2665, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2666, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2667, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2668, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2957
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2669, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3968
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2670, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2671, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2672, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2672
Update 2673, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2674, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2675, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2676, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1625
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2677, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0839
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2678, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2679, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2680, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2310
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2681, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2682, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3835
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2683, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.6014
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2684, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1434
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2685, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1467
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2686, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2687, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2688, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2689, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2690, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2691, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2188
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2692, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2912
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2693, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1810
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2694, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2695, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2695
Update 2696, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5212
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2697, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5473
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2698, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2658
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2699, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2700, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1412
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2701, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0859
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2702, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2679
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2703, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1550
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2704, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3648
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2705, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2706, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1646
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2707, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2708, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2709, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2710, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2711, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3573
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2712, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2006
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2713, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2714, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2715, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2716, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2717, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2718, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2718
Update 2719, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3316
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2720, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2721, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2172
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2722, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1708
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2723, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2724, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2725, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1233
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2726, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2727, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2728, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1351
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2729, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2730, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3136
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2731, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2732, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2180
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2733, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2734, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4150
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2735, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2736, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3422
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2737, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2738, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3134
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2739, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2740, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2741, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2741
Update 2742, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1590
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2743, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2744, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2555
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2745, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3434
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2746, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2747, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3014
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2748, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2749, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0553
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2750, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3548
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2751, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2416
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2752, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0662
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2753, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3576
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2754, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2755, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2756, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2757, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2758, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2759, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1806
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2760, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2761, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0263
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2762, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4033
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2763, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2122
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2764, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2764
Update 2765, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4527
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2766, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2767, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2654
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2768, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1183
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2769, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2770, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1445
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2771, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2772, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1354
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2773, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2774, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2775, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2776, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2777, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1401
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2778, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2998
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2779, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.4114
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2780, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2781, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2782, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2783, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2784, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2785, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2402
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2786, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1413
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2787, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3818
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2787
Update 2788, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2789, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.3399
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2790, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1455
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2791, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2792, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2793, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2794, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2795, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2796, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2797, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5593
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2798, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2799, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2800, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2801, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.1838
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2802, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2803, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2804, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2805, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2385
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2806, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2807, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.5845
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2808, num samples collected 5750, FPS 90
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2809, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2810, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2810
Update 2811, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2812, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2343
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2813, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2504
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2814, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1374
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2815, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3397
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2816, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3023
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2817, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2818, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2819, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1710
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2820, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2821, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1835
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2822, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1877
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2823, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2824, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3898
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2825, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2826, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2827, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2828, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0809
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2829, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2830, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2831, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2832, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3480
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2833, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5700
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2833
Update 2834, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3629
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2835, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2919
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2836, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2882
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2837, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4641
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2838, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1064
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2839, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2840, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2841, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2842, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2969
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2843, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2844, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3212
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2845, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2846, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1381
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2847, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2291
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2848, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2849, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2850, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2851, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2852, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1409
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2853, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4307
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2854, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2855, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2856, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2856
Update 2857, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3726
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2858, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1167
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2859, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2860, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3291
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2861, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0773
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2862, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2863, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5128
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2864, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2865, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2866, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2867, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2868, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2869, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2870, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2871, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2224
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2872, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3482
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2873, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2874, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2875, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3968
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2876, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2678
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2877, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2878, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2879, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2879
Update 2880, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1394
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2881, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2608
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2882, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2883, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2137
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2884, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2885, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2886, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3165
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2887, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4229
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2888, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2889, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3768
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2890, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2891, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2892, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2893, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5348
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2894, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2895, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2896, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2897, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2898, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2899, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2900, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2520
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2901, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2902, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2902
Update 2903, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2904, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2905, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2906, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2907, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2908, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2909, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2910, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2911, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4032
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2912, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2222
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2913, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2914, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4447
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2915, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2916, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2917, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2918, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.5717
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2919, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1428
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2920, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4818
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2921, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4027
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2922, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2113
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2923, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2924, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2925, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2911
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2925
Update 2926, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2927, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3304
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2928, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3782
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2929, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2930, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3283
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2931, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2932, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2933, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3045
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2934, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2935, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2936, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2563
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2937, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2938, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2939, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2940, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2941, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2942, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2943, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4231
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2944, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2945, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2932
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2946, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2947, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.3132
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2948, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2948
Update 2949, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2175
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2950, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2469
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2951, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1345
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2952, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1017
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2953, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2008
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2954, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2955, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2956, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1404
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2957, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2778
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2958, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2959, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2960, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1541
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2961, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.4318
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2962, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2963, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2964, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2965, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.6358
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2966, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2967, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2968, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2969, num samples collected 5750, FPS 89
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2970, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2971, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2971
Update 2972, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2217
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2973, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2974, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3116
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2975, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2976, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2977, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0311
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2978, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1006
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2979, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5441
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2980, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3482
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2981, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2982, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2983, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3469
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2984, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2985, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2986, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2987, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5032
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2988, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4230
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2989, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2990, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3594
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2991, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2992, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2993, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2994, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 2994
Update 2995, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2996, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2997, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2998, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2201
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 2999, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3339
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3000, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3001, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4052
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3002, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2718
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3003, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3004, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3005, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3006, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2926
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3007, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3008, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3009, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3010, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3011, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3012, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2563
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3013, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2482
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3014, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0841
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3015, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3016, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3809
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3017, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.9687
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3017
Update 3018, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3019, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.5198
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3020, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3999
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3021, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0874
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3022, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1623
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3023, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3024, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3025, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2826
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3026, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3027, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3028, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3634
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3029, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3030, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3031, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4022
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3032, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1429
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3033, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3034, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3035, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2236
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3036, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0975
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3037, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3038, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3039, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1207
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3040, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3040
Update 3041, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3042, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3043, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3044, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3045, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3046, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3811
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3047, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1561
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3048, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3502
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3049, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1176
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3050, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2486
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3051, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3052, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2128
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3053, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3054, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1146
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3055, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3056, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3057, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2292
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3058, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3059, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0647
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3060, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4558
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3061, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3062, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2280
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3063, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3063
Update 3064, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3065, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2567
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3066, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4536
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3067, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3068, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3069, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1173
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3070, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3071, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3072, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3073, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0644
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3074, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3075, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2710
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3076, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4138
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3077, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3078, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3079, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3941
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3080, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3081, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1808
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3082, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3083, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3084, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3085, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1856
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3086, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3086
Update 3087, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3088, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3089, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3090, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2175
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3091, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0978
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3092, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3093, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3094, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3190
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3095, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3430
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3096, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2966
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3097, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3098, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2155
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3099, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1463
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3100, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3259
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3101, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1751
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3102, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4469
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3103, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3104, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3105, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3106, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3107, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3108, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3109, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.7319
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3109
Update 3110, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3111, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3112, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4713
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3113, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1417
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3114, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3709
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3115, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3116, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3117, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3118, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3119, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3120, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3121, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.4712
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3122, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3123, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3124, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3125, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3126, num samples collected 5750, FPS 88
  Algorithm: train_loss 0.1094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3127, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3128, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3308
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3129, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3130, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3131, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3132, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2155
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3132
Update 3133, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3134, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2903
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3135, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3477
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3136, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2838
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3137, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3138, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2561
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3139, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3140, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4495
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3141, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3142, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1412
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3143, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3144, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3145, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3146, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1107
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3147, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3148, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3149, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3150, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3183
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3151, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1000
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3152, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3153, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3154, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3155, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4029
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3155
Update 3156, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3157, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0974
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3158, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3159, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3160, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3161, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3162, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3163, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3164, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4746
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3165, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3166, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3167, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3168, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3169, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3170, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4465
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3171, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3172, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4863
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3173, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3225
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3174, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3175, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3176, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1583
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3177, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3178, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2942
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3178
Update 3179, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3180, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3181, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5148
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3182, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3183, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4801
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3184, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3185, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3186, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2226
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3187, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4197
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3188, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0770
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3189, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3190, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2700
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3191, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3396
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3192, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3193, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3194, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3195, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2031
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3196, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3197, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3198, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3710
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3199, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3200, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3201, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3201
Update 3202, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3203, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2158
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3204, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3205, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3206, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5103
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3207, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3208, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1895
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3209, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3210, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3211, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3212, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3213, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3214, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1175
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3215, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3216, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1128
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3217, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3100
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3218, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6863
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3219, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2137
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3220, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3221, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3222, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3223, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2222
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3224, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3224
Update 3225, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2391
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3226, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3227, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3228, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3229, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3230, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3231, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3232, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2348
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3233, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4515
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3234, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2607
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3235, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2710
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3236, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4636
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3237, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3238, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3239, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3240, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3241, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3242, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.6825
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3243, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2511
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3244, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3245, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3246, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3247, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3247
Update 3248, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3249, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2622
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3250, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1085
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3251, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3252, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2195
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3253, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3254, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2126
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3255, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3256, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3257, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3258, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3259, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3260, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3261, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3262, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1191
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3263, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1933
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3264, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3322
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3265, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4342
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3266, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3267, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3268, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3924
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3269, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3859
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3270, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2926
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3270
Update 3271, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3272, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2172
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3273, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3274, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3624
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3275, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2588
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3276, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1740
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3277, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3278, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3279, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2421
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3280, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3281, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1167
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3282, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3283, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1840
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3284, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3285, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3286, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3287, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3270
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3288, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3289, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.4332
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3290, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1806
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3291, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3292, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3293, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0978
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3293
Update 3294, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.5421
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3295, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.3907
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3296, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3297, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3298, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3299, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3300, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3301, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3302, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3303, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3304, num samples collected 5750, FPS 87
  Algorithm: train_loss 0.1401
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3305, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3648
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3306, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3307, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2130
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3308, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3309, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2770
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3310, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3311, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1368
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3312, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3313, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3314, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4458
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3315, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.6013
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3316, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3316
Update 3317, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3149
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3318, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3319, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1364
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3320, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1788
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3321, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3322, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3323, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3324, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5751
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3325, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3326, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2328
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3327, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3328, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3329, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1667
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3330, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3331, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3331
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3332, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4825
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3333, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3334, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3335, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2447
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3336, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3337, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3338, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3339, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3339
Update 3340, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3341, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3342, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.6634
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3343, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3344, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3345, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3346, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3347, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3348, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3349, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3350, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2128
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3351, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3352, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2535
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3353, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1536
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3354, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3355, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3356, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3357, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2220
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3358, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3359, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3360, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0953
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3361, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3362, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2410
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3362
Update 3363, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3364, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3365, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3366, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3367, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3368, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3369, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3370, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3371, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3372, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4045
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3373, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3374, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.5652
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3375, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3376, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3377, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0692
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3378, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4019
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3379, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3380, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3381, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3382, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3383, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3384, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3385, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.7122
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3385
Update 3386, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3387, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3294
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3388, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3389, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3390, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3391, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3392, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2504
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3393, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3394, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1160
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3395, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3396, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3833
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3397, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3398, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3399, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3400, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1465
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3401, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3402, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4297
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3403, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1755
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3404, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3405, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3181
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3406, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2890
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3407, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3408, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3408
Update 3409, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3410, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4943
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3411, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3412, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3413, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3414, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4519
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3415, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2608
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3416, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3417, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3418, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3950
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3419, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3420, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3421, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3422, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3423, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3424, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3425, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3351
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3426, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1842
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3427, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3428, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1200
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3429, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3430, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3431, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3431
Update 3432, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3433, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3434, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3435, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3436, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3437, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2128
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3438, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3439, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4363
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3440, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3441, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3875
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3442, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4602
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3443, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3444, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1749
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3445, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3446, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3447, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3448, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3449, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3530
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3450, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.4266
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3451, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3452, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.3613
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3453, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3454, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3454
Update 3455, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3456, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3457, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2164
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3458, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0256
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3459, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2879
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3460, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3461, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3462, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0287
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3463, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.7553
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3464, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3465, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2126
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3466, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.8472
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3467, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3468, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3469, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2751
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3470, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3471, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3472, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.2766
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3473, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3474, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3475, num samples collected 5750, FPS 86
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3476, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3477, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3477
Update 3478, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2009
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3479, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2769
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3480, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3481, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3482, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3483, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5345
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3484, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3485, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3486, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3487, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3488, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3489, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3490, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3491, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4424
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3492, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2443
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3493, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3494, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3495, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3496, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2963
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3497, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3498, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3623
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3499, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3500, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4577
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3500
Update 3501, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3502, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1160
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3503, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3504, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3505, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3514
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3506, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2643
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3507, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3508, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3509, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1491
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3510, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.6582
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3511, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3001
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3512, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3513, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3494
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3514, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3515, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3516, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0740
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3517, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3518, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2552
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3519, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3520, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2212
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3521, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3522, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3523, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3523
Update 3524, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3525, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1092
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3526, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3527, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1386
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3528, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3241
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3529, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4770
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3530, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1858
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3531, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3532, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3533, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5312
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3534, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2437
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3535, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1967
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3536, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3537, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3538, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2228
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3539, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3540, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3541, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2155
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3542, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3543, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0944
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3544, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3545, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3546, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2971
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3546
Update 3547, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4362
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3548, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3549, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4671
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3550, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3551, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0877
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3552, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3553, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3554, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3555, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3556, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3557, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3558, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1867
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3559, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3560, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0770
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3561, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3562, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3563, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1176
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3564, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3565, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3338
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3566, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3567, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3568, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2706
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3569, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4516
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3569
Update 3570, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1098
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3571, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3572, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2032
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3573, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3574, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3575, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3576, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3577, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0283
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3578, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.6028
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3579, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4764
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3580, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3581, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3582, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0881
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3583, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3584, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4577
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3585, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3586, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2969
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3587, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3588, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2182
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3589, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3590, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3591, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3592, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3592
Update 3593, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3594, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3595, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3905
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3596, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0456
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3597, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3598, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.7841
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3599, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3514
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3600, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3601, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3602, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3603, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3604, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3605, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3606, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3607, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.4488
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3608, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3609, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3610, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0951
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3611, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2675
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3612, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3613, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3614, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3615, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3615
Update 3616, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3860
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3617, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3844
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3618, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3619, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2158
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3620, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3621, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2119
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3622, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3623, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3624, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2122
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3625, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3626, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3627, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3628, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3629, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3630, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3337
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3631, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1006
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3632, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0864
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3633, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1737
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3634, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3635, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1297
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3636, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5483
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3637, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3489
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3638, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3638
Update 3639, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3640, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.5692
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3641, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3642, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1379
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3643, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3644, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3645, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.7287
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3646, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3647, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.2192
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3648, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3649, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3573
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3650, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3651, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3652, num samples collected 5750, FPS 85
  Algorithm: train_loss 0.3222
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3653, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3654, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2555
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3655, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2855
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3656, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3657, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3658, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3659, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3660, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3661, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3661
Update 3662, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3663, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3664, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2052
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3665, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3666, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3667, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1561
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3668, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.6605
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3669, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3670, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3671, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1443
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3672, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3673, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3674, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3675, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3676, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3677, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3678, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3679, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3680, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3681, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3682, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3683, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3684, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2980
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3684
Update 3685, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0964
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3686, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3687, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3688, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3689, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3690, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3691, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3246
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3692, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4917
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3693, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3694, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3695, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3696, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3697, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3698, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3699, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1696
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3700, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3520
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3701, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3702, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1335
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3703, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4408
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3704, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3705, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3706, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2320
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3707, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4442
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3707
Update 3708, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2447
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3709, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3710, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2428
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3711, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3712, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3713, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2561
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3714, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3715, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3716, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3717, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2468
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3718, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3719, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3720, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3721, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3722, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2575
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3723, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2851
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3724, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3180
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3725, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3726, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5950
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3727, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3728, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3729, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3730, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3730
Update 3731, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3732, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3256
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3733, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2048
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3734, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2224
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3735, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1209
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3736, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3737, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4523
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3738, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1845
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3739, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2813
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3740, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3741, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3742, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3743, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0265
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3744, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3745, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0851
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3746, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3747, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3748, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3749, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3750, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3751, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3580
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3752, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3753, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3753
Update 3754, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1369
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3755, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3756, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3419
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3757, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1849
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3758, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3759, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5401
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3760, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3761, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3762, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2298
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3763, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2486
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3764, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3692
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3765, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3513
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3766, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3767, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3768, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2490
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3769, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3770, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3771, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3772, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3773, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3774, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3775, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3776, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2176
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3776
Update 3777, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3778, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3779, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2845
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3780, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3781, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3782, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3303
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3783, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3784, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3785, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3786, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3787, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3788, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.4929
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3789, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3790, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3735
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3791, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3792, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3793, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3794, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3795, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2187
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3796, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3797, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3395
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3798, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2593
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3799, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3799
Update 3800, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3801, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3802, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3196
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3803, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2882
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3804, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3805, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2484
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3806, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.3920
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3807, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3808, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3809, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3810, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3811, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3812, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.5602
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3813, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3814, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3815, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2333
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3816, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3817, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3818, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3819, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1496
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3820, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3821, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.1300
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3822, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3822
Update 3823, num samples collected 5750, FPS 84
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3824, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1086
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3825, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3826, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3827, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3828, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3829, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3204
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3830, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3831, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3832, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2367
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3833, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2444
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3834, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3835, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1610
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3836, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3837, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3838, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0894
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3839, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1748
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3840, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2189
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3841, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.5424
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3842, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4188
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3843, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3844, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2535
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3845, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3845
Update 3846, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3847, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3848, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1858
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3849, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3464
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3850, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3050
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3851, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3852, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3853, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3854, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1269
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3855, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3856, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2573
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3857, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3996
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3858, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4669
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3859, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3860, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1869
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3861, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3862, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0730
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3863, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0894
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3864, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3474
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3865, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3866, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3867, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2316
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3868, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3868
Update 3869, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3870, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3871, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3872, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2176
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3873, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3874, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3875, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3876, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3636
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3877, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1453
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3878, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3879, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3030
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3880, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4612
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3881, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3882, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2920
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3883, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3884, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3885, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2211
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3886, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2162
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3887, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3888, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1110
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3889, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3890, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3891, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1808
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3891
Update 3892, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3893, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3894, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3895, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3896, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2728
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3897, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3898, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3485
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3899, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3286
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3900, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3901, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3877
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3902, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3903, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3678
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3904, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4408
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3905, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0829
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3906, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1725
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3907, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0351
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3908, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2998
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3909, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2604
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3910, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3911, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3912, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3913, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3914, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3914
Update 3915, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3734
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3916, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3917, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3417
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3918, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3919, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3920, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1465
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3921, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3922, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1346
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3923, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0811
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3924, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.4778
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3925, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2907
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3926, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3927, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1116
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3928, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3929, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3930, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2667
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3931, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3932, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3933, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3870
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3934, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3935, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3936, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1889
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3937, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3937
Finished MB training, ran for 60 epochs
Update 3938, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3939, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3493
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3940, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3941, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3942, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3943, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1135
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3944, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3945, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2181
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3946, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2503
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3947, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1400
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3948, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2867
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3949, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3950, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2851
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3951, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3952, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3953, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3954, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3955, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3512
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3956, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0898
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3957, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.1758
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3958, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.3414
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3959, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
Update 3960, num samples collected 5750, FPS 83
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -497.6759, l 200.0000, t 144.4974, TestReward -82.8823
New EPOCH! 3960
Update 3961, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1501
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3962, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3963, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3964, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3965, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3698
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3966, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1509
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3967, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3968, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.6247
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3969, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1782
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3970, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1299
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3971, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2211
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3972, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2447
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3973, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3974, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1176
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3975, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2859
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3976, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3977, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3978, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3979, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3980, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2316
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3981, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3982, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3983, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3984, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3255
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 3984
Update 3985, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3986, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3987, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3988, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3989, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4447
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3990, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3991, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.5790
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3992, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3993, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3994, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3890
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3995, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3996, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3997, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3998, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1550
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 3999, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2746
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4000, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1162
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4001, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1907
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4002, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4003, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4004, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1338
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4005, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2755
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4006, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1443
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4007, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4008, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4008
Update 4009, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4010, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4011, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4012, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4013, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4014, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4769
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4015, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4016, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4017, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3406
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4018, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2287
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4019, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4020, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3013
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4021, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2877
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4022, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4023, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1910
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4024, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4025, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4026, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4027, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3777
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4028, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2578
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4029, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2326
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4030, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4031, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4032, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4032
Update 4033, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4034, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4035, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2445
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4036, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4037, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2384
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4038, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4039, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1901
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4040, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4041, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4042, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4043, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2685
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4044, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4045, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2251
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4046, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4047, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3610
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4048, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4778
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4049, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4050, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4051, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3863
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4052, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4053, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4054, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1590
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4055, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1291
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4056, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.7018
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4056
Update 4057, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4058, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4059, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4060, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4061, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4062, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3525
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4063, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4064, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4065, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2372
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4066, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4067, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4068, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4069, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4905
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4070, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3855
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4071, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4072, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4073, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4074, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2788
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4075, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4076, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4025
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4077, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0593
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4078, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2147
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4079, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3426
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4080, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.6104
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4080
Update 4081, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1893
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4082, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4083, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1216
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4084, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3646
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4085, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1848
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4086, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1212
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4087, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4088, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1228
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4089, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4424
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4090, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3570
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4091, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4092, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4093, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4094, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3112
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4095, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4096, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3797
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4097, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3956
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4098, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4099, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2239
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4100, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4101, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4102, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1330
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4103, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4104, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1855
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4104
Update 4105, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4694
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4106, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4107, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4108, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2315
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4109, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4110, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4111, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.4643
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4112, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4113, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4114, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4115, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1611
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4116, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4117, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4118, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4119, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.1259
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4120, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.3865
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4121, num samples collected 6000, FPS 69
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4122, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0791
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4123, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4124, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1457
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4125, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4126, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2150
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4127, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4128, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4128
Update 4129, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3349
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4130, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4131, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3390
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4132, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0995
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4133, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0354
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4134, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4135, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4839
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4136, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1804
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4137, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4138, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2235
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4139, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4140, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4141, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4142, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4143, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2381
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4144, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4145, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4146, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4147, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4148, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4149, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4971
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4150, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4151, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4152, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5646
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4152
Update 4153, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4154, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4155, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4156, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1400
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4157, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4158, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4159, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4160, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4161, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2359
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4162, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4714
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4163, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4164, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4165, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4166, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4167, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4168, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1210
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4169, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4170, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.7688
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4171, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3330
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4172, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2468
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4173, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2594
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4174, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4175, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4176, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4176
Update 4177, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4178, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4179, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4180, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4181, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4182, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3157
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4183, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1907
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4184, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0849
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4185, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5512
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4186, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4187, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4188, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4189, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4190, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1540
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4191, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3306
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4192, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4193, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4194, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4195, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4196, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4197, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4497
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4198, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4199, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6839
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4200, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4200
Update 4201, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4202, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0495
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4203, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4204, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4205, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5840
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4206, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2201
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4207, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4208, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2574
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4209, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2724
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4210, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4211, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4212, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4213, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4214, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4215, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4216, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4217, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4218, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4219, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4220, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4221, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4222, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4392
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4223, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1944
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4224, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5158
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4224
Update 4225, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4226, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.6885
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4227, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2516
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4228, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4229, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4230, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4231, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4232, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4233, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0606
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4234, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1513
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4235, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5995
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4236, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4237, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4238, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4239, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4240, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4241, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3516
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4242, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4243, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1554
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4244, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4245, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4246, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4247, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0753
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4248, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4248
Update 4249, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4250, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4251, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2223
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4252, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5325
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4253, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4254, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1792
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4255, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4256, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4257, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1054
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4258, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4259, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4260, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3729
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4261, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3657
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4262, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4263, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2300
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4264, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4265, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0852
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4266, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1381
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4267, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1484
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4268, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4269, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4270, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.5600
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4271, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4272, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4272
Update 4273, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2009
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4274, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2099
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4275, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2559
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4276, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3138
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4277, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4278, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4279, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4280, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4281, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4282, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4283, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4284, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4285, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4286, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.2172
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4287, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.3604
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4288, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4452
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4289, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4290, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4291, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4292, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0797
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4293, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4294, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4295, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.4752
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4296, num samples collected 6000, FPS 68
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4296
Update 4297, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4298, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1820
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4299, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2677
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4300, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4301, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4302, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3976
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4303, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2223
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4304, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4305, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4835
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4306, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4307, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1280
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4308, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2535
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4309, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3584
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4310, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4311, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4312, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1984
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4313, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4314, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4315, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4316, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4317, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4318, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3658
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4319, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4320, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4320
Update 4321, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4322, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4323, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4324, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4325, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4326, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4327, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4328, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4329, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4330, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1892
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4331, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1250
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4332, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4333, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4334, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4335, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5471
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4336, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4337, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4338, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4025
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4339, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4340, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3563
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4341, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3646
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4342, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4343, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4344, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5860
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4344
Update 4345, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2484
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4346, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0793
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4347, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4348, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2308
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4349, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4350, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3905
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4351, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4352, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4353, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4354, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4355, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4356, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1759
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4357, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4358, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4359, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.5774
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4360, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4361, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1194
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4362, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2801
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4363, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4364, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2248
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4365, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4366, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4367, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4368, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4368
Update 4369, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4370, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4371, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0915
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4372, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4373, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4162
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4374, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4375, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1731
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4376, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4377, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0481
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4378, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4379, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4380, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4381, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.7837
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4382, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6582
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4383, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4384, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4385, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4386, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4387, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1889
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4388, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2096
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4389, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4390, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2579
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4391, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4392, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4392
Update 4393, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0464
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4394, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1451
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4395, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4396, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4397, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2170
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4398, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4399, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1799
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4400, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3368
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4401, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4402, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4403, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2050
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4404, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6459
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4405, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4406, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2834
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4407, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2372
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4408, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4409, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3553
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4410, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4411, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4412, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2624
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4413, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4414, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4415, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0685
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4416, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4416
Update 4417, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2785
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4418, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4419, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1152
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4420, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4421, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4422, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0973
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4423, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4424, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2044
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4425, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4426, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4427, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4428, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4429, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4980
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4430, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4431, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.6220
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4432, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.4238
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4433, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4434, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4435, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4436, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1810
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4437, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.2348
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4438, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1405
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4439, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.3690
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4440, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4440
Update 4441, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4442, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1820
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4443, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4444, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4445, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4446, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4447, num samples collected 6000, FPS 67
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4448, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4449, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4450, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4451, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.5819
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4452, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4453, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.5843
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4454, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3183
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4455, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2121
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4456, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3957
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4457, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4458, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2900
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4459, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0879
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4460, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4461, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1244
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4462, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4463, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4464, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4464
Update 4465, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4466, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4467, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4468, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.3208
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4469, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2181
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4470, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4471, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4472, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4473, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2795
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4474, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.1797
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4475, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.2313
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4476, num samples collected 6000, FPS 66
  Algorithm: train_loss 0.4480
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4477, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4478, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4479, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4480, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0768
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4481, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3133
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4482, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4483, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5759
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4484, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4485, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4486, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4487, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4488, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2552
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4488
Update 4489, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4490, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4491, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4492, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0668
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4493, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1148
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4494, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1505
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4495, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4496, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4497, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3611
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4498, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4499, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1633
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4500, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4501, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4502, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4503, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.4119
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4504, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.6686
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4505, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4506, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4507, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4508, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4509, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2777
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4510, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4511, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1130
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4512, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4512
Update 4513, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4514, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4515, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4516, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0706
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4517, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1178
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4518, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0780
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4519, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2573
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4520, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.5156
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4521, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1127
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4522, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2757
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4523, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4524, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.2303
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4525, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3173
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4526, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4527, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.3195
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4528, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.1799
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4529, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4530, num samples collected 6000, FPS 65
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4531, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2697
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4532, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4533, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4534, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4535, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4536, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4536
Update 4537, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4538, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4539, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4540, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4541, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4542, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4543, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4544, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4545, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4546, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1767
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4547, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4299
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4548, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4549, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4209
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4550, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4551, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4552, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4553, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4554, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4555, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1254
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4556, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2625
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4557, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4558, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4559, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4560, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1207
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4560
Update 4561, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2836
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4562, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4563, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4564, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4565, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4566, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4567, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1875
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4568, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2569
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4569, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4570, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2218
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4571, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4008
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4572, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1310
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4573, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4574, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4575, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0479
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4576, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4577, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4578, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4579, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1705
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4580, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1239
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4581, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4582, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4583, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3246
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4584, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4584
Update 4585, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4586, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4587, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2071
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4588, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2132
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4589, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2227
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4590, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4591, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0307
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4592, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4593, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4594, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2584
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4595, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0897
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4596, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4597, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3141
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4598, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4599, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0640
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4600, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.7198
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4601, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4602, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4603, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4604, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4605, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1952
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4606, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4607, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4608, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4608
Update 4609, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4610, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4611, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4612, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4613, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4614, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4615, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4616, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4617, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3718
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4618, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4619, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2234
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4620, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4621, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4622, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4623, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4584
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4624, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3822
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4625, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2848
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4626, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0776
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4627, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4174
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4628, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3892
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4629, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4630, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4631, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4632, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4632
Update 4633, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3262
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4634, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4635, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4636, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3497
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4637, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4638, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4639, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4640, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3249
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4641, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4642, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2475
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4643, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4644, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2846
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4645, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4646, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4647, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4648, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2669
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4649, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1417
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4650, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3294
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4651, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4652, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4653, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4193
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4654, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2697
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4655, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4656, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4656
Update 4657, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4658, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1592
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4659, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2245
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4660, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4661, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3239
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4662, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4663, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4664, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.5484
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4665, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4666, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2633
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4667, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2687
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4668, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4669, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4670, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4671, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4672, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4673, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1466
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4674, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1041
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4675, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3574
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4676, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4677, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4911
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4678, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4679, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4680, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4680
Update 4681, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4682, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4683, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4684, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3932
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4685, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4686, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4687, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4688, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4689, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2701
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4690, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4691, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2193
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4692, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2388
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4693, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4694, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4695, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4696, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4697, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3993
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4698, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4699, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4700, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4701, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.5403
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4702, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4703, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4704, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3270
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4704
Update 4705, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4706, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4707, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1047
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4708, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4709, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4710, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4801
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4711, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.6028
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4712, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4713, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4714, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4715, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4506
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4716, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4717, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1971
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4718, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4719, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4720, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4721, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4722, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4723, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4724, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4725, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4726, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2803
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4727, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4728, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1404
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4728
Update 4729, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2760
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4730, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2843
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4731, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4732, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1941
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4733, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.6439
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4734, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4735, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4736, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4737, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4738, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.5426
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4739, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4740, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4741, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4742, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4743, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4744, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2942
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4745, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4746, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2368
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4747, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4748, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4749, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4750, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4751, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4752, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4752
Update 4753, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4754, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4755, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4756, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4757, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4758, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4759, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4760, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3190
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4761, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4762, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4763, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4764, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4765, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4766, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1413
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4767, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4768, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2456
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4769, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4770, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4771, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4138
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4772, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4773, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4905
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4774, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4775, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4776, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.7892
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4776
Update 4777, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4778, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4779, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4780, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4781, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4782, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4783, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1239
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4784, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2842
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4785, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3941
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4786, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3678
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4787, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4788, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1197
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4789, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4790, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3134
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4791, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4792, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4793, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4794, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1146
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4795, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0609
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4796, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4797, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4798, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4799, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3450
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4800, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4800
Update 4801, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4802, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0276
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4803, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4804, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4805, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.5660
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4806, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4807, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.3695
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4808, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1445
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4809, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2770
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4810, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4811, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4812, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1899
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4813, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2139
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4814, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4815, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4816, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4817, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4818, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4782
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4819, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4820, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4821, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4822, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4823, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4824, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4824
Update 4825, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4826, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.4759
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4827, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.1123
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4828, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.2481
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4829, num samples collected 6000, FPS 64
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4830, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4831, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4832, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4833, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4834, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4835, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4836, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1891
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4837, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4838, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4839, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.7712
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4840, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2608
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4841, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4842, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1518
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4843, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4844, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2628
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4845, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3868
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4846, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2434
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4847, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4848, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0673
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4848
Update 4849, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2508
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4850, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4851, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4852, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.5453
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4853, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4854, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3285
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4855, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4856, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4857, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2438
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4858, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4859, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4860, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4861, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4862, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1443
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4863, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4864, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2552
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4865, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4866, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4867, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4868, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4869, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4870, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4871, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4872, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4378
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4872
Update 4873, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4874, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1467
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4875, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4876, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4877, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4878, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4879, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2188
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4880, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4881, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4882, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4883, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2561
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4884, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4473
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4885, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3934
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4886, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4887, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4888, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4027
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4889, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2846
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4890, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0739
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4891, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4892, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4893, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4894, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1512
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4895, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2122
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4896, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4896
Update 4897, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4898, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4899, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4900, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3926
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4901, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0672
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4902, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4250
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4903, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.5744
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4904, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4905, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4906, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4907, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4908, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2875
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4909, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4910, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4911, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4912, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1008
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4913, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0297
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4914, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4915, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4916, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4917, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.6345
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4918, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4919, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4920, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4920
Update 4921, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4922, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4923, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4924, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4925, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4926, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4927, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4928, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4929, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2490
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4930, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4931, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4932, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4933, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4934, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4935, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4714
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4936, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4937, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4938, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4939, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4940, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4941, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4942, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3105
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4943, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2452
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4944, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4657
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4944
Update 4945, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4946, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4947, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4948, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4949, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4950, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2964
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4951, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4040
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4952, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4953, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3077
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4954, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4955, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4956, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2656
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4957, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2462
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4958, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4167
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4959, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4960, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4961, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4962, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4963, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4964, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2822
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4965, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0692
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4966, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2721
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4967, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2288
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4968, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4968
Update 4969, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4970, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4534
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4971, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1522
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4972, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4973, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3314
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4974, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4975, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4976, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2698
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4977, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1394
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4978, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4979, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4980, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4981, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2089
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4982, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2225
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4983, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0677
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4984, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4985, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3610
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4986, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4987, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4988, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4989, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4990, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4791
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4991, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4992, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1397
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 4992
Update 4993, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4466
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4994, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2879
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4995, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4996, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4997, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4998, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3524
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 4999, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1219
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5000, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5001, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5002, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5003, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0253
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5004, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5005, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5006, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1127
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5007, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5008, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5009, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5010, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4228
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5011, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2496
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5012, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2425
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5013, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5014, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4393
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5015, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0692
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5016, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5016
Update 5017, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3625
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5018, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1203
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5019, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5020, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5021, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5022, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3231
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5023, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3499
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5024, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5025, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5026, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5027, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5028, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2536
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5029, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5030, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5031, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5032, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2985
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5033, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5034, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5035, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.5295
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5036, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2942
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5037, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1259
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5038, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1490
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5039, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5040, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5040
Update 5041, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5042, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1483
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5043, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.6412
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5044, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5045, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5046, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5047, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2881
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5048, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2051
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5049, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5050, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5051, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2482
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5052, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0433
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5053, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5054, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5055, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5056, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5057, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5058, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2527
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5059, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2537
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5060, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4270
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5061, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4272
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5062, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5063, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5064, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5064
Update 5065, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5066, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5067, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5068, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5069, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3556
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5070, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2411
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5071, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5072, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5073, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3735
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5074, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5075, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5076, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5077, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2545
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5078, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3726
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5079, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5080, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5081, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2632
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5082, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5083, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1423
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5084, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5085, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4385
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5086, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2109
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5087, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5088, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5088
Update 5089, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5090, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5091, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2335
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5092, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2002
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5093, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5094, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5095, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4456
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5096, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5097, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1837
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5098, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5099, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2700
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5100, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4737
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5101, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0877
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5102, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2785
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5103, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2423
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5104, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5105, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5106, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5107, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5108, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0741
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5109, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5110, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5111, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5112, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3122
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5112
Update 5113, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5114, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4750
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5115, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3813
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5116, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5117, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5118, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5119, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5120, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5121, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5122, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5123, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5124, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5125, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5126, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5127, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5128, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4966
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5129, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5130, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5131, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5132, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2928
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5133, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.5645
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5134, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5135, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1491
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5136, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5136
Update 5137, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5138, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5139, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5140, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5141, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5142, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3748
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5143, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5144, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5145, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3778
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5146, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.5593
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5147, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5148, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.6035
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5149, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5150, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1344
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5151, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5152, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1599
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5153, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5154, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0691
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5155, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0694
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5156, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5157, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5158, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5159, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5160, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5160
Update 5161, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1755
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5162, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5163, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4746
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5164, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5165, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5166, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.2758
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5167, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.3490
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5168, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5169, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5170, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5171, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5172, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5173, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1466
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5174, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5175, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5176, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5177, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.4622
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5178, num samples collected 6000, FPS 63
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5179, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2310
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5180, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5181, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3137
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5182, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5183, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5184, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5184
Update 5185, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1922
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5186, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2761
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5187, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2496
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5188, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2220
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5189, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5190, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1128
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5191, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1419
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5192, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5193, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5194, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5195, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5196, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1507
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5197, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1055
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5198, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5199, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5200, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4201
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5201, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5202, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1406
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5203, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5204, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0733
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5205, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5206, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0975
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5207, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2806
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5208, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4930
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5208
Update 5209, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5210, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5211, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5212, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4122
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5213, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5214, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3764
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5215, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1397
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5216, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5217, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5218, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5219, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5220, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3615
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5221, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5222, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.5038
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5223, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5224, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2789
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5225, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0269
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5226, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5227, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2967
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5228, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5229, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1804
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5230, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5231, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5232, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.8319
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5232
Update 5233, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5234, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2232
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5235, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5236, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4308
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5237, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2646
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5238, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2641
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5239, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5240, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5241, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5242, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3566
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5243, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5244, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1922
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5245, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5246, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5247, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1610
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5248, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5249, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5250, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5251, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1489
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5252, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1516
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5253, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5254, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1180
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5255, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5256, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5256
Update 5257, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5258, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5259, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5260, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5261, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2920
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5262, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1510
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5263, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5264, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5265, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5266, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5267, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2102
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5268, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4569
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5269, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5270, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5271, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1809
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5272, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2462
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5273, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3296
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5274, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5275, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5276, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5277, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3257
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5278, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5279, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5280, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4978
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5280
Update 5281, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5282, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.6157
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5283, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5284, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.5453
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5285, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5286, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0319
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5287, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5288, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1608
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5289, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2307
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5290, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1954
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5291, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5292, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5293, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5294, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5295, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5296, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3446
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5297, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1417
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5298, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5299, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3982
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5300, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5301, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1724
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5302, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5303, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3691
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5304, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5304
Update 5305, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5306, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5307, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5308, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1020
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5309, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.9705
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5310, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1624
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5311, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5312, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5313, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5314, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5315, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5316, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3081
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5317, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0297
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5318, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5319, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5320, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2273
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5321, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5322, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5323, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4402
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5324, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5325, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5326, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.5506
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5327, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5328, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5328
Update 5329, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5330, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3626
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5331, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0938
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5332, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5333, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3301
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5334, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3971
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5335, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5336, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0706
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5337, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5338, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2297
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5339, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5340, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5341, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5342, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5343, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3538
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5344, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2167
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5345, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5346, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5347, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5348, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5349, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1662
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5350, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5351, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5352, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5352
Update 5353, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2765
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5354, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5355, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2478
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5356, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3503
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5357, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5358, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5359, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5360, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2772
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5361, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5362, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4333
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5363, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4826
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5364, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5365, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5366, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5367, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5368, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5369, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5370, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5371, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5372, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5373, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3838
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5374, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5375, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2213
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5376, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5376
Finished MB training, ran for 60 epochs
Update 5377, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5378, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5379, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0372
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5380, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.5103
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5381, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5382, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3388
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5383, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0968
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5384, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5385, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.3421
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5386, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5387, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2164
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5388, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5389, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5390, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4712
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5391, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5392, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.2721
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5393, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5394, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5395, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.4536
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5396, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5397, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5398, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5399, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
Update 5400, num samples collected 6000, FPS 62
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -128.0572, l 200.0000, t 168.2408, TestReward -283.0101
New EPOCH! 5400
Update 5401, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5402, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0970
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5403, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5404, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4285
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5405, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5406, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3951
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5407, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5408, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5409, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5410, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5411, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2366
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5412, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1568
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5413, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5414, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0730
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5415, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5416, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3178
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5417, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5418, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5419, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5420, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5421, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3357
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5422, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2409
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5423, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5424, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5425, num samples collected 6250, FPS 55
  Algorithm: train_loss 1.0251
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5425
Update 5426, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5427, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5428, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5429, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4309
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5430, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5431, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5432, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5433, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5434, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4489
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5435, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5436, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5437, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0717
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5438, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2508
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5439, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0963
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5440, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5441, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5442, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1176
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5443, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5444, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5445, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2143
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5446, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5447, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3172
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5448, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3594
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5449, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2238
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5450, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5450
Update 5451, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1107
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5452, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5453, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5454, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1500
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5455, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5456, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5457, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2562
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5458, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5459, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5460, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2213
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5461, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5462, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5463, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4271
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5464, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2556
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5465, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3568
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5466, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0341
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5467, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2210
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5468, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5469, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5470, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5471, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5472, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5473, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5474, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1723
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5475, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5475
Update 5476, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1255
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5477, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3130
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5478, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0810
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5479, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3262
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5480, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5481, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5482, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5483, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5889
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5484, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5485, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1151
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5486, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5487, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5488, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2551
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5489, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4211
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5490, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1034
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5491, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0770
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5492, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5493, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4856
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5494, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5495, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5496, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5497, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1163
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5498, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5499, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5500, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5500
Update 5501, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1105
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5502, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5503, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5504, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3472
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5505, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5506, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3214
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5507, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3141
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5508, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5509, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5510, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5511, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5512, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.5521
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5513, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5514, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1685
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5515, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5516, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2100
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5517, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1868
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5518, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2613
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5519, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2802
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5520, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0682
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5521, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3503
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5522, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5523, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5524, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5525, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5525
Update 5526, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4686
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5527, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5528, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2249
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5529, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0976
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5530, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5531, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1527
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5532, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5533, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5534, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5535, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2032
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5536, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5537, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4427
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5538, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5539, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5540, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5541, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5542, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3439
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5543, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5544, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5545, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1691
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5546, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1225
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5547, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5548, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5549, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.2551
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5550, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5550
Update 5551, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.4266
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5552, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5553, num samples collected 6250, FPS 55
  Algorithm: train_loss 0.3170
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5554, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5555, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0289
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5556, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5557, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5558, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5559, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2508
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5560, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4271
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5561, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5562, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5563, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5564, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0721
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5565, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2916
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5566, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5567, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5568, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5569, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5570, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5571, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1314
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5572, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5573, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3959
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5574, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2348
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5575, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4245
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5575
Update 5576, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5577, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5578, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0606
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5579, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1444
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5580, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5581, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5582, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5583, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4141
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5584, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5585, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3014
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5586, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5587, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2456
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5588, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2798
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5589, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2693
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5590, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3562
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5591, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5592, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5593, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1203
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5594, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5595, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5596, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5597, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1098
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5598, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5599, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2202
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5600, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5600
Update 5601, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0302
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5602, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4408
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5603, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5604, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5605, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5606, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5607, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2652
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5608, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3139
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5609, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2758
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5610, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0364
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5611, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0912
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5612, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5613, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2182
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5614, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5615, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5616, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5617, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2817
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5618, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5619, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4591
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5620, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4542
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5621, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5622, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0674
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5623, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5624, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5625, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5625
Update 5626, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3052
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5627, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5628, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5629, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3989
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5630, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3159
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5631, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5632, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5633, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3459
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5634, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2141
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5635, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5636, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2321
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5637, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5638, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2178
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5639, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5640, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5641, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5642, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5643, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.5485
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5644, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5645, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5646, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5647, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5648, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5649, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1461
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5650, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5650
Update 5651, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5652, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5653, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5654, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4713
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5655, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5656, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5657, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5658, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4176
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5659, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5660, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5661, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1875
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5662, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5663, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3391
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5664, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2443
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5665, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5666, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5667, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1248
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5668, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0735
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5669, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4552
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5670, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2205
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5671, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4842
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5672, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5673, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5674, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2192
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5675, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5675
Update 5676, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2270
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5677, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5678, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4525
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5679, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5680, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5681, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5682, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1386
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5683, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2151
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5684, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5685, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4046
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5686, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0901
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5687, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5688, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2947
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5689, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5690, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4442
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5691, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5692, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1242
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5693, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5694, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5695, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5696, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3736
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5697, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5698, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5699, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2574
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5700, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5700
Update 5701, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5702, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5703, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5704, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5705, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5706, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5707, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1846
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5708, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5709, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5710, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5711, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2875
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5712, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5713, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5714, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5715, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5716, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5717, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4807
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5718, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.4331
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5719, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5720, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.3036
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5721, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1743
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5722, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5723, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0835
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5724, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5725, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.6270
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5725
Update 5726, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5727, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5728, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5729, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5730, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.2954
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5731, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5732, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5733, num samples collected 6250, FPS 54
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5734, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5735, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4691
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5736, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5737, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5738, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6789
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5739, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0796
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5740, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5741, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2742
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5742, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5743, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2477
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5744, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1167
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5745, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2946
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5746, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5747, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1528
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5748, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5749, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5750, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5067
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5750
Update 5751, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5752, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2906
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5753, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3167
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5754, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1885
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5755, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5756, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2558
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5757, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4098
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5758, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1477
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5759, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5760, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2915
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5761, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5762, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2874
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5763, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2348
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5764, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5765, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5766, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5767, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5768, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2658
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5769, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5770, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5771, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5772, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5773, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5774, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5775, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5775
Update 5776, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5777, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4292
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5778, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2992
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5779, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2328
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5780, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5781, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5782, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2499
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5783, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1331
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5784, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5785, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5786, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5787, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5788, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5789, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5790, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2346
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5791, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5792, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5793, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5794, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2855
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5795, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3046
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5796, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2729
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5797, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2176
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5798, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2989
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5799, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5800, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4350
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5800
Update 5801, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5802, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5803, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4946
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5804, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5805, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5806, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5807, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1863
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5808, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2813
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5809, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2014
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5810, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5811, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5812, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5813, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5814, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5815, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5816, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5817, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5818, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6317
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5819, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5820, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4703
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5821, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5822, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5823, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0322
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5824, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5825, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5825
Update 5826, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2223
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5827, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4638
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5828, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5829, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5830, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2255
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5831, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5832, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5833, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5834, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5835, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5836, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5837, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5838, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5839, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5840, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4720
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5841, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5842, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5843, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5844, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5845, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4184
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5846, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5847, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2352
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5848, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5849, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5850, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5850
Update 5851, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5852, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2610
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5853, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5854, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2829
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5855, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1694
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5856, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2238
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5857, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5858, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5859, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5860, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2682
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5861, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2056
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5862, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5863, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5864, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5865, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5866, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5867, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5868, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5869, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4712
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5870, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5871, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5872, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2975
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5873, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4488
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5874, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5875, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5875
Update 5876, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5877, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5878, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2145
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5879, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5880, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5881, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1899
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5882, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5883, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5884, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5885, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5886, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5887, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4199
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5888, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1252
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5889, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3734
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5890, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5891, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5964
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5892, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5893, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1764
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5894, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5895, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5896, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2573
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5897, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5898, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5899, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5900, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5900
Update 5901, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0264
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5902, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5903, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5904, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5460
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5905, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2438
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5906, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2340
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5907, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5908, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2157
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5909, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5910, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5911, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5912, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5913, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0330
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5914, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2727
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5915, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6108
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5916, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5917, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5918, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5919, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5920, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1059
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5921, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5922, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3371
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5923, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5924, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5925, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3681
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5925
Update 5926, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3145
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5927, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5928, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5929, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3666
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5930, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5931, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5932, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0219
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5933, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5934, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5935, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5936, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5937, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4556
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5938, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5939, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3350
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5940, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5941, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5942, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5943, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5944, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5945, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0702
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5946, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2735
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5947, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0179
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5948, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5949, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5950, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5950
Update 5951, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5952, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2722
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5953, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0844
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5954, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5955, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3503
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5956, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5957, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5958, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5959, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3645
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5960, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5961, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5962, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5963, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5964, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2650
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5965, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5966, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5967, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5968, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5969, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3409
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5970, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2563
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5971, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1495
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5972, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5973, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5974, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0664
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5975, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3406
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 5975
Update 5976, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4162
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5977, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5291
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5978, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5979, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5980, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5981, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5982, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5983, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5984, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5985, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5986, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2331
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5987, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5988, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5801
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5989, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3986
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5990, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3036
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5991, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2675
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5992, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5993, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2328
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5994, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5995, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5996, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5997, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0829
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5998, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2046
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 5999, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6000, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1304
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6000
Update 6001, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6002, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4892
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6003, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6004, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1667
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6005, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6006, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6007, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.5241
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6008, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6009, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6010, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6011, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0606
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6012, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2552
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6013, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6014, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1275
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6015, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6016, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2250
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6017, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2240
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6018, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6019, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6020, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3829
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6021, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6022, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1540
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6023, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6024, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1485
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6025, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6025
Update 6026, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6027, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1317
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6028, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4296
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6029, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6030, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6031, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6032, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6033, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6034, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2520
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6035, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.6372
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6036, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6037, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6038, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4173
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6039, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6040, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6041, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1839
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6042, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6043, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6044, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6045, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1461
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6046, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6047, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6048, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2988
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6049, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6050, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6050
Update 6051, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6052, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6053, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1237
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6054, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1410
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6055, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6056, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6057, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2504
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6058, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6059, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3556
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6060, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6061, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6062, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6063, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3377
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6064, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6065, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1059
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6066, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6067, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6068, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6069, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0696
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6070, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6071, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6072, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2823
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6073, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3899
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6074, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4896
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6075, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6075
Update 6076, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6077, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6078, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6079, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0676
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6080, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6081, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0991
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6082, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6083, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6084, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6085, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6086, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2749
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6087, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6088, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6089, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6090, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0929
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6091, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0767
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6092, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2611
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6093, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6094, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1701
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6095, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6096, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.4567
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6097, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6098, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6099, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3632
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6100, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6100
Update 6101, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6102, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6103, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6104, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6105, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3440
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6106, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6107, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6108, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.3392
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6109, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6110, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6111, num samples collected 6250, FPS 53
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6112, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6113, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6114, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0674
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6115, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2173
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6116, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6117, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1216
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6118, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0877
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6119, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3643
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6120, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.8091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6121, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6122, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6123, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6124, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6125, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6125
Update 6126, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6127, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6128, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6129, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3363
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6130, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6131, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6132, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6133, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4486
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6134, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6135, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6136, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0897
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6137, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6138, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1369
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6139, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6140, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6141, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6142, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2806
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6143, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6144, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2335
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6145, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6146, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0288
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6147, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3657
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6148, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1265
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6149, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2982
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6150, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6150
Update 6151, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6152, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2516
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6153, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6154, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6155, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6156, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2435
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6157, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2300
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6158, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.5900
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6159, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6160, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6161, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2612
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6162, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0768
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6163, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6164, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6165, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3182
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6166, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6167, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6168, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6169, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2525
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6170, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6171, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1732
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6172, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6173, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6174, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6175, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6175
Update 6176, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6177, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6178, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6179, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1508
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6180, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1751
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6181, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6182, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6183, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6184, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6185, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6186, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6187, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3018
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6188, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3838
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6189, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6190, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6191, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6192, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2738
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6193, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6194, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6195, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4379
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6196, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6197, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2291
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6198, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6199, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0839
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6200, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6200
Update 6201, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6202, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6203, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6204, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3892
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6205, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1238
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6206, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6207, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6208, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2589
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6209, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6210, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6211, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3039
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6212, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3306
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6213, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6214, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6215, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1523
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6216, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1933
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6217, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6218, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0747
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6219, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6220, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6221, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4262
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6222, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1590
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6223, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2874
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6224, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6225, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2742
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6225
Update 6226, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0769
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6227, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6228, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6229, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6230, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.5549
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6231, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3353
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6232, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6233, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6234, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6235, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3757
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6236, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6237, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6238, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6239, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0731
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6240, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4006
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6241, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6242, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6243, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6244, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6245, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6246, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3823
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6247, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6248, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2236
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6249, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3894
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6250, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6250
Update 6251, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3174
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6252, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6253, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6254, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6255, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1540
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6256, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6257, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.6023
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6258, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0185
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6259, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6260, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1239
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6261, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2362
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6262, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0231
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6263, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6264, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6265, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3178
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6266, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6267, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2297
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6268, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1811
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6269, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3598
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6270, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6271, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6272, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0684
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6273, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6274, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6275, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6275
Update 6276, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6277, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6278, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6279, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2914
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6280, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3220
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6281, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6282, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6283, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6284, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6285, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6286, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6287, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0817
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6288, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6289, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3391
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6290, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1429
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6291, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2293
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6292, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6293, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6294, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6295, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6296, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2679
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6297, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1158
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6298, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6299, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6300, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.5541
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6300
Update 6301, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1677
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6302, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6303, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6304, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1934
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6305, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6306, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6307, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3842
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6308, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6309, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4489
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6310, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6311, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3020
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6312, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2561
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6313, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6314, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6315, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6316, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6317, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6318, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6319, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6320, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6321, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3499
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6322, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4205
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6323, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0408
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6324, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2371
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6325, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2676
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6325
Update 6326, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6327, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6328, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6329, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6330, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6331, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6332, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2782
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6333, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4523
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6334, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1239
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6335, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6336, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6337, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1998
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6338, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6339, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2394
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6340, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6341, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0712
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6342, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6343, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1672
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6344, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6345, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.5669
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6346, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4742
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6347, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6348, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6349, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6350, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6350
Update 6351, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6352, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6353, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6354, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6355, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6356, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2236
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6357, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6358, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6359, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6360, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1049
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6361, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2365
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6362, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6363, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6364, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0928
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6365, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2212
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6366, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3813
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6367, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1616
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6368, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3697
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6369, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6370, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3643
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6371, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1196
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6372, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3896
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6373, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6374, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6375, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1568
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6375
Update 6376, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6377, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1958
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6378, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2471
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6379, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6380, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6381, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1428
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6382, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2380
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6383, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6384, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.6489
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6385, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1839
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6386, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6387, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3007
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6388, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6389, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2472
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6390, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6391, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6392, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2252
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6393, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6394, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6395, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6396, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2232
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6397, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1463
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6398, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0793
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6399, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6400, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6400
Update 6401, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6402, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0702
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6403, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6404, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6405, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6406, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1965
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6407, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6408, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6409, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2489
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6410, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2744
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6411, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3347
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6412, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2595
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6413, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0250
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6414, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2187
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6415, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2258
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6416, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1341
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6417, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1669
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6418, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6419, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6420, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6421, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2410
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6422, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6423, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6424, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6425, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6425
Update 6426, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6427, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6428, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6429, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0979
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6430, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6431, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3504
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6432, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6433, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6434, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6435, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6436, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6437, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6438, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6439, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3317
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6440, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4673
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6441, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6442, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2248
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6443, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1762
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6444, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2669
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6445, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6446, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6447, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4011
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6448, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6449, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4701
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6450, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6450
Update 6451, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3229
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6452, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1208
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6453, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6454, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3470
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6455, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6456, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6457, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2479
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6458, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6459, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4931
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6460, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6461, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3761
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6462, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6463, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6464, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3804
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6465, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6466, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6467, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1894
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6468, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.4340
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6469, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6470, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6471, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6472, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1884
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6473, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6474, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6475, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6475
Update 6476, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6477, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0489
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6478, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6479, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1850
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6480, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6481, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6482, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.5721
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6483, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6484, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6485, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6486, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6487, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6488, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6489, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6490, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3743
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6491, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6492, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6493, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6494, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6495, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2068
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6496, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1822
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6497, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2676
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6498, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6499, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1950
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6500, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3002
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6500
Update 6501, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6502, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6503, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6504, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3651
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6505, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6506, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6507, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6508, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3820
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6509, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6510, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3958
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6511, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.3199
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6512, num samples collected 6250, FPS 52
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6513, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6514, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6515, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6516, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6517, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6518, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6519, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6520, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1891
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6521, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6522, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4443
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6523, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6524, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4081
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6525, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6525
Update 6526, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2064
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6527, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6528, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6529, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2385
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6530, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2989
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6531, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6532, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6533, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6534, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6535, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4648
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6536, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6537, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2545
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6538, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6539, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6540, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6541, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6542, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6543, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6544, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6545, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3253
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6546, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6547, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6548, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1777
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6549, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6550, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6550
Update 6551, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2724
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6552, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6553, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6554, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6555, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2599
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6556, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6557, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6558, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6559, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2751
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6560, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6561, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2568
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6562, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0362
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6563, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2280
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6564, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6565, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6566, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1209
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6567, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4624
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6568, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6569, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6570, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6571, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2783
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6572, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6573, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2686
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6574, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6575, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6575
Update 6576, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3130
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6577, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6578, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6579, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6580, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6581, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.5444
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6582, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6583, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2329
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6584, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6585, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6586, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2363
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6587, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1352
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6588, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6589, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6590, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6591, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2641
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6592, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6593, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6594, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6595, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2198
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6596, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6597, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6598, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3298
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6599, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6600, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6600
Update 6601, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2275
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6602, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6603, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2176
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6604, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6605, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6606, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1418
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6607, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6608, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6609, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4453
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6610, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6611, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6612, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6613, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6614, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4471
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6615, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1909
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6616, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6617, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3193
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6618, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6619, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6620, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6621, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3135
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6622, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6623, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3009
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6624, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6625, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2198
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6625
Update 6626, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6627, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6628, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6629, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2904
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6630, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6631, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6632, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.6531
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6633, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6634, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6635, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1708
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6636, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4014
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6637, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6638, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6639, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6640, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6641, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1196
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6642, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6643, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6644, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1239
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6645, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6646, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6647, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2288
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6648, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1642
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6649, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2850
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6650, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6650
Update 6651, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2320
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6652, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2439
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6653, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6654, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6655, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6656, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1248
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6657, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6658, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1554
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6659, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1453
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6660, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6661, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3177
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6662, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4822
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6663, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0519
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6664, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3172
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6665, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6666, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6667, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6668, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2047
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6669, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6670, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1233
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6671, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6672, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2310
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6673, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6674, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6675, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3982
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6675
Update 6676, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6677, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6678, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2935
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6679, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6680, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6681, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1067
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6682, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6683, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6684, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6685, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4431
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6686, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6687, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6688, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6689, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2767
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6690, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2999
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6691, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6692, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6693, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3382
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6694, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6695, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2508
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6696, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3048
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6697, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6698, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6699, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6700, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6700
Update 6701, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6702, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2022
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6703, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6704, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6705, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4479
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6706, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6707, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2903
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6708, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1631
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6709, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6710, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6711, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.5125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6712, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1485
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6713, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6714, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3918
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6715, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2850
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6716, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6717, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6718, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6719, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6720, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6721, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6722, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0679
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6723, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3204
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6724, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6725, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6725
Update 6726, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3593
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6727, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2441
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6728, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6729, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6730, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3244
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6731, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.5678
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6732, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2512
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6733, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1737
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6734, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6735, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6736, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6737, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6738, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6739, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6740, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6741, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2111
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6742, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6743, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6744, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6745, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2783
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6746, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2763
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6747, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6748, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1484
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6749, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6750, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6750
Update 6751, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6752, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6753, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6754, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6755, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6756, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6757, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6758, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4754
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6759, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6760, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0475
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6761, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3971
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6762, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6763, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6764, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6765, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6766, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1237
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6767, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3539
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6768, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1208
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6769, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6770, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2161
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6771, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1609
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6772, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1938
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6773, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3401
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6774, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6775, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6775
Update 6776, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6777, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4814
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6778, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0927
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6779, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6780, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1966
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6781, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6782, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2075
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6783, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6784, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6785, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6786, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2305
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6787, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1900
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6788, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2434
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6789, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6790, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6791, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6792, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6793, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3965
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6794, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6795, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3326
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6796, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6797, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6798, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2367
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6799, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6800, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2630
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6800
Update 6801, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6802, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3672
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6803, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1612
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6804, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3205
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6805, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6806, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6807, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3912
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6808, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6809, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6810, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1064
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6811, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6812, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6813, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6814, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6815, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2319
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6816, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6817, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6818, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6819, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4121
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6820, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6821, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6822, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3775
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6823, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6824, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0707
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6825, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2551
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6825
Update 6826, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6827, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2244
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6828, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1662
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6829, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6830, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3331
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6831, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6832, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6833, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6834, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3418
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6835, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6836, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6837, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3400
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6838, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1334
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6839, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6840, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6841, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0981
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6842, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3708
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6843, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6844, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6845, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2994
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6846, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6847, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6848, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6849, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1200
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6850, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6850
Update 6851, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3103
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6852, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3689
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6853, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3948
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6854, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2102
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6855, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6856, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6857, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6858, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6859, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6860, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3010
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6861, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.5659
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6862, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6863, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6864, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6865, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6866, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6867, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6868, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6869, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6870, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1071
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6871, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6872, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6873, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0775
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6874, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4107
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6875, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6875
Finished MB training, ran for 60 epochs
Update 6876, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3258
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6877, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6878, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1503
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6879, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1014
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6880, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6881, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2352
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6882, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0914
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6883, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3725
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6884, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0759
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6885, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6886, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6887, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.4791
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6888, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2587
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6889, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6890, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6891, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6892, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6893, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6894, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6895, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.3043
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6896, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6897, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6898, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6899, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.1551
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
Update 6900, num samples collected 6250, FPS 51
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -257.8208, l 200.0000, t 196.3637, TestReward -239.7983
New EPOCH! 6900
Update 6901, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2947
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6902, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6903, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6904, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5244
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6905, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6906, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6907, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3423
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6908, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3448
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6909, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6910, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1876
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6911, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6912, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6913, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6914, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6915, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6916, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6917, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6918, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2752
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6919, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1554
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6920, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6921, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6922, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3532
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6923, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5275
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6924, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6925, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6926, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0370
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 6926
Update 6927, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6928, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6929, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6930, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0869
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6931, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4449
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6932, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6933, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2651
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6934, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6935, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6037
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6936, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6937, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6938, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6939, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6940, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6941, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4906
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6942, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6943, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6944, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6945, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6946, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6947, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2014
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6948, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1474
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6949, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3715
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6950, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6951, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4204
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6952, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2677
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 6952
Update 6953, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6954, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2615
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6955, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6956, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6957, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6958, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6959, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6960, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6961, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6962, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6963, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6964, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2235
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6965, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6966, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2880
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6967, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6045
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6968, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6969, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6970, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6971, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6972, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6973, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6974, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6975, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2445
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6976, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3267
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6977, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2468
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6978, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5498
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 6978
Update 6979, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1429
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6980, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4320
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6981, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1664
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6982, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6983, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2730
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6984, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3015
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6985, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6986, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6987, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6988, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6989, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1758
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6990, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6991, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6992, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6993, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6994, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6995, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6996, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5383
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6997, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3641
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6998, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 6999, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0315
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7000, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7001, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7002, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2298
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7003, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7004, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0362
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7004
Update 7005, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1602
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7006, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7007, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7008, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2319
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7009, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7010, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7011, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7012, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7013, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2205
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7014, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1544
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7015, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7016, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7017, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2229
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7018, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7019, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4695
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7020, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7021, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7022, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1177
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7023, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7024, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3177
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7025, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2384
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7026, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7027, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1535
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7028, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2591
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7029, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7030, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7030
Update 7031, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2385
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7032, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1637
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7033, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6696
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7034, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2371
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7035, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7036, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7037, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2777
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7038, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7039, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7040, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7041, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1575
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7042, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7043, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2572
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7044, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7045, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7046, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4243
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7047, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7048, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7049, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1197
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7050, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7051, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2620
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7052, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2191
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7053, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2713
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7054, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7055, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7056, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7056
Update 7057, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7058, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5817
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7059, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1691
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7060, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3842
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7061, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1246
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7062, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7063, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7064, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7065, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7066, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1721
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7067, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7068, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7069, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2672
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7070, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0697
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7071, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1614
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7072, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3495
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7073, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2232
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7074, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7075, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7076, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3860
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7077, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7078, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7079, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7080, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7081, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7082, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7082
Update 7083, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2816
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7084, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7085, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7086, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7087, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7088, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4759
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7089, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7090, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7091, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7092, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7093, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5794
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7094, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7095, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7096, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0919
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7097, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1303
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7098, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7099, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7100, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2272
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7101, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7102, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7103, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7104, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7105, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7106, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3706
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7107, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1847
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7108, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7108
Update 7109, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7110, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7111, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0974
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7112, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7113, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7114, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7115, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1564
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7116, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5027
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7117, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7118, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3097
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7119, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7120, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2525
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7121, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1503
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7122, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1677
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7123, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1242
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7124, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2294
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7125, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7126, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5284
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7127, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7128, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7129, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7130, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7131, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7132, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7133, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4569
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7134, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3787
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7134
Update 7135, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7136, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7137, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7138, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5915
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7139, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2326
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7140, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5308
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7141, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7142, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7143, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1762
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7144, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7145, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7146, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1928
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7147, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7148, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7149, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4743
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7150, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4035
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7151, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7152, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7153, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7154, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3142
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7155, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7156, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7157, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7158, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1176
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7159, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7160, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7160
Update 7161, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7162, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7163, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7164, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7165, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2963
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7166, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7167, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3732
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7168, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1689
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7169, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1992
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7170, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7171, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7172, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2031
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7173, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6166
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7174, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2150
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7175, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0632
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7176, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1506
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7177, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7178, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0262
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7179, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2821
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7180, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7181, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7182, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7183, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7184, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2397
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7185, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7186, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7186
Update 7187, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7188, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1565
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7189, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7190, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7191, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2643
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7192, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0757
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7193, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2375
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7194, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7195, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7196, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7197, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2656
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7198, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2363
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7199, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3904
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7200, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7201, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7202, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1321
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7203, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0413
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7204, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7205, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7206, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1273
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7207, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7208, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4702
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7209, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1513
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7210, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0503
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7211, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3315
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7212, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2901
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7212
Update 7213, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3578
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7214, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7215, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2380
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7216, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2713
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7217, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7218, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7219, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2150
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7220, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0934
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7221, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1272
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7222, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5418
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7223, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7224, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7225, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7226, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1689
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7227, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7228, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2741
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7229, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7230, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7231, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7232, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7233, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7234, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7235, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7236, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1081
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7237, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7238, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7238
Update 7239, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7240, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7241, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7242, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7243, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2999
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7244, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0423
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7245, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7246, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7247, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3346
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7248, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2567
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7249, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7250, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7251, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2221
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7252, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5844
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7253, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3389
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7254, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2370
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7255, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7256, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7257, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7258, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7259, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7260, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7261, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7262, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7263, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7264, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2849
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7264
Update 7265, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7266, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7267, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1071
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7268, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6832
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7269, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7270, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7271, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7272, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0316
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7273, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7274, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3787
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7275, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7276, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7277, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7278, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7279, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7280, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7281, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3101
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7282, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1261
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7283, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7284, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7285, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5950
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7286, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7287, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7288, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3980
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7289, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7290, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7290
Update 7291, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7292, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7293, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7294, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3891
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7295, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7296, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7297, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4294
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7298, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7299, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1490
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7300, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7301, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7302, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7303, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0851
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7304, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5247
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7305, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7306, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2734
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7307, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7308, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5045
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7309, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7310, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1902
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7311, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1939
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7312, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7313, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2733
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7314, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7315, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7316, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7316
Update 7317, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1664
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7318, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7319, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7320, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1889
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7321, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4482
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7322, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7323, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7324, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7325, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7326, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7327, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3356
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7328, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7329, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7330, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7331, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1203
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7332, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7333, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7334, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2958
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7335, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7336, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7337, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3210
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7338, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2803
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7339, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2234
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7340, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3682
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7341, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7342, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7342
Update 7343, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7344, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7345, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7346, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7347, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7348, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7349, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7350, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3515
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7351, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7352, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1940
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7353, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7354, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7355, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7356, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7357, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7358, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0677
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7359, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7360, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2229
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7361, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3036
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7362, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4600
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7363, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1651
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7364, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3219
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7365, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4357
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7366, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7367, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7368, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7368
Update 7369, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7370, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7371, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7372, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0839
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7373, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7374, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7375, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7376, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7377, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7378, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2270
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7379, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7380, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7381, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7859
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7382, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1252
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7383, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2554
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7384, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7385, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7386, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7387, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.6668
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7388, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2139
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7389, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7390, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7391, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1259
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7392, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7393, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3760
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7394, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7394
Update 7395, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4545
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7396, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1690
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7397, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7398, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1000
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7399, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7400, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7401, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1904
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7402, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7403, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7404, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.5713
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7405, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7406, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7407, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4574
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7408, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7409, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7410, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7411, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7412, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7413, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7414, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7415, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1280
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7416, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1120
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7417, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7418, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7419, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7420, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.7471
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7420
Update 7421, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7422, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7423, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2475
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7424, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7425, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7426, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3943
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7427, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7428, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7429, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2458
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7430, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7431, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1670
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7432, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3393
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7433, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1672
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7434, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2312
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7435, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7436, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7437, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1657
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7438, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7439, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7440, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7441, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3279
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7442, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7443, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2812
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7444, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7445, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7446, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7446
Update 7447, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7448, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7449, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7450, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7451, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7452, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7453, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1807
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7454, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2998
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7455, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7456, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7457, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4563
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7458, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1623
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7459, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0236
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7460, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7461, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3009
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7462, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.3302
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7463, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0668
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7464, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7465, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4820
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7466, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7467, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7468, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0903
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7469, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7470, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.4049
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7471, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.1439
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7472, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7472
Update 7473, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2556
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7474, num samples collected 6500, FPS 46
  Algorithm: train_loss 0.2400
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7475, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1953
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7476, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7477, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1450
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7478, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7479, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1192
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7480, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7481, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7482, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1424
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7483, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7484, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7485, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7486, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7487, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7488, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5039
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7489, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4426
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7490, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7491, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4494
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7492, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7493, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7494, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7495, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7496, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7497, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2053
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7498, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7498
Update 7499, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4610
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7500, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5745
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7501, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7502, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7503, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7504, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7505, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7506, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7507, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7508, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7509, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7510, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1439
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7511, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1733
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7512, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1108
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7513, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7514, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2195
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7515, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1622
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7516, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7517, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7518, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1805
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7519, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7520, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1929
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7521, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7522, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1436
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7523, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0760
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7524, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3032
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7524
Update 7525, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7526, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4626
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7527, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2633
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7528, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7529, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2415
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7530, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2946
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7531, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7532, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7533, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3132
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7534, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7535, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7536, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7537, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7538, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7539, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1243
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7540, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7541, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7542, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1458
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7543, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7544, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0873
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7545, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7546, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7547, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7548, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7549, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3597
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7550, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7550
Update 7551, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7552, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7553, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7554, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2386
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7555, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7556, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2245
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7557, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7558, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0307
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7559, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7560, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7561, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1915
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7562, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3511
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7563, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0183
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7564, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2412
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7565, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7566, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7567, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1899
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7568, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3331
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7569, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1904
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7570, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7571, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7572, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7573, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0736
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7574, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2705
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7575, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1971
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7576, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7576
Update 7577, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7578, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7579, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7580, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7581, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7582, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7583, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7584, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1227
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7585, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7586, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7587, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6794
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7588, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7589, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7590, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2151
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7591, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1748
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7592, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7593, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2624
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7594, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2390
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7595, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7596, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5351
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7597, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7598, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7599, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0977
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7600, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7601, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7602, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7602
Update 7603, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7604, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7605, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7606, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3992
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7607, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4750
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7608, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7609, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0443
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7610, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4837
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7611, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7612, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7613, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7614, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2339
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7615, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1481
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7616, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7617, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7618, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7619, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1223
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7620, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7621, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7622, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5062
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7623, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7624, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1463
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7625, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7626, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7627, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2301
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7628, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4185
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7628
Update 7629, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2234
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7630, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7631, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7632, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2234
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7633, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7634, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7635, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7636, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7637, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7638, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7639, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4516
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7640, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7641, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7642, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2861
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7643, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7644, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7645, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3651
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7646, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7647, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7648, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3184
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7649, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7650, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1781
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7651, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7652, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5670
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7653, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7654, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7654
Update 7655, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7656, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7657, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7658, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7659, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0992
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7660, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7661, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7662, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7663, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1180
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7664, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7665, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5134
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7666, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0479
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7667, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7668, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3027
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7669, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7670, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7671, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3479
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7672, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7673, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7674, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2840
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7675, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7676, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2251
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7677, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2761
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7678, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3397
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7679, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7680, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7680
Update 7681, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1491
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7682, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7683, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7684, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7685, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5305
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7686, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7687, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4165
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7688, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7689, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7690, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7691, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7692, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7693, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7694, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7695, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7696, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7697, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7698, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.7029
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7699, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0333
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7700, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7701, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3418
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7702, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7703, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0280
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7704, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7705, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3372
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7706, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7706
Update 7707, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7708, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4016
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7709, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7710, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7711, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7712, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7713, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7714, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7715, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2146
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7716, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7717, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7718, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2387
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7719, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7720, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2862
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7721, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7722, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3207
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7723, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0682
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7724, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6424
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7725, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7726, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7727, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7728, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1518
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7729, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7730, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7731, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7732, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7732
Update 7733, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7734, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2341
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7735, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2302
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7736, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7737, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7738, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7739, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7740, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0890
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7741, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1037
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7742, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1699
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7743, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7744, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7745, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7746, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7747, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7748, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7749, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7750, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7751, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7752, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1665
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7753, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7754, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7755, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1991
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7756, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3711
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7757, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2410
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7758, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7758
Update 7759, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4642
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7760, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7761, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6684
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7762, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7763, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7764, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1775
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7765, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7766, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7767, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7768, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7769, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2891
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7770, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7771, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1274
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7772, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1688
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7773, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0862
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7774, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7775, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1276
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7776, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7777, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3747
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7778, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7779, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7780, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7781, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0422
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7782, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7783, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2399
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7784, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7784
Update 7785, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7786, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5335
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7787, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7788, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7789, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7790, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7791, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7792, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7793, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7794, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7795, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7796, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4182
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7797, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3953
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7798, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7799, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7800, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7801, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4237
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7802, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7803, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1170
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7804, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7805, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7806, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7807, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2007
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7808, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7809, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7810, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4867
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7810
Update 7811, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1745
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7812, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7813, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7814, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1469
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7815, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3138
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7816, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1239
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7817, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7818, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7819, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1546
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7820, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2898
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7821, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7822, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7823, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7824, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4481
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7825, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7826, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7827, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7828, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1255
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7829, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0892
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7830, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7831, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7832, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6802
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7833, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7834, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7835, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7836, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7836
Update 7837, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7838, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7839, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2340
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7840, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7841, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7842, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7843, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7844, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7845, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7846, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7847, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7848, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7849, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1543
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7850, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4206
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7851, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7852, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7853, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2472
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7854, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7855, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7856, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7857, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7858, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7859, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1202
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7860, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2168
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7861, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7862, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7862
Update 7863, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1917
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7864, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7865, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0540
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7866, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3988
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7867, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7868, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2956
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7869, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7870, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1467
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7871, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7872, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7873, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7874, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7875, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5311
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7876, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7877, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7878, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7879, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7880, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7881, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7882, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7883, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7884, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7885, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7886, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2678
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7887, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7888, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6508
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7888
Update 7889, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7890, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7891, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5423
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7892, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7893, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7894, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2268
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7895, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1528
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7896, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3576
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7897, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7898, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7899, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7900, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4174
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7901, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2160
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7902, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2559
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7903, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7904, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0653
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7905, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7906, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7907, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0775
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7908, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7909, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1207
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7910, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7911, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7912, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3783
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7913, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1160
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7914, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6359
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7914
Update 7915, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7916, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7917, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7918, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7919, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2649
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7920, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2240
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7921, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7922, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7923, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1180
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7924, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7925, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7926, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7927, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7928, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7929, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7930, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7931, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7932, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5649
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7933, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7934, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1246
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7935, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7936, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7937, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1873
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7938, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4257
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7939, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7940, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7940
Update 7941, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3028
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7942, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4605
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7943, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7944, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7945, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4503
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7946, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7947, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2800
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7948, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7949, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2939
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7950, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7951, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0945
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7952, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7953, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0701
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7954, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0831
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7955, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1961
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7956, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7957, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7958, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7959, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7960, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1522
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7961, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7962, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0282
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7963, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7964, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7965, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1299
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7966, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7966
Update 7967, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7968, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2528
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7969, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7970, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2387
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7971, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7972, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2452
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7973, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1727
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7974, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7975, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7976, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7977, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3814
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7978, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7979, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7980, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4691
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7981, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7982, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7983, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7984, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1660
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7985, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7986, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7987, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7988, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7989, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7990, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2989
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7991, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7992, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5909
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 7992
Update 7993, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7994, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7995, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7996, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7997, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2043
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7998, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 7999, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8000, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8001, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1010
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8002, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1192
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8003, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1726
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8004, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2401
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8005, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8006, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8007, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0205
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8008, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8009, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8010, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4771
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8011, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8012, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1773
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8013, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4871
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8014, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8015, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4559
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8016, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2715
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8017, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1189
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8018, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8018
Update 8019, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8020, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8021, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2370
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8022, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8023, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8024, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2900
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8025, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8026, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8027, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1171
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8028, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8029, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1975
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8030, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1683
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8031, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0978
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8032, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2533
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8033, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1711
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8034, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8035, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8036, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8037, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2883
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8038, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5753
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8039, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8040, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8041, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8042, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8043, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8044, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8044
Update 8045, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8046, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2929
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8047, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8048, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8049, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8050, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8051, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8052, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1973
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8053, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1900
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8054, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8055, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8056, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5828
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8057, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8058, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1080
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8059, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8060, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8061, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4926
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8062, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8063, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8064, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8065, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2147
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8066, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8067, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1266
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8068, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6706
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8069, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1811
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8070, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8070
Update 8071, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1874
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8072, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8073, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8074, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8075, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8076, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8077, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1373
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8078, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8079, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8080, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0492
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8081, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8082, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2248
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8083, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5465
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8084, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1468
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8085, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1439
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8086, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8087, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2110
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8088, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.5169
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8089, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3007
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8090, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8091, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.3061
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8092, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8093, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8094, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8095, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8096, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8096
Update 8097, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8098, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8099, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2301
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8100, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8101, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1973
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8102, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0334
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8103, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8104, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8105, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8106, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8107, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8108, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8109, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1178
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8110, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0527
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8111, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.2596
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8112, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1597
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8113, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6018
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8114, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4258
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8115, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8116, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1994
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8117, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1634
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8118, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4780
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8119, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8120, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8121, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8122, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8122
Update 8123, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1477
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8124, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1580
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8125, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8126, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.4351
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8127, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1916
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8128, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1674
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8129, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8130, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8131, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8132, num samples collected 6500, FPS 45
  Algorithm: train_loss 0.6733
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8133, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8134, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0322
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8135, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8136, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1854
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8137, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8138, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3410
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8139, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0690
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8140, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8141, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8142, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8143, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8144, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8145, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8146, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8147, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4135
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8148, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8148
Update 8149, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8150, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8151, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8152, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3306
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8153, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8154, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8155, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3298
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8156, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8157, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8158, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8159, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2443
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8160, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8161, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8162, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8163, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1166
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8164, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1771
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8165, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8166, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4205
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8167, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8168, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3407
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8169, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8170, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3743
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8171, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8172, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8173, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4962
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8174, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8174
Update 8175, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3401
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8176, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8177, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8178, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8179, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8180, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3044
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8181, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8182, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8183, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8184, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8185, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8186, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5088
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8187, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8188, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2316
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8189, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8190, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8191, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8192, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8193, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8194, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2375
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8195, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8196, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8197, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1675
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8198, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1979
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8199, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8200, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8200
Update 8201, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3557
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8202, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8203, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8204, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8205, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8206, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8207, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8208, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3266
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8209, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8210, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8211, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4035
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8212, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0344
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8213, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8214, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1469
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8215, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8216, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5765
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8217, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8218, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8219, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8220, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3249
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8221, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8222, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8223, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1004
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8224, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2745
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8225, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8226, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8226
Update 8227, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3802
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8228, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0856
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8229, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4471
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8230, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8231, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2793
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8232, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8233, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1149
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8234, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1480
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8235, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8236, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8237, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8238, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8239, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0849
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8240, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8241, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1694
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8242, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8243, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8244, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5827
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8245, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2421
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8246, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8247, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8248, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8249, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2846
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8250, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8251, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8252, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8252
Update 8253, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8254, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5915
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8255, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8256, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2478
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8257, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2321
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8258, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8259, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4130
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8260, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8261, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8262, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8263, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8264, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2221
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8265, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8266, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1674
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8267, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1782
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8268, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8269, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8270, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4757
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8271, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8272, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8273, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8274, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8275, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8276, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8277, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8278, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8278
Update 8279, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8280, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2242
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8281, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8282, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1583
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8283, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2599
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8284, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2292
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8285, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8286, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2198
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8287, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8288, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8289, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1498
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8290, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8291, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1717
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8292, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2848
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8293, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4789
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8294, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8295, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8296, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3633
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8297, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8298, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1505
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8299, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0892
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8300, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8301, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8302, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8303, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8304, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8304
Update 8305, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2693
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8306, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8307, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8308, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8309, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8310, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8311, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2766
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8312, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1254
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8313, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8314, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8315, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3875
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8316, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8317, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1221
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8318, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8319, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8320, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8321, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8322, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5828
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8323, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8324, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1492
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8325, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8326, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2507
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8327, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8328, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1803
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8329, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8330, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8330
Update 8331, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2621
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8332, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8333, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8334, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8335, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8336, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3758
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8337, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2520
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8338, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1953
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8339, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1562
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8340, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8341, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4343
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8342, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8343, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1746
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8344, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1512
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8345, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8346, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2757
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8347, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8348, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8349, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8350, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0817
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8351, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8352, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8353, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8354, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1325
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8355, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1612
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8356, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8356
Update 8357, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2300
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8358, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8359, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4593
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8360, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2208
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8361, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2524
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8362, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8363, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8364, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8365, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8366, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2927
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8367, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0882
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8368, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8369, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1933
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8370, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8371, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8372, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0878
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8373, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3802
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8374, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8375, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3031
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8376, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8377, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8378, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8379, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8380, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8381, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8382, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8382
Update 8383, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8384, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2812
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8385, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8386, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1012
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8387, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8388, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0730
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8389, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0911
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8390, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8391, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1193
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8392, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2668
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8393, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8394, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8395, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8396, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8397, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8398, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0980
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8399, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8400, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5841
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8401, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4431
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8402, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3721
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8403, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1154
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8404, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1736
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8405, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8406, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4247
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8407, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8408, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8408
Update 8409, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2643
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8410, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8411, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8412, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3134
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8413, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8414, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3221
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8415, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8416, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8417, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8418, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8419, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1998
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8420, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8421, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3771
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8422, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8423, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0358
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8424, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8425, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8426, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0900
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8427, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0268
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8428, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8429, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8430, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4562
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8431, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2244
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8432, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8433, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8434, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.5807
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8434
Finished MB training, ran for 60 epochs
Update 8435, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8436, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8437, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8438, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8439, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8440, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8441, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8442, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1218
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8443, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8444, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8445, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1118
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8446, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8447, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0233
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8448, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8449, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8450, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1447
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8451, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.3648
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8452, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8453, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8454, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2174
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8455, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0837
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8456, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.1314
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8457, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.4445
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8458, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8459, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.2418
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
Update 8460, num samples collected 6500, FPS 44
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -232.3699, l 200.0000, t 221.4779, TestReward -519.7425
New EPOCH! 8460
Update 8461, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8462, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0955
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8463, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8464, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3549
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8465, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8466, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0720
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8467, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8468, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8469, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1861
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8470, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8471, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8472, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0697
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8473, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8474, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8475, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2801
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8476, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8477, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8478, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2775
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8479, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3584
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8480, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4797
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8481, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2483
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8482, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8483, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8484, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2219
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8485, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4651
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8486, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8487, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8487
Update 8488, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1541
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8489, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8490, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1879
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8491, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1300
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8492, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0414
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8493, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8494, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8495, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8496, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8497, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6358
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8498, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8499, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8500, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8501, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8502, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1019
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8503, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0389
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8504, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8505, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8506, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8507, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8508, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8509, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2513
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8510, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1816
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8511, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1689
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8512, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8513, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8514, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8514
Update 8515, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8516, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2765
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8517, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8518, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8519, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8520, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8521, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8522, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8523, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8524, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8525, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8526, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8527, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8528, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8529, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8530, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2683
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8531, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8532, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4782
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8533, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2227
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8534, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8535, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2229
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8536, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8537, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2964
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8538, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8539, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3197
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8540, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0480
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8541, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8541
Update 8542, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8543, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8544, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3450
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8545, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8546, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8547, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0189
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8548, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8549, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3533
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8550, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2394
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8551, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8552, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4697
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8553, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1313
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8554, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8555, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8556, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8557, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8558, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4203
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8559, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4246
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8560, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1465
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8561, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8562, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8563, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8564, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2428
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8565, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3202
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8566, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1579
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8567, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8568, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8568
Update 8569, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8570, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1226
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8571, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4915
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8572, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8573, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2398
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8574, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8575, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0955
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8576, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8577, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2448
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8578, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8579, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2386
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8580, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2958
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8581, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4524
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8582, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8583, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3046
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8584, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8585, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8586, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8587, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8588, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2680
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8589, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8590, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8591, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8592, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2743
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8593, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8594, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8595, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8595
Update 8596, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0958
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8597, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2143
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8598, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8599, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8600, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8601, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8602, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8603, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8604, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8605, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8606, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8607, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8608, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8609, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8610, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8611, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4137
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8612, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2677
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8613, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8614, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6778
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8615, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2755
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8616, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8617, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1618
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8618, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3558
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8619, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2179
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8620, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8621, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8622, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8622
Update 8623, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8624, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8625, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0620
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8626, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8627, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8628, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8629, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3381
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8630, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8631, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.6206
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8632, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8633, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8634, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3436
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8635, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8636, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1471
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8637, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8638, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8639, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8640, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8641, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8642, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8643, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8644, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4247
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8645, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8646, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1258
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8647, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8648, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8649, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8649
Update 8650, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8651, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8652, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2589
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8653, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8654, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8655, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8656, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3580
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8657, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1171
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8658, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8659, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1638
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8660, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1866
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8661, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8662, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0922
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8663, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1982
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8664, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8665, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3048
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8666, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2376
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8667, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1705
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8668, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8669, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8670, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8671, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2441
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8672, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8673, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2654
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8674, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8675, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2174
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8676, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8676
Update 8677, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2518
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8678, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8679, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8680, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8681, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8682, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8683, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8684, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8685, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8686, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1297
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8687, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2867
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8688, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1460
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8689, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8690, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8691, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5247
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8692, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8693, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8694, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5720
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8695, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4936
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8696, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8697, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8698, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8699, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8700, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8701, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8702, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8703, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8703
Update 8704, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1303
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8705, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8706, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8707, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8708, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8709, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4211
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8710, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8711, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3950
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8712, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8713, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8714, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8715, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0897
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8716, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8717, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8718, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8719, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5435
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8720, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3688
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8721, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8722, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8723, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3371
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8724, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8725, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8726, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8727, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3976
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8728, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2471
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8729, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0329
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8730, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8730
Update 8731, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8732, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1943
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8733, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2155
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8734, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8735, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2762
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8736, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8737, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8738, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8739, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3721
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8740, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8741, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1194
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8742, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8743, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8744, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8745, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8746, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8747, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3293
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8748, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8749, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8750, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8751, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8752, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0273
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8753, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2312
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8754, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2681
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8755, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8756, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8757, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4345
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8757
Update 8758, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8759, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8760, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8761, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0804
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8762, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8763, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8764, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8765, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8766, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2855
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8767, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8768, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8769, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3578
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8770, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8771, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8772, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8773, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1282
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8774, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2287
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8775, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8776, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3584
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8777, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8778, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8779, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8780, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8781, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8782, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8783, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8784, num samples collected 6750, FPS 41
  Algorithm: train_loss 1.1326
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8784
Update 8785, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8786, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8787, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2868
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8788, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4332
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8789, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8790, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8791, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8792, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0303
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8793, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8794, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8795, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1015
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8796, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.4953
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8797, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8798, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2099
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8799, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8800, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8801, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8802, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1191
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8803, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2458
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8804, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8805, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3018
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8806, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3751
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8807, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2346
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8808, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8809, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1425
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8810, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8811, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8811
Update 8812, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8813, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.5961
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8814, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1168
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8815, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8816, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1256
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8817, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8818, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8819, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8820, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8821, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3602
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8822, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8823, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8824, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3979
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8825, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1509
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8826, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3104
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8827, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8828, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8829, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8830, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8831, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8832, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8833, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8834, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0270
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8835, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8836, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1876
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8837, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.2874
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8838, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8838
Update 8839, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8840, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3223
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8841, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3428
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8842, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8843, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8844, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8845, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.1824
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8846, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8847, num samples collected 6750, FPS 41
  Algorithm: train_loss 0.3397
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8848, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8849, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2773
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8850, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8851, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8852, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5608
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8853, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8854, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2811
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8855, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8856, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8857, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8858, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1616
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8859, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8860, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3545
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8861, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3718
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8862, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8863, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8864, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8865, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0435
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8865
Update 8866, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8867, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8868, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8869, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2209
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8870, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0929
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8871, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8872, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0684
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8873, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8874, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8875, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8876, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8877, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8878, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8879, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8880, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4311
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8881, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8882, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8883, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2904
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8884, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3732
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8885, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8886, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8887, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4555
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8888, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4387
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8889, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0206
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8890, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1223
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8891, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8892, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8892
Update 8893, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8894, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8895, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8896, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8897, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3774
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8898, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4523
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8899, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8900, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4448
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8901, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8902, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2824
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8903, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8904, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8905, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0267
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8906, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0338
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8907, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1979
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8908, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3171
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8909, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8910, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0314
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8911, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8912, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8913, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8914, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8915, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8916, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2454
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8917, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1527
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8918, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1505
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8919, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8919
Update 8920, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8921, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2642
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8922, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2828
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8923, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2337
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8924, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8925, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2617
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8926, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8927, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8928, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4437
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8929, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8930, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8931, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2410
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8932, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8933, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1826
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8934, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8935, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0648
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8936, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8937, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8938, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8939, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8940, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3876
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8941, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8942, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8943, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8944, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8945, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8946, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8946
Update 8947, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8948, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8949, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0842
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8950, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8951, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2970
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8952, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8953, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8954, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1181
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8955, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8956, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8957, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8958, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2983
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8959, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8960, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8961, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1939
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8962, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1568
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8963, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3867
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8964, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8965, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8966, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8967, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3823
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8968, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4623
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8969, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8970, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3424
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8971, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2423
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8972, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8973, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 8973
Update 8974, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1244
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8975, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8976, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3631
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8977, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8978, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8979, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3584
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8980, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8981, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8982, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0905
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8983, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8984, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8985, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2239
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8986, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2959
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8987, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8988, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8989, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8990, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3270
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8991, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2165
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8992, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4511
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8993, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8994, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1555
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8995, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8996, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1636
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8997, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8998, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 8999, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9000, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1221
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9000
Update 9001, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9002, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1276
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9003, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9004, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9005, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1417
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9006, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9007, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9008, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2242
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9009, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2439
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9010, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2143
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9011, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9012, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2819
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9013, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1535
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9014, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3023
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9015, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2675
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9016, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9017, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9018, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1341
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9019, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9020, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1664
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9021, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9022, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5895
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9023, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0940
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9024, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9025, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1761
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9026, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9027, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1338
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9027
Update 9028, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9029, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9030, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9031, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9032, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2255
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9033, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9034, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9035, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9036, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1642
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9037, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9038, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9039, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9040, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9041, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9042, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0725
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9043, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6029
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9044, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2711
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9045, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9046, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9047, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9048, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9049, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9050, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3590
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9051, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9052, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9053, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9054, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6496
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9054
Update 9055, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1449
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9056, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9057, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9058, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9059, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0888
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9060, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9061, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4172
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9062, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9063, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9064, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9065, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9066, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6127
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9067, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0519
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9068, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9069, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9070, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9071, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9072, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9073, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9074, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5622
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9075, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9076, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9077, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0792
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9078, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9079, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2450
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9080, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9081, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6242
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9081
Update 9082, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1247
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9083, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9084, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2226
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9085, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9086, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0956
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9087, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9088, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9089, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2344
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9090, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9091, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2890
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9092, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9093, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9094, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9095, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3350
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9096, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9097, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5032
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9098, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9099, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1708
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9100, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9101, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9102, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9103, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9104, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9105, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9106, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9107, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9108, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9108
Update 9109, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1500
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9110, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9111, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1224
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9112, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9113, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0980
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9114, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9115, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9116, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9117, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9118, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9119, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1198
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9120, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9121, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9122, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9123, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0033
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9124, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9125, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4313
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9126, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6404
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9127, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0281
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9128, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1272
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9129, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2870
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9130, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9131, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6037
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9132, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1657
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9133, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9134, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0722
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9135, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9135
Update 9136, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0739
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9137, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9138, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1947
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9139, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9140, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9141, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9142, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2632
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9143, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4745
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9144, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1910
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9145, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2272
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9146, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9147, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9148, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9149, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2437
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9150, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9151, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4111
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9152, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2395
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9153, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1513
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9154, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9155, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2538
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9156, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9157, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3383
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9158, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9159, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9160, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9161, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9162, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9162
Update 9163, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9164, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9165, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9166, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4024
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9167, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2661
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9168, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9169, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9170, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9171, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9172, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9173, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4978
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9174, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9175, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9176, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5851
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9177, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9178, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0866
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9179, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9180, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2015
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9181, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1803
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9182, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1926
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9183, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9184, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9185, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9186, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2863
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9187, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9188, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9189, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9189
Update 9190, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3908
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9191, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3866
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9192, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9193, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9194, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0754
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9195, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9196, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9197, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0817
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9198, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0751
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9199, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2617
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9200, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9201, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9202, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9203, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9204, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9205, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1785
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9206, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9207, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1437
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9208, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9209, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0863
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9210, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9211, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4841
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9212, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9213, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0943
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9214, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9215, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1370
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9216, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9216
Update 9217, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9218, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1641
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9219, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1240
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9220, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9221, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2158
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9222, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9223, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9224, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9225, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9226, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5591
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9227, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3565
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9228, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3356
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9229, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0356
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9230, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9231, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9232, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9233, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9234, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9235, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9236, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2435
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9237, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9238, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9239, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9240, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9241, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0403
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9242, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9243, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9243
Update 9244, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9245, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2349
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9246, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9247, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0853
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9248, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4479
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9249, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2698
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9250, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9251, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9252, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1416
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9253, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0376
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9254, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9255, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0916
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9256, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1770
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9257, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9258, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9259, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9260, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9261, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4327
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9262, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9263, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9264, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2465
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9265, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9266, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9267, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0723
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9268, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9269, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9270, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9270
Update 9271, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9272, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9273, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9274, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9275, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4773
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9276, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9277, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0674
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9278, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9279, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2333
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9280, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3757
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9281, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9282, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5527
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9283, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9284, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9285, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0234
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9286, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1297
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9287, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3173
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9288, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9289, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1233
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9290, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1519
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9291, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9292, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9293, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9294, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9295, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9296, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1032
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9297, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9297
Update 9298, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1969
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9299, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3161
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9300, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2411
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9301, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9302, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9303, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9304, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2560
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9305, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9306, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3588
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9307, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2402
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9308, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9309, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9310, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9311, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9312, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0201
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9313, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2459
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9314, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9315, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9316, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9317, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1713
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9318, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9319, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9320, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0833
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9321, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9322, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9323, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9324, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9324
Update 9325, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1456
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9326, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1542
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9327, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3445
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9328, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9329, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9330, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9331, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1071
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9332, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2677
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9333, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1753
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9334, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9335, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2316
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9336, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9337, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2450
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9338, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2918
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9339, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9340, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9341, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9342, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9343, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9344, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9345, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9346, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2961
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9347, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9348, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9349, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2356
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9350, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9351, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9351
Update 9352, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9353, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4683
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9354, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4721
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9355, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9356, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0602
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9357, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9358, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1389
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9359, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3522
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9360, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9361, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9362, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2679
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9363, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9364, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9365, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9366, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9367, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9368, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9369, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9370, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9371, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2140
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9372, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3256
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9373, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2300
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9374, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0801
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9375, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1101
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9376, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9377, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1278
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9378, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9378
Update 9379, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9380, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9381, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9382, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9383, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4181
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9384, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1770
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9385, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9386, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2421
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9387, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2409
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9388, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0785
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9389, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2873
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9390, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9391, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9392, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9393, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9394, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9395, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9396, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9397, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9398, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9399, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1674
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9400, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9401, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9402, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9403, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1517
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9404, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0876
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9405, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9405
Update 9406, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9407, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4052
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9408, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9409, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1979
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9410, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9411, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9412, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9413, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9414, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9415, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3788
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9416, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3278
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9417, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2909
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9418, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9419, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0470
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9420, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9421, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3981
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9422, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1204
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9423, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9424, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9425, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9426, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9427, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9428, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9429, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9430, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9431, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9432, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9432
Update 9433, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9434, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0301
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9435, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9436, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2346
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9437, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9438, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2932
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9439, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9440, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4131
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9441, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1499
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9442, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9443, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2792
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9444, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9445, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9446, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9447, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9448, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0479
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9449, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2383
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9450, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9451, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4835
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9452, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3376
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9453, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9454, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1482
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9455, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9456, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1595
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9457, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9458, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9459, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9459
Update 9460, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9461, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9462, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2840
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9463, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2488
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9464, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9465, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9466, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9467, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9468, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2608
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9469, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1973
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9470, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9471, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9472, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3255
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9473, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9474, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2437
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9475, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9476, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9477, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1917
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9478, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2415
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9479, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2666
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9480, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9481, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9482, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1626
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9483, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9484, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9485, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1733
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9486, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9486
Update 9487, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4255
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9488, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9489, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9490, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9491, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9492, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1391
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9493, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9494, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2699
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9495, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9496, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9497, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9498, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1201
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9499, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2479
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9500, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9501, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0856
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9502, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9503, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0867
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9504, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9505, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9506, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1031
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9507, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1287
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9508, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9509, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9510, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9511, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2450
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9512, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9513, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.6549
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9513
Update 9514, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9515, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1215
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9516, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3363
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9517, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0942
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9518, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1995
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9519, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9520, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2504
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9521, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9522, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9523, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9524, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2530
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9525, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9526, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9527, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3582
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9528, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9529, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2326
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9530, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2607
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9531, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1420
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9532, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9533, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9534, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0677
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9535, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1242
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9536, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9537, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9538, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9539, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9540, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9540
Update 9541, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0863
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9542, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9543, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0458
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9544, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9545, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3278
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9546, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9547, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2029
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9548, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1631
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9549, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9550, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9551, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9552, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3221
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9553, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5906
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9554, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9555, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2400
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9556, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9557, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9558, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9559, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9560, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9561, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2441
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9562, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9563, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2873
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9564, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9565, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9566, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9567, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9567
Update 9568, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9569, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9570, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9571, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2758
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9572, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9573, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0483
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9574, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3262
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9575, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1650
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9576, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9577, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9578, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9579, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9580, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9581, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9582, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9583, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.7453
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9584, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9585, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9586, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9587, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9588, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9589, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9590, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4102
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9591, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9592, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9593, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4345
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9594, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9594
Update 9595, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9596, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1225
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9597, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9598, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0935
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9599, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0392
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9600, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9601, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9602, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9603, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2898
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9604, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1231
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9605, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2408
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9606, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9607, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0266
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9608, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5207
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9609, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3049
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9610, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9611, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4292
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9612, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9613, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9614, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9615, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9616, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9617, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9618, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9619, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9620, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9621, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9621
Update 9622, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.5659
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9623, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9624, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9625, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9626, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9627, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9628, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2170
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9629, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9630, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9631, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9632, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9633, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1931
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9634, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9635, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9636, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9637, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2022
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9638, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3812
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9639, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4524
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9640, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9641, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1605
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9642, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9643, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9644, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.7004
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9645, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9646, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9647, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9648, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9648
Update 9649, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9650, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2309
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9651, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9652, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2023
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9653, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9654, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9655, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2129
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9656, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2241
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9657, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9658, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9659, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4675
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9660, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9661, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9662, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9663, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1984
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9664, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2290
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9665, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9666, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1184
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9667, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2814
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9668, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9669, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3948
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9670, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9671, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2286
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9672, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0355
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9673, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9674, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9675, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9675
Update 9676, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9677, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9678, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9679, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9680, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9681, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9682, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9683, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9684, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9685, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3429
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9686, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9687, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9688, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0834
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9689, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9690, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9691, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9692, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9693, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3464
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9694, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2144
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9695, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9696, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9697, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1544
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9698, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2722
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9699, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2042
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9700, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9701, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3456
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9702, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9702
Update 9703, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9704, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9705, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0850
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9706, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.4635
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9707, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9708, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9709, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.3288
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9710, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.2316
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9711, num samples collected 6750, FPS 40
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9712, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.5050
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9713, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9714, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9715, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9716, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1731
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9717, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9718, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9719, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9720, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3373
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9721, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9722, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9723, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9724, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1830
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9725, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9726, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9727, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9728, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9729, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9729
Update 9730, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9731, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3462
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9732, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1408
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9733, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3948
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9734, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2359
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9735, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0173
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9736, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1985
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9737, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9738, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2157
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9739, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9740, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9741, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1247
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9742, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9743, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9744, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9745, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1623
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9746, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9747, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9748, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9749, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9750, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9751, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0807
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9752, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2733
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9753, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2351
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9754, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9755, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9756, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9756
Update 9757, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3377
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9758, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9759, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9760, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2153
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9761, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2681
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9762, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9763, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9764, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1891
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9765, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9766, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3205
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9767, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9768, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1487
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9769, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9770, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9771, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9772, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3205
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9773, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2049
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9774, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9775, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2691
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9776, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2335
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9777, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9778, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9779, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1743
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9780, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2402
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9781, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2471
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9782, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9783, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9783
Update 9784, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9785, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3769
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9786, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9787, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9788, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9789, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9790, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9791, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9792, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9793, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9794, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9795, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2313
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9796, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9797, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9798, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0920
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9799, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.8851
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9800, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1660
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9801, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3801
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9802, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1859
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9803, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0454
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9804, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9805, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1384
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9806, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4112
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9807, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9808, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9809, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1228
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9810, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9810
Update 9811, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3534
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9812, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2525
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9813, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9814, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9815, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9816, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9817, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2067
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9818, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3268
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9819, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9820, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9821, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2442
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9822, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9823, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9824, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9825, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1587
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9826, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1148
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9827, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9828, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9829, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9830, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9831, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3531
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9832, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2751
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9833, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2732
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9834, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1994
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9835, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1504
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9836, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9837, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9837
Update 9838, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0506
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9839, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9840, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9841, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.6231
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9842, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9843, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9844, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9845, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9846, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1268
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9847, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4666
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9848, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9849, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9850, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9851, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9852, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2335
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9853, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9854, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9855, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0606
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9856, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4212
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9857, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3113
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9858, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0934
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9859, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9860, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9861, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9862, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9863, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9864, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9864
Update 9865, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2022
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9866, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9867, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9868, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9869, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0225
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9870, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9871, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9872, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2201
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9873, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9874, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9875, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9876, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9877, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9878, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9879, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3349
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9880, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9881, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2688
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9882, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9883, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0791
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9884, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9885, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4611
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9886, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1268
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9887, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9888, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9889, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9890, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4435
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9891, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9891
Update 9892, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9893, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9894, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9895, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3526
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9896, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9897, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0491
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9898, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9899, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9900, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0463
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9901, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0596
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9902, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9903, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9904, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3276
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9905, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2393
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9906, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9907, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9908, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2622
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9909, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9910, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9911, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4865
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9912, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9913, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9914, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9915, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2039
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9916, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3794
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9917, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9918, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9918
Update 9919, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9920, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9921, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9922, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0838
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9923, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9924, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9925, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2464
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9926, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2085
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9927, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9928, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9929, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0861
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9930, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9931, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1315
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9932, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1473
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9933, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3974
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9934, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9935, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9936, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2243
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9937, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3790
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9938, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0363
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9939, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2489
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9940, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2652
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9941, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3974
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9942, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9943, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9944, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9945, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9945
Update 9946, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9947, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1178
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9948, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9949, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1043
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9950, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4379
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9951, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2447
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9952, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9953, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1889
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9954, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9955, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0609
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9956, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9957, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9958, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9959, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9960, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9961, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2945
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9962, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9963, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2403
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9964, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9965, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.7381
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9966, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9967, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0317
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9968, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2404
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9969, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1002
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9970, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0674
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9971, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9972, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1579
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9972
Update 9973, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2362
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9974, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2081
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9975, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9976, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3460
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9977, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2786
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9978, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9979, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9980, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2297
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9981, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1265
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9982, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9983, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9984, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0488
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9985, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9986, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9987, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1283
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9988, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4424
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9989, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1893
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9990, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9991, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9992, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1742
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9993, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9994, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9995, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9996, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9997, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1025
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9998, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4022
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 9999, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0251
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 9999
Update 10000, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10001, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10002, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2733
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10003, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10004, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10005, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3242
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10006, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10007, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3433
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10008, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10009, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10010, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10011, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10012, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10013, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10014, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1040
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10015, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10016, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3704
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10017, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10018, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10019, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2830
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10020, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4766
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10021, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10022, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10023, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1210
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10024, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10025, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2745
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10026, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 10026
Update 10027, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10028, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3871
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10029, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10030, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10031, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10032, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10033, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10034, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2516
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10035, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10036, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10037, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10038, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2373
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10039, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2932
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10040, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4059
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10041, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.3526
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10042, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10043, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10044, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10045, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10046, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10047, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10048, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2354
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10049, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4257
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10050, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10051, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2678
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10052, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1396
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10053, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.5937
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 10053
Finished MB training, ran for 60 epochs
Update 10054, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10055, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10056, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10057, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1353
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10058, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10059, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10060, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10061, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10062, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.2005
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10063, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.9638
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10064, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0871
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10065, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10066, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10067, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4773
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10068, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10069, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10070, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10071, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10072, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10073, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10074, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10075, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10076, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10077, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.5055
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10078, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10079, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
Update 10080, num samples collected 6750, FPS 39
  Algorithm: train_loss 0.4585
  Episodes: TrainReward -2.4182, l 200.0000, t 244.6780, TestReward -258.6286
New EPOCH! 10080
Update 10081, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3225
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10082, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1387
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10083, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10084, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10085, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10086, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10087, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4585
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10088, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3254
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10089, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10090, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10091, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2397
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10092, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3510
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10093, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10094, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1333
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10095, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0682
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10096, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0983
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10097, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10098, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10099, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2822
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10100, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1014
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10101, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10102, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10103, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10104, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1851
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10105, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10106, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10107, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10108, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4825
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10108
Update 10109, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10110, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10111, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10112, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2988
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10113, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10114, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10115, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3147
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10116, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10117, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10118, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10119, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10120, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10121, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10122, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10123, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3998
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10124, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10125, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10126, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10127, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1743
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10128, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5634
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10129, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10130, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3007
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10131, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1494
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10132, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10133, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10134, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0934
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10135, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1278
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10136, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10136
Update 10137, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2409
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10138, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2622
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10139, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0509
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10140, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10141, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10142, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10143, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2593
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10144, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3543
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10145, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1264
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10146, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10147, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3052
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10148, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2812
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10149, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10150, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10151, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10152, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10153, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10154, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10155, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10156, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10157, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1287
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10158, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1722
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10159, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10160, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10161, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10162, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3117
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10163, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10164, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.8409
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10164
Update 10165, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1109
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10166, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10167, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10168, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0590
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10169, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10170, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10171, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10172, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10173, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2404
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10174, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10175, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10176, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10177, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10178, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10179, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10180, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10181, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10182, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10183, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4431
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10184, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3227
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10185, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1949
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10186, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2742
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10187, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0931
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10188, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10189, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4890
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10190, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2709
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10191, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10192, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.6164
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10192
Update 10193, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3794
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10194, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10195, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10196, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2395
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10197, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3772
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10198, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1946
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10199, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1143
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10200, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10201, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10202, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4216
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10203, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0941
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10204, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10205, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10206, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1529
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10207, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10208, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10209, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10210, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10211, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10212, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10213, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10214, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4020
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10215, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10216, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10217, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10218, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3638
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10219, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10220, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10220
Update 10221, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10222, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10223, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3373
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10224, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10225, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3444
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10226, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10227, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3920
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10228, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10229, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10230, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2914
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10231, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10232, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10233, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10234, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1969
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10235, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1199
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10236, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1274
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10237, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2400
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10238, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0593
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10239, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10240, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3580
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10241, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10242, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2220
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10243, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10244, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10245, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10246, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2764
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10247, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2825
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10248, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10248
Update 10249, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10250, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2439
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10251, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0278
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10252, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0954
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10253, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10254, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10255, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10256, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4357
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10257, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10258, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10259, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1682
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10260, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10261, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10262, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10263, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10264, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10265, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3117
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10266, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10267, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10268, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10269, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10270, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10271, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10272, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10273, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4556
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10274, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10275, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1303
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10276, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10276
Update 10277, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10278, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2211
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10279, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0803
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10280, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10281, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1896
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10282, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4515
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10283, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10284, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2994
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10285, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10286, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10287, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1648
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10288, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10289, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10290, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10291, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10292, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10293, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0761
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10294, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10295, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1463
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10296, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10297, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4638
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10298, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10299, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2514
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10300, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3708
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10301, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10302, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0855
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10303, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10304, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10304
Update 10305, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10306, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10307, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10308, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10309, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10310, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2267
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10311, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2257
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10312, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1263
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10313, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1415
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10314, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4306
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10315, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10316, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10317, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.7140
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10318, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2875
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10319, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10320, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10321, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10322, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10323, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1558
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10324, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10325, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0936
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10326, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10327, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10328, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10329, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1702
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10330, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3365
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10331, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10332, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10332
Update 10333, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10334, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10335, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10336, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10337, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10338, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1514
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10339, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3789
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10340, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1182
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10341, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10342, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0932
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10343, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10344, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10345, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1965
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10346, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10347, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0902
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10348, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10349, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1104
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10350, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10351, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10352, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0399
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10353, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10354, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4865
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10355, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2855
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10356, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5044
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10357, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3625
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10358, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10359, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10360, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10360
Update 10361, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10362, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3368
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10363, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10364, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1643
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10365, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10366, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10367, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10368, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4582
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10369, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10370, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10371, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3056
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10372, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10373, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0609
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10374, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10375, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10376, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10377, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2302
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10378, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10379, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10380, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1710
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10381, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2427
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10382, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2032
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10383, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10384, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3451
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10385, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10386, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1664
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10387, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10388, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4854
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10388
Update 10389, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10390, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10391, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10392, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10393, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10394, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4245
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10395, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10396, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10397, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10398, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2747
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10399, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10400, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10401, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10402, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10403, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0831
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10404, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4035
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10405, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10406, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0971
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10407, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10408, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10409, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10410, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1136
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10411, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10412, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4817
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10413, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2017
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10414, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10415, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10416, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10416
Update 10417, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10418, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10419, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4813
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10420, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1234
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10421, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3154
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10422, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3899
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10423, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10424, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10425, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0892
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10426, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3950
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10427, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10428, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10429, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10430, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3024
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10431, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2128
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10432, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10433, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1570
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10434, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1333
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10435, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1589
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10436, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10437, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10438, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0815
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10439, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10440, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10441, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10442, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10443, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10444, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0028
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10444
Update 10445, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10446, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10447, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10448, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10449, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10450, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1700
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10451, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10452, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3954
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10453, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10454, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2419
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10455, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2454
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10456, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3067
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10457, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10458, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10459, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10460, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2458
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10461, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10462, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1258
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10463, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10464, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1808
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10465, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10466, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2387
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10467, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0743
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10468, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10469, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1536
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10470, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10471, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10472, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10472
Update 10473, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1034
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10474, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10475, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2744
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10476, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10477, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3477
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10478, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10479, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10480, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2033
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10481, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4224
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10482, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10483, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10484, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10485, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2311
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10486, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10487, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10488, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10489, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0715
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10490, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10491, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1536
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10492, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10493, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10494, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0513
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10495, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2852
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10496, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10497, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1393
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10498, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10499, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10500, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10500
Update 10501, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10502, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5700
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10503, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10504, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10505, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10506, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10507, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10508, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10509, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10510, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10511, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10512, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2806
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10513, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10514, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3170
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10515, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1908
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10516, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1716
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10517, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10518, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1286
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10519, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10520, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10521, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2811
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10522, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0976
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10523, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0553
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10524, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10525, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10526, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0348
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10527, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10528, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10528
Update 10529, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10530, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10531, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2323
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10532, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1008
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10533, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10534, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10535, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2333
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10536, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10537, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10538, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.6534
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10539, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10540, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2620
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10541, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1997
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10542, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10543, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10544, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10545, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3186
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10546, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10547, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0023
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10548, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10549, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0991
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10550, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2891
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10551, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2307
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10552, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10553, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3602
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10554, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10555, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10556, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10556
Update 10557, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2626
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10558, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10559, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3133
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10560, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2468
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10561, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10562, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10563, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1122
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10564, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1094
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10565, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1611
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10566, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10567, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1587
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10568, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10569, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0954
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10570, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10571, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10572, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10573, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0203
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10574, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0368
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10575, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10576, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10577, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10578, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4039
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10579, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10580, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10581, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10582, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2454
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10583, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.4944
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10584, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10584
Update 10585, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.2954
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10586, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.5449
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10587, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10588, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10589, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10590, num samples collected 7000, FPS 37
  Algorithm: train_loss 0.1691
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10591, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10592, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2220
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10593, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10594, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10595, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10596, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3302
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10597, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10598, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3572
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10599, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10600, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10601, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2107
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10602, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10603, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10604, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1993
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10605, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3224
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10606, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10607, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10608, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10609, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10610, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1450
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10611, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10612, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10612
Update 10613, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10614, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3150
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10615, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10616, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10617, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10618, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10619, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2807
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10620, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10621, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5371
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10622, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2867
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10623, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10624, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2261
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10625, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10626, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10627, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10628, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10629, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10630, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10631, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2457
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10632, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3330
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10633, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0861
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10634, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2439
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10635, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10636, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10637, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10638, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10639, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3571
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10640, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10640
Update 10641, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10642, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2147
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10643, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10644, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10645, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0920
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10646, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4356
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10647, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4010
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10648, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0572
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10649, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1492
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10650, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10651, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10652, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10653, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0626
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10654, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0332
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10655, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4772
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10656, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10657, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10658, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10659, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10660, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0981
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10661, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1919
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10662, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3004
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10663, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10664, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10665, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1273
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10666, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10667, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1654
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10668, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10668
Update 10669, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10670, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10671, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10672, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10673, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2197
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10674, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1800
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10675, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2131
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10676, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10677, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3887
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10678, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10679, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10680, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10681, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.7516
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10682, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10683, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10684, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10685, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1248
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10686, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3184
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10687, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10688, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10689, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1167
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10690, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10691, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0938
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10692, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10693, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10694, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2067
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10695, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10696, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10696
Update 10697, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2578
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10698, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0951
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10699, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10700, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10701, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2449
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10702, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10703, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10704, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10705, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10706, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10707, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2660
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10708, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10709, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1693
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10710, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10711, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0180
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10712, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2274
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10713, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3185
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10714, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10715, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10716, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2548
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10717, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1651
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10718, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5027
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10719, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10720, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10721, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2007
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10722, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10723, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10724, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10724
Update 10725, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4033
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10726, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10727, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10728, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10729, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10730, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10731, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10732, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0518
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10733, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1188
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10734, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0973
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10735, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1365
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10736, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10737, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.6148
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10738, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10739, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1341
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10740, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10741, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10742, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0939
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10743, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10744, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2045
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10745, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2024
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10746, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10747, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10748, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10749, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10750, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4979
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10751, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10752, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.7691
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10752
Update 10753, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10754, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2328
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10755, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1089
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10756, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10757, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10758, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4757
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10759, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10760, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10761, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10762, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0232
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10763, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2625
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10764, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1784
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10765, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10766, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2133
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10767, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10768, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10769, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10770, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0505
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10771, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5140
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10772, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10773, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10774, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10775, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0994
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10776, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10777, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10778, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1441
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10779, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1279
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10780, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10780
Update 10781, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10782, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10783, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1754
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10784, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10785, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10786, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10787, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10788, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1430
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10789, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10790, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0726
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10791, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2444
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10792, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1045
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10793, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2652
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10794, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10795, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10796, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10797, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1442
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10798, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10799, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.6762
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10800, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10801, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10802, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2938
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10803, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2581
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10804, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10805, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0686
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10806, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10807, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10808, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10808
Update 10809, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0551
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10810, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1497
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10811, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4544
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10812, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1390
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10813, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10814, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2757
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10815, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0632
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10816, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10817, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1958
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10818, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10819, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0290
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10820, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1493
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10821, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10822, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10823, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10824, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10825, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10826, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5678
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10827, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0373
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10828, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3618
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10829, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10830, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10831, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2185
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10832, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10833, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10834, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1428
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10835, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10836, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10836
Update 10837, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1193
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10838, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10839, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10840, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10841, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4900
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10842, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4317
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10843, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10844, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10845, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.6034
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10846, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10847, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10848, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3304
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10849, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10850, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10851, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10852, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10853, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10854, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10855, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1427
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10856, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10857, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10858, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10859, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10860, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10861, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1200
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10862, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10863, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10864, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10864
Update 10865, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10866, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10867, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10868, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10869, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10870, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2322
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10871, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10872, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10873, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2572
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10874, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0651
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10875, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10876, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2503
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10877, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10878, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0884
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10879, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10880, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10881, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10882, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2643
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10883, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10884, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5951
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10885, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1624
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10886, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10887, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4420
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10888, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3021
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10889, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1618
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10890, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10891, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2758
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10892, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10892
Update 10893, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10894, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10895, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10896, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10897, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10898, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3903
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10899, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0469
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10900, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0889
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10901, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10902, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1505
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10903, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10904, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10905, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2308
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10906, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10907, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10908, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10909, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2404
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10910, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2767
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10911, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3135
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10912, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10913, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10914, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1238
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10915, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4796
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10916, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10917, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10918, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10919, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2513
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10920, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10920
Update 10921, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0971
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10922, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10923, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10924, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4676
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10925, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10926, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3404
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10927, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10928, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2308
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10929, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10930, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10931, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10932, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2427
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10933, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2511
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10934, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10935, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10936, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1731
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10937, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10938, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10939, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10940, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10941, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3020
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10942, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10943, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10944, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10945, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3218
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10946, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2711
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10947, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2112
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10948, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4551
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10948
Update 10949, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10950, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10951, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5225
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10952, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10953, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10954, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10955, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0474
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10956, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.8454
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10957, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10958, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10959, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10960, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10961, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1233
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10962, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1337
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10963, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10964, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10965, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10966, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4848
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10967, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10968, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10969, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10970, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10971, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10972, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10973, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10974, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10975, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1007
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10976, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 10976
Update 10977, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10978, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10979, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2416
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10980, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10981, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3190
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10982, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0416
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10983, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0312
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10984, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10985, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10986, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10987, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10988, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0223
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10989, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1173
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10990, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10991, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10992, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4707
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10993, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10994, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10995, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10996, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10997, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10998, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2214
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 10999, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11000, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5966
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11001, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2144
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11002, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4122
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11003, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11004, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5967
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11004
Update 11005, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11006, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11007, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11008, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11009, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11010, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11011, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11012, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11013, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5763
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11014, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11015, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4481
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11016, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3113
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11017, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11018, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11019, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11020, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11021, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11022, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11023, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0860
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11024, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2758
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11025, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11026, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11027, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11028, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5408
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11029, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11030, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3610
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11031, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11032, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11032
Update 11033, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2575
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11034, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1680
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11035, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11036, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11037, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11038, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11039, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11040, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0867
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11041, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11042, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11043, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11044, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11045, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2312
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11046, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11047, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0851
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11048, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1768
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11049, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11050, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11051, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11052, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11053, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11054, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4179
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11055, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4456
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11056, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2485
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11057, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4898
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11058, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0994
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11059, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11060, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11060
Update 11061, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11062, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11063, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11064, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11065, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11066, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1812
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11067, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1330
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11068, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1228
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11069, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4379
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11070, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2846
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11071, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11072, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11073, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4120
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11074, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11075, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3525
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11076, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1345
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11077, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11078, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11079, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11080, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11081, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1464
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11082, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0390
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11083, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11084, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11085, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11086, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2215
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11087, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2347
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11088, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11088
Update 11089, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11090, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11091, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11092, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0847
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11093, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1263
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11094, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0669
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11095, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11096, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11097, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11098, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2489
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11099, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11100, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2807
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11101, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11102, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0778
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11103, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11104, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11105, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2412
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11106, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2790
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11107, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1841
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11108, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2593
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11109, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2720
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11110, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2599
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11111, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3999
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11112, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11113, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1778
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11114, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1699
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11115, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11116, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11116
Update 11117, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1377
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11118, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5707
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11119, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11120, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11121, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11122, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11123, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11124, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11125, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11126, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1204
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11127, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11128, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4533
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11129, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11130, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11131, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.7134
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11132, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0911
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11133, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11134, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2421
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11135, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11136, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11137, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11138, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11139, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2672
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11140, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11141, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11142, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11143, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11144, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11144
Update 11145, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2362
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11146, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3723
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11147, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11148, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3524
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11149, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1536
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11150, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11151, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2171
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11152, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11153, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11154, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11155, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1980
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11156, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0435
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11157, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4218
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11158, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11159, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11160, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0827
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11161, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11162, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0274
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11163, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2499
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11164, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2104
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11165, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11166, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11167, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11168, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11169, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11170, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4539
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11171, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11172, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11172
Update 11173, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11174, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11175, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11176, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11177, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0414
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11178, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11179, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1132
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11180, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11181, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11182, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11183, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2354
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11184, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2434
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11185, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11186, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11187, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11188, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3176
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11189, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11190, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11191, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0476
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11192, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11193, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11194, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3211
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11195, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11196, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1996
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11197, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2436
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11198, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2345
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11199, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1534
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11200, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.6287
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11200
Update 11201, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11202, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11203, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11204, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11205, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11206, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11207, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3948
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11208, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11209, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2732
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11210, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11211, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0805
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11212, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4404
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11213, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0851
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11214, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11215, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11216, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11217, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11218, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0360
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11219, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3719
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11220, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1776
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11221, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2858
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11222, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0653
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11223, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4238
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11224, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11225, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11226, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1968
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11227, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11228, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11228
Update 11229, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11230, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0712
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11231, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2827
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11232, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11233, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11234, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11235, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11236, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11237, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4086
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11238, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11239, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2704
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11240, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2700
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11241, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11242, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2647
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11243, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11244, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3324
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11245, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11246, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11247, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11248, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11249, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1921
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11250, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1187
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11251, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11252, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1434
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11253, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2029
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11254, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11255, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11256, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11256
Update 11257, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2178
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11258, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0774
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11259, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2545
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11260, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11261, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3343
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11262, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11263, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11264, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11265, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11266, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2303
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11267, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0726
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11268, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0033
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11269, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3690
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11270, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11271, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1214
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11272, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11273, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11274, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11275, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11276, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4102
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11277, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3021
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11278, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11279, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0357
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11280, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2271
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11281, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3798
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11282, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11283, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4145
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11284, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11284
Update 11285, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11286, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11287, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0439
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11288, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11289, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0966
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11290, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1545
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11291, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11292, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11293, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11294, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11295, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3668
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11296, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11297, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0526
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11298, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1039
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11299, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2422
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11300, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1932
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11301, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1865
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11302, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11303, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2420
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11304, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.6144
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11305, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1205
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11306, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11307, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11308, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11309, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3523
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11310, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0843
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11311, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11312, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11312
Update 11313, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11314, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11315, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1925
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11316, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11317, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0994
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11318, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11319, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2771
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11320, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2910
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11321, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11322, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11323, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11324, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11325, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11326, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1008
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11327, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11328, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11329, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11330, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11331, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4482
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11332, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11333, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11334, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11335, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11336, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1288
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11337, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11338, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3606
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11339, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11340, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11340
Update 11341, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11342, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11343, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1217
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11344, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4023
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11345, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1859
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11346, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11347, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2678
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11348, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11349, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2882
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11350, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1758
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11351, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11352, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11353, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11354, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2320
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11355, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1488
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11356, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11357, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11358, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11359, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11360, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1463
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11361, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2318
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11362, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11363, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11364, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11365, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2584
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11366, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2391
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11367, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0820
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11368, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11368
Update 11369, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3194
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11370, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0777
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11371, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1612
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11372, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2923
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11373, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11374, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0724
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11375, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11376, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1213
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11377, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11378, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11379, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1538
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11380, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11381, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11382, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0513
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11383, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11384, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1451
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11385, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11386, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1062
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11387, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2327
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11388, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2162
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11389, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11390, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11391, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11392, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11393, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1410
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11394, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2550
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11395, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11396, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4861
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11396
Update 11397, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1278
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11398, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1563
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11399, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11400, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11401, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2403
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11402, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2891
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11403, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11404, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0785
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11405, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1938
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11406, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11407, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0925
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11408, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11409, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11410, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2716
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11411, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5207
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11412, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11413, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11414, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11415, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2582
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11416, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0229
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11417, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11418, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1764
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11419, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2221
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11420, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0700
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11421, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3368
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11422, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1648
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11423, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0365
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11424, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11424
Update 11425, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11426, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11427, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11428, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11429, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11430, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3162
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11431, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11432, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11433, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11434, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11435, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11436, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11437, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11438, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1582
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11439, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11440, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2519
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11441, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11442, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1563
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11443, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11444, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2678
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11445, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1970
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11446, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11447, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1230
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11448, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2587
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11449, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11450, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0833
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11451, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3585
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11452, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1572
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11452
Update 11453, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1619
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11454, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11455, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11456, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1061
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11457, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11458, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1939
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11459, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11460, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11461, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11462, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11463, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5829
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11464, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11465, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2679
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11466, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1598
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11467, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11468, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4507
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11469, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11470, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2283
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11471, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11472, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1887
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11473, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11474, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11475, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11476, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11477, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11478, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1283
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11479, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3768
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11480, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1751
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11480
Update 11481, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11482, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11483, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0955
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11484, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1943
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11485, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11486, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0894
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11487, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11488, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4877
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11489, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11490, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.6221
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11491, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11492, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0818
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11493, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3384
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11494, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11495, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11496, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11497, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11498, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11499, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3792
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11500, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11501, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0241
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11502, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11503, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11504, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11505, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11506, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2506
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11507, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0498
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11508, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5108
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11508
Update 11509, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11510, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1953
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11511, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11512, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2807
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11513, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11514, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11515, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2669
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11516, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0968
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11517, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0875
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11518, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11519, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11520, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2365
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11521, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11522, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11523, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3388
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11524, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1657
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11525, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11526, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1295
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11527, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1275
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11528, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2723
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11529, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3756
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11530, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11531, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11532, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1557
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11533, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11534, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1807
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11535, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0326
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11536, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11536
Update 11537, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11538, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11539, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2490
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11540, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11541, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11542, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11543, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11544, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1190
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11545, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0543
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11546, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1366
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11547, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11548, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11549, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11550, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11551, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11552, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11553, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.7369
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11554, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2434
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11555, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11556, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11557, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11558, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1334
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11559, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11560, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11561, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4196
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11562, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.7786
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11563, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11564, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.7300
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11564
Update 11565, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2903
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11566, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11567, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11568, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11569, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11570, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2296
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11571, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11572, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11573, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11574, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11575, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2374
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11576, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3274
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11577, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0388
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11578, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4169
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11579, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11580, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2456
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11581, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11582, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11583, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0967
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11584, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1307
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11585, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3862
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11586, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11587, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11588, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0981
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11589, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11590, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11591, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11592, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11592
Update 11593, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5512
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11594, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11595, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11596, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4207
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11597, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11598, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1566
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11599, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11600, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2540
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11601, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4178
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11602, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11603, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1587
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11604, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0381
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11605, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11606, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11607, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11608, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2578
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11609, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0880
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11610, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11611, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1668
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11612, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11613, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11614, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0214
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11615, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11616, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11617, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3404
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11618, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11619, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2774
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11620, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3982
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11620
Update 11621, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1174
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11622, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11623, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11624, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11625, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11626, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2669
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11627, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11628, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11629, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3180
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11630, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4040
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11631, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11632, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2538
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11633, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0813
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11634, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3586
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11635, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11636, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2451
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11637, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11638, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11639, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1633
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11640, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0816
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11641, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11642, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11643, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11644, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2663
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11645, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3746
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11646, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0209
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11647, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11648, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11648
Update 11649, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1011
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11650, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4722
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11651, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0994
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11652, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0162
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11653, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11654, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11655, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3133
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11656, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11657, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2094
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11658, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11659, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0486
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11660, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11661, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2830
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11662, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11663, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11664, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1335
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11665, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11666, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11667, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0710
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11668, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2700
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11669, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5201
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11670, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11671, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11672, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5088
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11673, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1641
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11674, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11675, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11676, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11676
Update 11677, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11678, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11679, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4473
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11680, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.4956
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11681, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11682, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3218
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11683, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11684, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3846
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11685, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11686, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11687, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0528
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11688, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11689, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11690, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3218
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11691, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2459
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11692, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11693, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11694, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1048
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11695, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1988
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11696, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11697, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11698, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11699, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11700, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.2050
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11701, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11702, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11703, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0414
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11704, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11704
Update 11705, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11706, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11707, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.3053
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11708, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11709, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11710, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11711, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11712, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11713, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0033
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11714, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11715, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.5776
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11716, num samples collected 7000, FPS 36
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11717, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2643
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11718, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.3030
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11719, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11720, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11721, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11722, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11723, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2002
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11724, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0945
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11725, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.3264
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11726, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.3585
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11727, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11728, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2501
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11729, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11730, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0976
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11731, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11732, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.6468
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11732
Finished MB training, ran for 60 epochs
Update 11733, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11734, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2712
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11735, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11736, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2765
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11737, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11738, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2163
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11739, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11740, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.2478
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11741, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11742, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11743, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.3623
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11744, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.3022
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11745, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.1021
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11746, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0249
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11747, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11748, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11749, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11750, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.4329
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11751, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11752, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11753, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11754, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0887
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11755, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.4733
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11756, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.1990
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11757, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11758, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11759, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
Update 11760, num samples collected 7000, FPS 35
  Algorithm: train_loss 0.0020
  Episodes: TrainReward -124.8596, l 200.0000, t 269.6740, TestReward -278.4055
New EPOCH! 11760
Update 11761, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2641
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11762, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11763, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11764, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11765, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0904
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11766, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2567
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11767, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0300
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11768, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11769, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11770, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1344
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11771, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11772, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11773, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11774, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11775, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4217
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11776, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.5525
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11777, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11778, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11779, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4564
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11780, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11781, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11782, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1707
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11783, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11784, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11785, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11786, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11787, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3861
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11788, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11789, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3722
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11789
Update 11790, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2698
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11791, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11792, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2619
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11793, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1293
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11794, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11795, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3699
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11796, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1856
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11797, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11798, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0213
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11799, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11800, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1806
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11801, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3263
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11802, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0539
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11803, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11804, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3993
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11805, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11806, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11807, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11808, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11809, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11810, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11811, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11812, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2194
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11813, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1119
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11814, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2616
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11815, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1637
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11816, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11817, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11818, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2181
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11818
Update 11819, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1100
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11820, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11821, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11822, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3287
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11823, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11824, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11825, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11826, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11827, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11828, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11829, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0765
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11830, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2450
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11831, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3421
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11832, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11833, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0564
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11834, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2463
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11835, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11836, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11837, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11838, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4293
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11839, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11840, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1167
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11841, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1586
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11842, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11843, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11844, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2584
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11845, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11846, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2661
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11847, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11847
Update 11848, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1281
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11849, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11850, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1501
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11851, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2955
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11852, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11853, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11854, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11855, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11856, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11857, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11858, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0611
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11859, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11860, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11861, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2267
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11862, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11863, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11864, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0534
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11865, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3174
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11866, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2269
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11867, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2514
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11868, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3003
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11869, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11870, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11871, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3083
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11872, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11873, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2226
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11874, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0904
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11875, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2560
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11876, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11876
Update 11877, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11878, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0665
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11879, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11880, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3355
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11881, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11882, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11883, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11884, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11885, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11886, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2793
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11887, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11888, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11889, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2987
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11890, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1483
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11891, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11892, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1627
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11893, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0440
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11894, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1411
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11895, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0968
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11896, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2705
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11897, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2469
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11898, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2560
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11899, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2541
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11900, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0212
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11901, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11902, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11903, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11904, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1268
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11905, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11905
Update 11906, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11907, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3197
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11908, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0137
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11909, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11910, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1675
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11911, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0588
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11912, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2792
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11913, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3432
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11914, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1681
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11915, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0668
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11916, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2805
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11917, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1560
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11918, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11919, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11920, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11921, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2940
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11922, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0230
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11923, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11924, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11925, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11926, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2639
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11927, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11928, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11929, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11930, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2276
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11931, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2809
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11932, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11933, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1200
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11934, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11934
Update 11935, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11936, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1076
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11937, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2372
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11938, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2310
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11939, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11940, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11941, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11942, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11943, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2313
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11944, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2265
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11945, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1783
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11946, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0689
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11947, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11948, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11949, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11950, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0593
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11951, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11952, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11953, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2255
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11954, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1380
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11955, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2364
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11956, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1744
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11957, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11958, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3597
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11959, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11960, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2577
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11961, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11962, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2831
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11963, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11963
Update 11964, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11965, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11966, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3573
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11967, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1997
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11968, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1261
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11969, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1728
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11970, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4565
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11971, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0618
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11972, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11973, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11974, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11975, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11976, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11977, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11978, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11979, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11980, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11981, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1994
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11982, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11983, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11984, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11985, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4155
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11986, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11987, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2911
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11988, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2428
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11989, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3745
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11990, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11991, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11992, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 11992
Update 11993, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11994, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4634
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11995, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11996, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11997, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0464
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11998, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 11999, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12000, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2728
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12001, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12002, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12003, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12004, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12005, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0259
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12006, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1145
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12007, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12008, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12009, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12010, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.5048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12011, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12012, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1680
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12013, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12014, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2696
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12015, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12016, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2553
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12017, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12018, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2231
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12019, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3010
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12020, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12021, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12021
Update 12022, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.5185
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12023, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12024, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1309
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12025, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12026, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3830
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12027, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12028, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12029, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12030, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12031, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1604
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12032, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4338
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12033, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3026
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12034, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4433
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12035, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12036, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12037, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12038, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1955
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12039, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12040, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12041, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12042, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12043, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12044, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1476
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12045, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2494
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12046, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12047, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0025
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12048, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1030
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12049, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0393
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12050, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0284
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12050
Update 12051, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2463
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12052, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12053, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2571
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12054, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12055, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12056, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1293
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12057, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.5083
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12058, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2976
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12059, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0989
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12060, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0964
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12061, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0445
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12062, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12063, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12064, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12065, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12066, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12067, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4255
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12068, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12069, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2372
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12070, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4923
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12071, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12072, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12073, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2809
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12074, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12075, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12076, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12077, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12078, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12079, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1333
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12079
Update 12080, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12081, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1634
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12082, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12083, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12084, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1296
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12085, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2748
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12086, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2108
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12087, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12088, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12089, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2361
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12090, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0610
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12091, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3134
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12092, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12093, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12094, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12095, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2640
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12096, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12097, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0437
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12098, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3570
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12099, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12100, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12101, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1605
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12102, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3377
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12103, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12104, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3895
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12105, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12106, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12107, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12108, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.5106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12108
Update 12109, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12110, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12111, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2244
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12112, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2502
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12113, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12114, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12115, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12116, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1757
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12117, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12118, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2873
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12119, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12120, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12121, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4466
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12122, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0678
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12123, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2701
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12124, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12125, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12126, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0779
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12127, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12128, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12129, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12130, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12131, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1351
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12132, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12133, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1550
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12134, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4203
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12135, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12136, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4876
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12137, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12137
Update 12138, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12139, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12140, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12141, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0948
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12142, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12143, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12144, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.4338
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12145, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.5431
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12146, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12147, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12148, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2102
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12149, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12150, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12151, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0243
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12152, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12153, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0408
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12154, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1431
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12155, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12156, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12157, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3836
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12158, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2200
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12159, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12160, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12161, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12162, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12163, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2826
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12164, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12165, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1144
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12166, num samples collected 7250, FPS 34
  Algorithm: train_loss 1.1298
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12166
Update 12167, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12168, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12169, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2000
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12170, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12171, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3493
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12172, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0211
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12173, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12174, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12175, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12176, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2810
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12177, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0504
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12178, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2368
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12179, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3600
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12180, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12181, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1930
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12182, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12183, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.6304
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12184, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3975
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12185, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12186, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12187, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12188, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12189, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12190, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12191, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12192, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12193, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12194, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0462
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12195, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12195
Update 12196, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12197, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12198, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12199, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12200, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0965
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12201, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12202, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12203, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.3458
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12204, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12205, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0952
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12206, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12207, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12208, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0933
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12209, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.2117
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12210, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12211, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12212, num samples collected 7250, FPS 34
  Algorithm: train_loss 0.1596
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12213, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12214, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4739
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12215, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1985
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12216, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12217, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12218, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12219, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12220, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12221, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2190
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12222, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4891
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12223, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4898
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12224, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12224
Update 12225, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12226, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12227, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1910
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12228, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12229, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12230, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0576
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12231, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0828
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12232, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12233, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3172
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12234, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3878
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12235, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2056
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12236, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1441
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12237, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12238, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12239, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1523
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12240, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0331
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12241, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3930
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12242, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1287
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12243, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2398
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12244, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12245, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0468
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12246, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1547
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12247, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4695
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12248, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1332
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12249, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12250, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0178
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12251, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2357
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12252, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0431
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12253, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12253
Update 12254, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12255, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12256, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2280
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12257, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12258, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12259, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12260, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2135
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12261, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1694
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12262, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3487
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12263, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12264, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0169
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12265, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12266, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12267, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12268, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12269, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12270, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12271, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3671
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12272, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5653
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12273, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12274, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12275, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12276, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6362
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12277, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0516
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12278, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12279, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12280, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2415
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12281, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0488
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12282, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12282
Update 12283, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3859
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12284, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12285, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12286, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12287, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12288, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2538
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12289, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12290, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1962
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12291, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12292, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12293, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12294, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2322
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12295, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1309
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12296, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1659
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12297, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2236
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12298, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2040
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12299, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12300, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12301, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0585
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12302, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1220
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12303, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12304, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12305, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12306, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12307, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2833
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12308, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3915
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12309, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0488
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12310, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0493
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12311, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6220
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12311
Update 12312, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12313, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3171
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12314, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1462
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12315, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12316, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1806
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12317, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12318, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12319, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0957
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12320, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1632
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12321, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0515
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12322, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12323, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12324, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0858
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12325, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4843
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12326, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12327, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12328, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12329, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2124
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12330, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1402
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12331, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12332, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0658
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12333, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12334, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6013
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12335, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1229
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12336, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5985
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12337, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0947
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12338, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12339, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12340, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0800
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12340
Update 12341, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12342, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0654
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12343, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2093
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12344, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2887
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12345, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1342
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12346, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12347, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12348, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12349, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12350, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2302
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12351, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12352, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12353, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12354, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12355, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12356, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1733
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12357, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12358, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2319
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12359, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3679
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12360, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1264
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12361, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0722
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12362, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12363, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12364, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4017
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12365, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0449
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12366, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1967
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12367, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12368, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0709
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12369, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12369
Update 12370, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1191
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12371, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0619
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12372, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12373, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12374, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12375, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1723
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12376, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12377, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12378, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12379, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12380, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12381, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2406
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12382, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1697
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12383, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5730
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12384, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12385, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12386, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12387, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0583
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12388, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2739
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12389, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1644
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12390, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12391, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12392, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12393, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0693
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12394, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6169
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12395, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12396, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1983
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12397, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3021
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12398, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0030
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12398
Update 12399, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2658
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12400, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12401, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1774
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12402, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1902
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12403, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1708
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12404, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2304
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12405, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2757
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12406, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12407, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3725
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12408, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12409, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0204
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12410, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12411, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12412, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3819
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12413, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12414, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1009
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12415, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2452
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12416, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12417, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2862
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12418, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12419, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0327
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12420, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4261
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12421, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12422, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12423, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12424, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12425, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12426, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0789
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12427, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12427
Update 12428, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3544
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12429, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1179
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12430, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0397
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12431, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12432, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0560
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12433, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4115
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12434, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12435, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2629
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12436, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1959
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12437, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0681
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12438, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2491
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12439, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12440, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12441, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12442, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12443, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12444, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12445, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12446, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2882
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12447, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12448, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2526
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12449, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0255
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12450, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12451, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12452, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12453, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12454, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3378
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12455, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1413
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12456, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12456
Update 12457, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12458, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3175
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12459, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12460, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12461, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0927
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12462, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12463, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1779
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12464, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12465, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1206
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12466, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12467, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2206
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12468, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12469, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2509
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12470, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1990
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12471, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2341
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12472, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12473, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2510
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12474, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4116
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12475, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12476, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12477, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2612
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12478, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2703
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12479, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12480, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12481, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1294
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12482, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2689
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12483, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12484, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2907
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12485, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12485
Update 12486, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12487, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12488, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12489, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12490, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12491, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0507
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12492, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1269
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12493, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12494, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0319
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12495, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12496, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12497, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12498, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1948
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12499, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12500, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3293
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12501, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0442
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12502, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1338
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12503, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1665
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12504, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1600
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12505, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0694
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12506, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4827
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12507, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12508, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1786
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12509, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.7593
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12510, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12511, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3581
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12512, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12513, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12514, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12514
Update 12515, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2422
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12516, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0467
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12517, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2713
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12518, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12519, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2498
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12520, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12521, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12522, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12523, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2692
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12524, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0627
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12525, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0482
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12526, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12527, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12528, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2826
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12529, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12530, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12531, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12532, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12533, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2259
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12534, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2396
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12535, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12536, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12537, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.7823
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12538, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1396
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12539, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12540, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12541, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12542, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0649
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12543, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0854
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12543
Update 12544, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3191
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12545, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1169
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12546, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1232
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12547, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3330
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12548, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12549, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2205
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12550, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12551, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0992
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12552, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12553, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12554, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12555, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12556, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1081
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12557, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12558, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12559, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2483
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12560, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1529
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12561, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12562, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12563, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12564, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2540
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12565, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2496
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12566, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12567, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1537
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12568, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12569, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0447
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12570, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12571, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12572, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12572
Update 12573, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12574, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1358
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12575, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1029
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12576, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12577, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0855
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12578, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12579, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12580, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12581, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12582, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2425
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12583, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1941
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12584, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12585, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12586, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6132
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12587, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0026
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12588, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3804
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12589, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12590, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12591, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2694
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12592, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2191
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12593, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12594, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2440
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12595, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12596, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12597, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1796
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12598, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12599, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12600, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12601, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12601
Update 12602, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12603, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12604, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12605, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12606, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0297
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12607, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2611
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12608, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4898
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12609, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1632
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12610, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12611, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0866
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12612, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12613, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12614, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12615, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2367
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12616, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12617, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2474
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12618, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12619, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2701
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12620, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12621, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12622, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2849
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12623, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12624, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0127
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12625, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2916
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12626, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12627, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0969
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12628, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2014
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12629, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2414
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12630, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12630
Update 12631, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12632, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12633, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2820
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12634, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12635, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12636, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4337
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12637, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2546
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12638, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2007
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12639, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12640, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12641, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1105
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12642, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0895
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12643, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1636
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12644, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0688
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12645, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12646, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12647, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12648, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12649, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4664
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12650, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12651, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5866
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12652, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12653, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0771
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12654, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12655, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0497
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12656, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12657, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12658, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2203
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12659, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12659
Update 12660, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12661, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0319
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12662, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12663, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12664, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3265
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12665, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12666, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12667, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2702
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12668, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1649
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12669, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0465
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12670, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2937
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12671, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2025
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12672, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1862
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12673, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0210
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12674, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0308
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12675, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2974
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12676, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3012
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12677, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12678, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1549
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12679, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12680, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12681, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2698
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12682, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0538
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12683, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12684, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12685, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2983
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12686, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1633
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12687, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12688, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5186
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12688
Update 12689, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12690, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2765
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12691, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0802
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12692, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0660
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12693, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12694, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12695, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1236
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12696, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12697, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12698, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12699, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0579
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12700, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12701, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12702, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2336
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12703, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0821
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12704, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4916
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12705, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12706, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12707, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12708, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2771
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12709, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12710, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12711, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1629
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12712, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12713, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2980
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12714, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2866
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12715, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12716, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1524
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12717, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0361
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12717
Update 12718, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12719, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3494
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12720, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12721, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2161
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12722, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2199
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12723, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1567
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12724, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0422
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12725, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12726, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12727, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12728, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1662
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12729, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1029
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12730, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2558
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12731, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0347
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12732, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12733, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12734, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12735, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12736, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12737, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1216
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12738, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4417
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12739, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12740, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12741, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2795
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12742, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2633
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12743, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2707
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12744, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0593
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12745, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1789
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12746, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12746
Update 12747, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0874
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12748, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12749, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2876
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12750, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12751, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12752, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1093
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12753, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12754, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3466
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12755, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1016
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12756, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2444
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12757, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2825
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12758, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0581
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12759, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12760, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2372
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12761, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12762, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3797
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12763, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0422
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12764, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12765, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2873
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12766, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1304
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12767, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0973
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12768, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1420
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12769, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12770, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2424
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12771, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0285
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12772, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1650
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12773, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12774, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12775, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12775
Update 12776, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12777, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12778, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2177
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12779, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4121
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12780, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12781, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1013
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12782, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2416
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12783, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1690
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12784, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3005
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12785, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12786, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0029
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12787, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12788, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12789, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12790, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12791, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12792, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2168
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12793, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1702
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12794, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2379
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12795, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12796, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6310
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12797, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1310
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12798, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12799, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12800, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12801, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12802, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2950
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12803, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0778
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12804, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12804
Update 12805, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12806, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12807, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0435
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12808, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2675
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12809, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12810, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12811, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12812, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3329
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12813, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12814, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12815, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1661
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12816, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4300
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12817, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12818, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12819, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3708
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12820, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12821, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12822, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4404
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12823, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12824, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12825, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12826, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12827, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2027
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12828, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12829, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1584
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12830, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12831, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12832, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2615
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12833, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12833
Update 12834, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2799
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12835, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12836, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12837, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1687
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12838, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2432
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12839, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12840, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12841, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3399
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12842, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1221
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12843, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2483
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12844, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0487
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12845, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12846, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12847, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12848, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1714
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12849, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12850, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0277
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12851, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2083
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12852, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12853, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12854, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12855, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12856, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12857, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12858, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6377
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12859, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12860, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2887
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12861, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12862, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12862
Update 12863, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12864, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3730
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12865, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12866, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12867, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12868, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12869, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1984
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12870, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12871, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0824
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12872, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12873, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12874, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12875, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12876, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1723
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12877, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0030
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12878, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4253
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12879, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0030
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12880, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12881, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12882, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2452
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12883, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12884, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12885, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12886, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4374
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12887, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2289
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12888, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0613
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12889, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2009
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12890, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6241
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12891, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12891
Update 12892, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12893, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12894, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12895, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12896, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12897, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12898, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0033
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12899, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12900, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0511
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12901, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2394
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12902, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2459
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12903, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12904, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2671
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12905, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12906, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12907, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0791
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12908, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1246
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12909, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12910, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2681
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12911, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12912, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5118
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12913, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4295
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12914, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1022
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12915, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12916, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3491
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12917, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2925
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12918, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12919, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12920, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12920
Update 12921, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12922, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12923, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1337
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12924, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0471
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12925, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0984
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12926, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12927, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0861
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12928, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12929, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3778
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12930, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1271
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12931, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2295
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12932, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12933, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12934, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3626
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12935, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3076
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12936, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3742
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12937, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12938, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12939, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12940, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12941, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12942, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12943, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0159
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12944, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2378
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12945, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12946, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12947, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4215
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12948, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12949, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12949
Update 12950, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12951, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3730
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12952, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12953, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12954, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1530
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12955, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12956, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1551
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12957, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12958, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0814
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12959, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12960, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12961, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1192
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12962, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12963, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12964, num samples collected 7250, FPS 33
  Algorithm: train_loss 1.1421
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12965, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12966, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0286
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12967, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6630
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12968, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12969, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12970, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12971, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12972, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1197
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12973, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0478
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12974, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12975, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12976, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1823
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12977, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2516
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12978, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1155
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 12978
Update 12979, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12980, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12981, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0154
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12982, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12983, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3928
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12984, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12985, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12986, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12987, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6721
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12988, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12989, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12990, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12991, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1594
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12992, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12993, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1750
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12994, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12995, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12996, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12997, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2529
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12998, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5116
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 12999, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13000, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2749
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13001, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1248
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13002, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13003, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13004, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13005, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0609
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13006, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1001
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13007, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13007
Update 13008, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0554
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13009, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0401
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13010, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0218
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13011, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1949
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13012, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0988
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13013, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13014, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1718
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13015, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13016, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4774
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13017, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13018, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13019, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13020, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13021, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0840
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13022, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2353
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13023, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13024, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13025, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4828
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13026, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13027, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13028, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13029, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4174
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13030, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13031, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13032, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13033, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5175
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13034, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3315
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13035, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13036, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13036
Update 13037, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13038, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13039, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3349
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13040, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2628
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13041, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13042, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13043, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3726
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13044, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13045, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13046, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2205
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13047, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0836
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13048, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0531
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13049, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13050, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13051, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13052, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2538
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13053, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2323
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13054, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13055, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13056, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1862
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13057, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1697
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13058, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13059, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3188
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13060, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0574
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13061, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2520
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13062, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1539
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13063, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13064, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0248
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13065, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13065
Update 13066, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1671
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13067, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1723
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13068, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3803
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13069, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0990
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13070, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13071, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0296
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13072, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13073, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5227
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13074, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13075, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13076, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1897
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13077, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2781
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13078, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2838
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13079, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13080, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13081, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13082, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13083, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13084, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13085, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1248
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13086, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13087, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1273
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13088, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13089, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13090, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3714
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13091, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0993
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13092, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5730
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13093, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13094, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1186
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13094
Update 13095, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0342
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13096, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1546
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13097, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13098, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13099, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13100, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4498
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13101, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1338
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13102, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13103, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13104, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3623
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13105, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0556
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13106, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13107, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13108, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13109, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13110, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13111, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13112, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2830
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13113, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2382
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13114, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13115, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13116, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13117, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1640
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13118, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0409
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13119, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1874
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13120, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4337
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13121, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13122, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2847
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13123, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13123
Update 13124, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13125, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13126, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0612
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13127, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0030
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13128, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1613
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13129, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2284
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13130, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13131, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3839
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13132, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13133, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0388
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13134, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13135, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13136, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13137, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2931
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13138, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0567
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13139, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1348
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13140, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13141, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13142, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1815
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13143, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13144, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13145, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13146, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13147, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13148, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2260
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13149, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5183
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13150, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13151, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5350
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13152, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13152
Update 13153, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13154, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2287
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13155, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13156, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2850
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13157, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2327
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13158, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13159, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1872
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13160, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4680
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13161, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13162, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13163, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2395
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13164, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0092
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13165, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0987
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13166, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0432
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13167, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13168, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0246
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13169, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13170, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1131
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13171, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1755
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13172, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1617
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13173, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13174, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13175, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13176, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4407
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13177, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13178, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13179, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3225
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13180, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13181, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13181
Update 13182, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13183, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13184, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13185, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13186, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13187, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1327
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13188, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1469
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13189, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2036
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13190, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13191, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13192, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13193, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13194, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1028
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13195, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1249
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13196, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1624
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13197, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13198, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2895
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13199, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2497
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13200, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13201, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3500
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13202, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2442
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13203, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2028
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13204, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4413
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13205, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13206, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1033
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13207, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3179
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13208, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1283
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13209, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1639
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13210, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13210
Update 13211, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1343
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13212, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13213, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3792
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13214, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0643
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13215, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13216, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2713
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13217, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13218, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1299
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13219, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13220, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1205
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13221, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4109
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13222, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13223, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3219
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13224, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2042
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13225, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13226, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13227, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13228, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13229, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2102
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13230, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1606
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13231, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13232, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13233, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0839
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13234, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0139
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13235, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1501
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13236, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13237, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3364
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13238, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0369
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13239, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13239
Update 13240, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0517
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13241, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13242, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13243, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13244, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13245, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13246, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13247, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13248, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0962
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13249, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13250, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1235
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13251, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5133
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13252, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13253, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13254, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2251
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13255, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13256, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2410
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13257, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1615
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13258, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13259, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2342
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13260, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2118
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13261, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13262, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1222
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13263, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13264, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13265, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13266, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13267, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13268, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13268
Update 13269, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0328
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13270, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13271, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2091
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13272, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2033
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13273, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1772
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13274, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13275, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13276, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13277, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0617
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13278, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2457
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13279, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2052
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13280, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13281, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13282, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13283, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1388
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13284, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13285, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13286, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.7254
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13287, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4421
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13288, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1111
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13289, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13290, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0847
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13291, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0391
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13292, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0709
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13293, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0500
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13294, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2256
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13295, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2355
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13296, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0128
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13297, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1682
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13297
Update 13298, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2592
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13299, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13300, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1578
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13301, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13302, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13303, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3245
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13304, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2828
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13305, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2745
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13306, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13307, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13308, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13309, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0452
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13310, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1850
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13311, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3269
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13312, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0597
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13313, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13314, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13315, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13316, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13317, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13318, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3958
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13319, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2123
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13320, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13321, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13322, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1298
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13323, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2524
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13324, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2043
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13325, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13326, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13326
Update 13327, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13328, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2562
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13329, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0998
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13330, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13331, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13332, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0024
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13333, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2278
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13334, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13335, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13336, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4455
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13337, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0350
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13338, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13339, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13340, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0738
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13341, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2447
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13342, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1053
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13343, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13344, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2377
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13345, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2236
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13346, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5147
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13347, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13348, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2281
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13349, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1525
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13350, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2323
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13351, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0661
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13352, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2499
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13353, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1684
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13354, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13355, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13355
Update 13356, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3027
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13357, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2630
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13358, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13359, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2613
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13360, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0450
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13361, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2299
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13362, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13363, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13364, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0726
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13365, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2779
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13366, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13367, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13368, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1082
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13369, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13370, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0110
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13371, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13372, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13373, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2470
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13374, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13375, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13376, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2154
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13377, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1454
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13378, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0756
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13379, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5158
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13380, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13381, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2084
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13382, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0608
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13383, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2179
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13384, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13384
Update 13385, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13386, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3071
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13387, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13388, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0760
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13389, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13390, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1533
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13391, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13392, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13393, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13394, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1385
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13395, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6011
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13396, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13397, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13398, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13399, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13400, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13401, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13402, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2505
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13403, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2515
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13404, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0247
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13405, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13406, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0327
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13407, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3498
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13408, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5239
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13409, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13410, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0473
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13411, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13412, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3655
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13413, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13413
Update 13414, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13415, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3632
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13416, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13417, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13418, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0945
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13419, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13420, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13421, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13422, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1655
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13423, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1886
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13424, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13425, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1251
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13426, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0146
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13427, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1662
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13428, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13429, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13430, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2570
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13431, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13432, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0565
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13433, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1601
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13434, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13435, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1833
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13436, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3753
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13437, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13438, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3329
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13439, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4395
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13440, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13441, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2350
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13442, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0242
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13442
Update 13443, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2349
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13444, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2594
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13445, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3210
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13446, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0698
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13447, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0199
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13448, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13449, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13450, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0428
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13451, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1596
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13452, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2442
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13453, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13454, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1324
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13455, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13456, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0525
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13457, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13458, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1231
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13459, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2189
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13460, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13461, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13462, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13463, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0910
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13464, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13465, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13466, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1392
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13467, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2459
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13468, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13469, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.5664
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13470, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0923
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13471, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13471
Finished MB training, ran for 60 epochs
Update 13472, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13473, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13474, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2768
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13475, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13476, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13477, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13478, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13479, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13480, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13481, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.6432
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13482, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13483, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13484, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3844
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13485, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13486, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13487, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0192
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13488, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13489, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1005
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13490, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.2542
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13491, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13492, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1574
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13493, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1413
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13494, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.4725
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13495, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13496, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13497, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0924
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13498, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.1576
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13499, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.3348
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
Update 13500, num samples collected 7250, FPS 33
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -241.1033, l 200.0000, t 294.8456, TestReward -530.9876
New EPOCH! 13500
Update 13501, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13502, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0699
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13503, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3759
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13504, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2311
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13505, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13506, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13507, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13508, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13509, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0522
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13510, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1970
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13511, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13512, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13513, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13514, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4360
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13515, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1890
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13516, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2501
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13517, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1398
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13518, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13519, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2576
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13520, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2264
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13521, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4445
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13522, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13523, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13524, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13525, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2627
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13526, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2399
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13527, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13528, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2349
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13529, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13530, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13530
Update 13531, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2322
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13532, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2804
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13533, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13534, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13535, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13536, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2207
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13537, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13538, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13539, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1698
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13540, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5154
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13541, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2792
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13542, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2746
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13543, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13544, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3798
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13545, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13546, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0238
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13547, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13548, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13549, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13550, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13551, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3558
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13552, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13553, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13554, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13555, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13556, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13557, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13558, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0129
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13559, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0414
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13560, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13560
Update 13561, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0030
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13562, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13563, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13564, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13565, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2819
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13566, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3595
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13567, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3788
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13568, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13569, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13570, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13571, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1279
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13572, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5390
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13573, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13574, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13575, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13576, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13577, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0424
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13578, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2859
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13579, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0785
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13580, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13581, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13582, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13583, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1734
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13584, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13585, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1129
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13586, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2371
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13587, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13588, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2558
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13589, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1137
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13590, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4421
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13590
Update 13591, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13592, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2353
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13593, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1913
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13594, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13595, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3480
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13596, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13597, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13598, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13599, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2766
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13600, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0299
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13601, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13602, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1829
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13603, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13604, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13605, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13606, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2587
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13607, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13608, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13609, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13610, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0396
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13611, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13612, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4560
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13613, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1281
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13614, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13615, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1490
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13616, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3779
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13617, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13618, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3915
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13619, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4509
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13620, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13620
Update 13621, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13622, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0174
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13623, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0161
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13624, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13625, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2586
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13626, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13627, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13628, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13629, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13630, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3344
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13631, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1695
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13632, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1340
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13633, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13634, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13635, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1479
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13636, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13637, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13638, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13639, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13640, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13641, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13642, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13643, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13644, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13645, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.6968
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13646, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4684
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13647, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1336
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13648, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4797
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13649, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1414
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13650, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1173
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13650
Update 13651, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13652, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1153
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13653, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1561
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13654, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13655, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13656, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13657, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13658, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13659, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13660, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3149
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13661, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13662, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13663, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0632
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13664, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2731
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13665, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1371
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13666, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13667, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4437
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13668, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2026
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13669, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13670, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13671, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1315
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13672, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3159
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13673, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0080
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13674, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0986
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13675, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13676, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13677, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2116
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13678, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2744
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13679, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0657
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13680, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.8547
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13680
Update 13681, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13682, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1403
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13683, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4541
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13684, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13685, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3387
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13686, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5179
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13687, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13688, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13689, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0221
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13690, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13691, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0202
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13692, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13693, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4746
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13694, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2885
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13695, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0680
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13696, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13697, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1036
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13698, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13699, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1260
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13700, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0633
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13701, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13702, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2839
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13703, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13704, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0566
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13705, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13706, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0532
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13707, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3438
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13708, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0444
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13709, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13710, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4213
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13710
Update 13711, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13712, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2444
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13713, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13714, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1814
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13715, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13716, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0642
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13717, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0908
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13718, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1357
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13719, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4128
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13720, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13721, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1127
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13722, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13723, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2551
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13724, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1325
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13725, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13726, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1265
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13727, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13728, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0949
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13729, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13730, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13731, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2314
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13732, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2806
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13733, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13734, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2234
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13735, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13736, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2127
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13737, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13738, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2540
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13739, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4733
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13740, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5136
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13740
Update 13741, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2826
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13742, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4250
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13743, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2712
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13744, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1421
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13745, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0387
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13746, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1253
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13747, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1806
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13748, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13749, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13750, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13751, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13752, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13753, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13754, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13755, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13756, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2642
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13757, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2671
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13758, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1386
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13759, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13760, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2282
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13761, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13762, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1795
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13763, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2580
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13764, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1515
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13765, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13766, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1481
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13767, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0067
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13768, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1028
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13769, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2500
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13770, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13770
Update 13771, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13772, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1888
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13773, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13774, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13775, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13776, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13777, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0861
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13778, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1373
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13779, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3551
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13780, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13781, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3718
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13782, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2607
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13783, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1667
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13784, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2217
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13785, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13786, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13787, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1422
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13788, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13789, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3771
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13790, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0357
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13791, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13792, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13793, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2566
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13794, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1368
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13795, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13796, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13797, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2100
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13798, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2855
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13799, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13800, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13800
Update 13801, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13802, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13803, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13804, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0829
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13805, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13806, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0628
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13807, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0720
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13808, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1658
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13809, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0514
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13810, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13811, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13812, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13813, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1897
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13814, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13815, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3796
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13816, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13817, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0029
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13818, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2626
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13819, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2536
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13820, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2343
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13821, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1648
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13822, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0167
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13823, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1813
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13824, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1343
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13825, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2246
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13826, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5192
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13827, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1536
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13828, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2674
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13829, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2904
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13830, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13830
Update 13831, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13832, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1555
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13833, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2476
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13834, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13835, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13836, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2905
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13837, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13838, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13839, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13840, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0577
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13841, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2797
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13842, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1292
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13843, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1729
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13844, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13845, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0417
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13846, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3553
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13847, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13848, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13849, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13850, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13851, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3121
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13852, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3311
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13853, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0655
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13854, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13855, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0239
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13856, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13857, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13858, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1467
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13859, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13860, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4024
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13860
Update 13861, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13862, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13863, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13864, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13865, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13866, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13867, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1331
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13868, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13869, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13870, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2266
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13871, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3753
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13872, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0586
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13873, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2614
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13874, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13875, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2623
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13876, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0323
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13877, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5875
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13878, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4425
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13879, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13880, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13881, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13882, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2764
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13883, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13884, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2169
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13885, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13886, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13887, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2148
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13888, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3607
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13889, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1735
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13890, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13890
Update 13891, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13892, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13893, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0549
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13894, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13895, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0226
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13896, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0926
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13897, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13898, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1262
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13899, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1242
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13900, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13901, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1670
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13902, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1750
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13903, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0592
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13904, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13905, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0544
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13906, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1583
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13907, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2489
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13908, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13909, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.7536
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13910, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13911, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13912, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4126
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13913, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2521
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13914, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0937
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13915, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13916, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1390
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13917, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0722
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13918, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0718
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13919, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3396
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13920, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13920
Update 13921, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13922, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13923, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13924, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3752
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13925, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13926, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4429
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13927, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2739
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13928, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13929, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13930, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13931, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0172
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13932, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13933, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13934, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1977
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13935, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0153
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13936, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13937, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2623
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13938, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.6396
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13939, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3118
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13940, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0711
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13941, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0272
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13942, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13943, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13944, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0896
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13945, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3386
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13946, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0438
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13947, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1026
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13948, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13949, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13950, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13950
Update 13951, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1877
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13952, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0530
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13953, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13954, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1572
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13955, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13956, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0669
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13957, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2655
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13958, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13959, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13960, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13961, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2549
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13962, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13963, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1322
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13964, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13965, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2572
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13966, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0198
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13967, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1530
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13968, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1890
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13969, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1617
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13970, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13971, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1747
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13972, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13973, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3297
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13974, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13975, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0995
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13976, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2638
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13977, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13978, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1095
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13979, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0533
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13980, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 13980
Update 13981, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0593
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13982, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13983, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13984, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13985, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2729
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13986, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1985
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13987, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3901
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13988, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13989, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13990, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13991, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13992, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1505
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13993, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0407
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13994, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13995, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5259
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13996, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0254
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13997, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0208
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13998, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4772
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 13999, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1124
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14000, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14001, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0864
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14002, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0168
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14003, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14004, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2528
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14005, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0671
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14006, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1211
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14007, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14008, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1350
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14009, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5289
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14010, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14010
Update 14011, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14012, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14013, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2633
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14014, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0294
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14015, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0508
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14016, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14017, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3766
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14018, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0027
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14019, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2798
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14020, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2182
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14021, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4557
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14022, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0453
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14023, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1621
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14024, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14025, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3210
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14026, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14027, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0309
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14028, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0799
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14029, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14030, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14031, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1593
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14032, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14033, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2431
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14034, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2454
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14035, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14036, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2540
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14037, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0386
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14038, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14039, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0635
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14040, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14040
Update 14041, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14042, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14043, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1521
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14044, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1331
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14045, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3196
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14046, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14047, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14048, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14049, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14050, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0477
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14051, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2423
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14052, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0641
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14053, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3547
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14054, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3448
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14055, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2644
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14056, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2020
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14057, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0975
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14058, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0600
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14059, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2578
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14060, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14061, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14062, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2304
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14063, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2233
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14064, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14065, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1676
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14066, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14067, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0337
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14068, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1399
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14069, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2557
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14070, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1478
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14070
Update 14071, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14072, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14073, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1313
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14074, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4965
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14075, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14076, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.7081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14077, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1381
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14078, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0029
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14079, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2787
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14080, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1686
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14081, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0605
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14082, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0416
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14083, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14084, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3035
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14085, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14086, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1923
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14087, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0794
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14088, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14089, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14090, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2517
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14091, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14092, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14093, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0558
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14094, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0240
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14095, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3191
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14096, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2496
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14097, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0190
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14098, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1322
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14099, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0200
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14100, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14100
Update 14101, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0603
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14102, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4192
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14103, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14104, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14105, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14106, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0571
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14107, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14108, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0638
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14109, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14110, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14111, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0339
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14112, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0027
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14113, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0906
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14114, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0524
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14115, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14116, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0021
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14117, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2435
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14118, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14119, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1905
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14120, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1446
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14121, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0039
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14122, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5125
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14123, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3757
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14124, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1382
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14125, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14126, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5844
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14127, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14128, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3270
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14129, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14130, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.8588
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14130
Update 14131, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0621
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14132, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14133, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1291
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14134, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14135, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14136, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3886
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14137, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14138, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0123
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14139, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14140, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1758
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14141, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14142, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14143, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1810
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14144, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14145, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1491
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14146, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4314
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14147, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0170
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14148, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2184
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14149, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14150, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2034
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14151, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14152, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14153, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1355
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14154, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0176
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14155, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4583
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14156, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3000
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14157, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14158, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2615
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14159, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14160, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14160
Update 14161, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2393
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14162, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14163, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1535
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14164, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0305
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14165, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14166, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14167, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0157
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14168, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14169, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1008
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14170, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14171, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1945
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14172, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1301
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14173, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4717
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14174, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2533
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14175, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14176, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1750
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14177, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14178, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14179, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14180, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0843
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14181, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14182, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2609
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14183, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.6064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14184, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0175
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14185, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3235
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14186, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14187, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14188, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14189, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14190, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14190
Update 14191, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14192, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3246
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14193, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1550
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14194, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2018
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14195, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0107
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14196, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14197, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0900
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14198, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14199, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0630
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14200, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0584
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14201, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2819
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14202, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14203, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.7233
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14204, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14205, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1761
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14206, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2712
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14207, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14208, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14209, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14210, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14211, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1956
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14212, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1474
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14213, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2569
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14214, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14215, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0941
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14216, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0224
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14217, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14218, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1879
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14219, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3879
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14220, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14220
Update 14221, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2543
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14222, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0607
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14223, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14224, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2004
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14225, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14226, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2830
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14227, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0292
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14228, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14229, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0415
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14230, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3406
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14231, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0420
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14232, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2227
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14233, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0593
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14234, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2149
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14235, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2856
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14236, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0109
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14237, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14238, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14239, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0148
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14240, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0644
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14241, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5156
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14242, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1901
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14243, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0646
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14244, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14245, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1278
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14246, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0275
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14247, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2458
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14248, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14249, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14250, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14250
Update 14251, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4581
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14252, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1306
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14253, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2038
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14254, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14255, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14256, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1290
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14257, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14258, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14259, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14260, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14261, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14262, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1012
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14263, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3748
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14264, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2257
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14265, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14266, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0502
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14267, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0122
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14268, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14269, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4711
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14270, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2637
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14271, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1034
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14272, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0656
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14273, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14274, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3871
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14275, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0457
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14276, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3735
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14277, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0664
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14278, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14279, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14280, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0164
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14280
Update 14281, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0367
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14282, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14283, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3120
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14284, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14285, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2329
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14286, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2864
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14287, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14288, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.6166
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14289, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14290, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14291, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14292, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14293, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0371
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14294, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4592
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14295, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1920
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14296, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14297, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0494
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14298, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14299, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0184
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14300, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0412
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14301, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0795
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14302, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14303, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14304, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14305, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14306, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14307, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14308, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14309, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14310, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14310
Update 14311, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14312, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14313, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0987
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14314, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14315, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0663
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14316, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4767
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14317, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1326
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14318, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14319, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14320, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1074
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14321, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14322, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2031
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14323, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2253
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14324, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0410
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14325, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1517
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14326, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14327, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1407
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14328, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0030
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14329, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14330, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1347
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14331, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5357
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14332, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14333, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14334, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2496
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14335, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2341
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14336, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2360
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14337, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14338, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0542
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14339, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4233
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14340, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0028
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14340
Update 14341, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14342, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14343, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14344, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14345, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14346, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1957
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14347, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2691
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14348, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0252
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14349, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2279
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14350, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1643
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14351, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14352, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0991
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14353, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2605
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14354, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0152
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14355, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1529
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14356, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1221
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14357, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14358, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14359, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14360, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1938
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14361, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14362, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4535
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14363, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1099
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14364, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14365, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0388
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14366, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14367, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0674
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14368, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3749
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14369, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14370, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0396
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14370
Update 14371, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14372, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0685
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14373, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14374, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14375, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3322
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14376, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14377, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1245
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14378, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0335
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14379, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1349
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14380, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14381, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0561
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14382, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1335
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14383, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1714
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14384, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14385, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14386, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14387, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0105
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14388, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14389, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2415
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14390, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2338
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14391, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2919
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14392, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2305
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14393, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0435
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14394, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3031
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14395, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4000
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14396, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0227
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14397, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0113
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14398, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0915
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14399, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14400, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14400
Update 14401, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14402, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14403, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14404, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2358
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14405, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14406, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14407, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2426
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14408, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14409, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14410, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3318
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14411, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1963
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14412, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14413, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3100
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14414, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2512
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14415, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2216
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14416, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0306
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14417, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2159
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14418, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0714
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14419, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14420, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1150
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14421, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0163
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14422, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3217
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14423, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1372
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14424, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14425, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3272
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14426, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14427, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14428, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0166
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14429, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14430, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14430
Update 14431, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3925
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14432, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2534
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14433, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2175
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14434, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1180
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14435, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14436, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0499
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14437, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0394
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14438, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5004
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14439, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0245
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14440, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14441, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14442, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14443, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14444, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0181
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14445, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14446, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2056
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14447, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2814
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14448, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1999
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14449, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1172
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14450, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0825
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14451, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14452, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0197
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14453, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14454, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0595
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14455, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0708
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14456, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14457, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5227
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14458, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0155
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14459, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0260
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14460, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1764
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14460
Update 14461, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2733
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14462, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1756
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14463, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2790
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14464, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2609
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14465, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1904
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14466, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14467, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14468, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14469, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1561
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14470, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0279
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14471, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14472, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0429
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14473, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5615
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14474, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0061
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14475, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14476, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14477, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0207
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14478, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2446
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14479, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1156
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14480, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2959
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14481, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3687
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14482, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14483, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14484, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0441
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14485, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14486, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0832
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14487, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2523
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14488, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1432
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14489, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0622
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14490, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1360
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14490
Update 14491, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14492, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0435
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14493, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2201
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14494, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14495, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14496, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4109
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14497, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14498, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14499, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14500, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1270
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14501, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2378
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14502, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1573
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14503, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14504, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0857
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14505, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2916
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14506, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0752
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14507, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0217
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14508, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14509, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14510, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2531
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14511, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4527
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14512, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0418
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14513, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14514, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14515, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0336
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14516, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4093
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14517, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14518, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14519, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1553
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14520, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14520
Update 14521, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14522, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0521
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14523, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3116
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14524, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14525, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5436
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14526, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1315
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14527, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14528, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1287
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14529, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0746
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14530, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14531, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14532, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14533, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14534, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1534
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14535, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4744
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14536, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14537, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0616
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14538, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0883
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14539, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0614
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14540, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5708
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14541, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14542, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14543, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2527
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14544, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1492
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14545, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0372
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14546, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14547, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14548, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0563
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14549, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0324
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14550, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.8121
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14550
Update 14551, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14552, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14553, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0568
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14554, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3465
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14555, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2618
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14556, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14557, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2648
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14558, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2917
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14559, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14560, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0623
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14561, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14562, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2601
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14563, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2634
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14564, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2196
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14565, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2325
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14566, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0141
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14567, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0448
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14568, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4608
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14569, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1423
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14570, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2513
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14571, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14572, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14573, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1334
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14574, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0881
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14575, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14576, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0125
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14577, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1378
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14578, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14579, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1630
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14580, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0095
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14580
Update 14581, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0573
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14582, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14583, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14584, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1596
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14585, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14586, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0079
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14587, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1140
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14588, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2392
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14589, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14590, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14591, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0165
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14592, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14593, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0398
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14594, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0026
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14595, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14596, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3189
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14597, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1085
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14598, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4742
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14599, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14600, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0557
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14601, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3263
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14602, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1233
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14603, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2852
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14604, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0652
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14605, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14606, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14607, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0971
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14608, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.5248
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14609, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0406
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14610, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0135
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14610
Update 14611, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.1267
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14612, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14613, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.4916
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14614, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14615, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14616, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0548
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14617, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0909
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14618, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14619, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.3213
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14620, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.2828
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14621, num samples collected 7500, FPS 31
  Algorithm: train_loss 0.0705
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14622, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14623, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0402
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14624, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2635
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14625, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1868
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14626, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14627, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14628, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14629, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2491
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14630, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3640
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14631, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0145
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14632, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0160
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14633, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0147
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14634, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0666
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14635, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1648
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14636, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2501
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14637, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2692
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14638, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14639, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2495
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14640, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1834
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14640
Update 14641, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14642, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14643, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0191
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14644, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14645, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2130
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14646, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14647, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0724
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14648, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14649, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1741
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14650, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14651, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2532
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14652, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14653, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14654, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4256
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14655, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14656, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0149
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14657, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1704
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14658, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1308
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14659, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0150
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14660, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2455
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14661, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14662, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14663, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0950
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14664, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4147
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14665, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2493
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14666, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14667, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14668, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0650
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14669, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14670, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0057
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14670
Update 14671, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0510
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14672, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1166
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14673, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2317
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14674, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0158
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14675, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3758
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14676, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14677, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14678, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2585
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14679, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0313
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14680, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4548
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14681, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2042
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14682, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3132
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14683, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14684, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0961
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14685, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0419
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14686, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14687, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14688, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0512
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14689, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14690, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1632
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14691, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1185
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14692, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0625
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14693, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14694, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14695, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3164
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14696, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1404
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14697, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14698, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14699, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14700, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14700
Update 14701, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0096
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14702, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0374
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14703, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0637
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14704, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1039
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14705, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3824
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14706, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2935
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14707, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14708, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2480
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14709, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0709
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14710, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3813
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14711, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0685
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14712, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14713, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0076
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14714, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2837
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14715, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2346
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14716, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14717, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14718, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0604
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14719, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2862
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14720, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14721, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14722, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2872
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14723, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14724, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0023
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14725, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3484
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14726, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1277
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14727, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14728, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14729, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1300
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14730, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14730
Update 14731, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1526
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14732, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2672
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14733, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14734, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0171
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14735, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14736, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0384
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14737, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0582
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14738, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0520
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14739, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0760
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14740, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14741, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1766
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14742, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2209
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14743, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0461
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14744, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3718
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14745, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3991
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14746, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14747, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1727
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14748, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0809
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14749, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1448
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14750, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.6315
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14751, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1581
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14752, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0194
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14753, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0490
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14754, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2606
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14755, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0634
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14756, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14757, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14758, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0049
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14759, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2442
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14760, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0237
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14760
Update 14761, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14762, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14763, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1666
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14764, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0983
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14765, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0352
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14766, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2465
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14767, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1305
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14768, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1769
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14769, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2106
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14770, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0870
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14771, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0078
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14772, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0291
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14773, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0097
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14774, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2913
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14775, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14776, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2477
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14777, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14778, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1588
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14779, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5128
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14780, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4549
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14781, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0151
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14782, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14783, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2627
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14784, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0235
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14785, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3290
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14786, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14787, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1571
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14788, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0536
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14789, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14790, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14790
Update 14791, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0144
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14792, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1845
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14793, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1675
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14794, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14795, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14796, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0627
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14797, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14798, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14799, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0120
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14800, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14801, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4493
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14802, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0359
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14803, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14804, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2784
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14805, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14806, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0849
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14807, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0182
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14808, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2003
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14809, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0382
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14810, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14811, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0093
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14812, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1802
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14813, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2460
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14814, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4541
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14815, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4609
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14816, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1050
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14817, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4619
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14818, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0318
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14819, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0559
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14820, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14820
Update 14821, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0130
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14822, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0143
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14823, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1018
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14824, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14825, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1907
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14826, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14827, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14828, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0063
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14829, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1284
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14830, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2603
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14831, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1933
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14832, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0918
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14833, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4517
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14834, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2554
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14835, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2412
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14836, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0436
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14837, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3257
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14838, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0186
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14839, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1727
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14840, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0687
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14841, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14842, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2166
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14843, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2840
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14844, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0996
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14845, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0578
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14846, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0121
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14847, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14848, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14849, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14850, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5289
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14850
Update 14851, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5415
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14852, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3728
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14853, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4003
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14854, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2604
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14855, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14856, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1603
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14857, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0375
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14858, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4378
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14859, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0090
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14860, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0088
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14861, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1215
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14862, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14863, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1142
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14864, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0580
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14865, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14866, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0742
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14867, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14868, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0196
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14869, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14870, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0545
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14871, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14872, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14873, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14874, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14875, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1821
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14876, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0037
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14877, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14878, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2602
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14879, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14880, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.7532
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14880
Update 14881, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0900
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14882, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14883, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4276
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14884, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0126
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14885, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2306
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14886, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0598
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14887, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1678
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14888, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0834
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14889, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1419
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14890, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2629
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14891, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0048
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14892, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14893, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14894, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14895, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0091
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14896, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3117
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14897, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0271
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14898, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0594
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14899, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14900, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0550
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14901, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14902, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14903, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14904, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14905, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2467
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14906, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.7148
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14907, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2745
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14908, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1320
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14909, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0546
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14910, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0020
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14910
Update 14911, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0992
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14912, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3480
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14913, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0116
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14914, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4938
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14915, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3139
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14916, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4071
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14917, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14918, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14919, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0044
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14920, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0675
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14921, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0215
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14922, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2142
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14923, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14924, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14925, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0804
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14926, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14927, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0589
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14928, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1703
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14929, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14930, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1760
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14931, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0033
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14932, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2552
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14933, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0575
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14934, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0703
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14935, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1353
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14936, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14937, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1715
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14938, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0552
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14939, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1556
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14940, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0015
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14940
Update 14941, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0083
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14942, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4186
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14943, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0131
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14944, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5611
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14945, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1280
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14946, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14947, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14948, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0052
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14949, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0472
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14950, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14951, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0569
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14952, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14953, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0034
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14954, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1426
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14955, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0362
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14956, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0570
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14957, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0645
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14958, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14959, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2613
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14960, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4472
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14961, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0087
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14962, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1628
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14963, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2204
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14964, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0187
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14965, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0343
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14966, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1502
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14967, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3153
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14968, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0455
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14969, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1159
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14970, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1697
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 14970
Update 14971, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14972, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2486
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14973, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0298
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14974, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14975, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1645
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14976, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14977, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0117
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14978, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0939
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14979, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0112
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14980, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0066
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14981, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0038
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14982, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1312
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14983, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14984, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0142
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14985, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0780
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14986, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1312
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14987, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2597
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14988, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0177
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14989, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.7394
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14990, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0059
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14991, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2352
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14992, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14993, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2201
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14994, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14995, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1846
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14996, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0591
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14997, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0085
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14998, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1506
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 14999, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4284
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15000, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15000
Update 15001, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15002, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1569
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15003, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15004, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0042
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15005, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1363
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15006, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0295
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15007, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1880
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15008, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2736
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15009, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1936
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15010, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0529
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15011, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0081
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15012, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2567
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15013, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15014, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15015, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1313
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15016, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15017, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4985
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15018, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15019, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2285
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15020, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1914
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15021, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1438
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15022, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1832
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15023, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15024, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0055
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15025, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1819
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15026, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15027, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3457
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15028, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2136
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15029, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0084
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15030, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15030
Update 15031, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0108
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15032, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15033, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1134
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15034, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3901
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15035, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0032
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15036, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15037, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2683
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15038, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15039, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1771
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15040, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15041, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15042, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0615
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15043, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0535
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15044, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3458
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15045, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15046, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0327
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15047, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4017
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15048, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0077
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15049, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2012
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15050, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4496
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15051, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0466
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15052, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0601
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15053, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0400
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15054, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15055, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0304
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15056, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0051
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15057, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0031
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15058, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15059, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4263
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15060, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3295
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15060
Update 15061, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15062, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2245
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15063, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0064
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15064, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0133
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15065, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15066, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0523
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15067, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0823
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15068, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15069, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2394
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15070, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15071, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2082
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15072, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0065
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15073, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3048
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15074, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3974
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15075, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0036
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15076, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2857
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15077, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4206
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15078, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1395
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15079, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1375
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15080, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0040
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15081, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4099
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15082, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0366
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15083, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0667
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15084, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0321
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15085, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15086, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0156
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15087, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0101
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15088, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4210
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15089, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1738
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15090, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15090
Update 15091, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15092, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15093, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0134
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15094, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15095, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0073
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15096, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0793
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15097, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0098
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15098, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1591
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15099, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1274
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15100, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0035
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15101, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0941
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15102, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2413
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15103, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1909
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15104, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0587
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15105, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2367
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15106, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2980
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15107, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2433
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15108, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0041
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15109, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0636
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15110, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5805
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15111, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2356
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15112, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0114
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15113, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0257
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15114, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4357
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15115, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0195
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15116, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1042
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15117, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2522
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15118, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15119, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0119
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15120, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0045
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15120
Update 15121, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0293
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15122, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15123, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0555
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15124, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0501
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15125, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15126, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1161
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15127, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5671
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15128, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2572
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15129, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0599
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15130, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0115
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15131, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0537
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15132, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3228
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15133, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1787
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15134, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1679
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15135, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3481
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15136, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0647
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15137, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3538
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15138, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1844
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15139, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2405
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15140, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0118
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15141, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1440
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15142, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0140
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15143, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0222
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15144, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15145, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0099
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15146, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2011
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15147, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2544
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15148, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0425
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15149, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0086
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15150, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15150
Update 15151, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2511
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15152, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0395
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15153, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0830
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15154, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0547
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15155, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0100
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15156, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.5203
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15157, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15158, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1577
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15159, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2092
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15160, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0132
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15161, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0562
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15162, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0136
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15163, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15164, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1302
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15165, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0072
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15166, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1273
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15167, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1006
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15168, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0403
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15169, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1347
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15170, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2583
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15171, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3847
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15172, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2525
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15173, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0058
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15174, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15175, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1720
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15176, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0089
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15177, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2176
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15178, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0325
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15179, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2841
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15180, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15180
Update 15181, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0047
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15182, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2564
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15183, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0074
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15184, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3001
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15185, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4686
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15186, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0054
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15187, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2608
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15188, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15189, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3170
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15190, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15191, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0102
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15192, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0681
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15193, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0053
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15194, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1257
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15195, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1329
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15196, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.4656
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15197, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0340
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15198, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0094
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15199, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0070
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15200, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15201, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3871
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15202, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15203, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0345
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15204, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0872
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15205, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0624
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15206, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1195
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15207, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0405
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15208, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2270
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15209, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0193
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15210, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1782
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15210
Update 15211, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1636
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15212, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1075
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15213, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15214, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0111
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15215, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0639
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15216, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2152
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15217, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0310
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15218, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0056
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15219, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0104
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15220, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3635
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15221, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1635
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15222, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1960
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15223, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0106
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15224, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2017
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15225, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0138
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15226, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0103
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15227, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2475
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15228, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1046
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15229, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0385
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15230, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15231, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1964
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15232, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2677
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15233, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3552
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15234, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1513
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15235, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0062
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15236, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1356
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15237, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0060
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15238, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3484
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15239, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2324
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15240, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0220
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
New EPOCH! 15240
Update 15241, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0188
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15242, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0853
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15243, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3707
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15244, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.2664
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15245, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0043
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15246, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0629
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15247, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.3507
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15248, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0068
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15249, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0124
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15250, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.0069
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
Update 15251, num samples collected 7500, FPS 30
  Algorithm: train_loss 0.1631
  Episodes: TrainReward -124.2596, l 200.0000, t 318.6653, TestReward -254.0345
